{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. On attention [2 points]\n",
    "\n",
    "Answer each question in max. 3 lines. Points will be subtracted for long answers so be short and precise. Add your explanations in the below box - NO added handwritten images, write in latex!\n",
    "\n",
    "1. [0.5 point] You are given an input $X=[x_1,...,x_L]$ (e.g. a sentence of tokenised words). Your goal is to predict the next token $x_i$ conditional on $x_{1},...,x_{i}$. In other words, the $i$-th position can only be influenced by a position $j$ if and only if $j \\leq i$, namely a position cannot be influenced by the subsequent positions.\n",
    "    a. What would be the problem when passing $X$ through attention? Explain using the computations that attention performs, e.g. you can write out the matrix multiplication. \n",
    "    b. How could you ensure that the prediction for position $i$ depends solely on positions $j\\leq i$? Hint: what in the attention computation should be masked & in what way? \n",
    "2. [0.5 point] Let's consider multi-head attention. How does multi-head attention change the computations inside the softmax (the $QK^T$ term) and the product with $V$ and why could this be beneficial to learning? \n",
    "3. [0.5 point] Remember that standard attention is given by \n",
    "\\begin{align}\n",
    "Q = XW^Q, \\; K=XW^K, \\; V=XW^V, \\;\\; \\textnormal{with} A(X) = softmax\\left(\\frac{QK^T}{\\sqrt{D}}\\right)V\n",
    "\\end{align}\n",
    "for a sequence $X\\in\\mathbb{R}^{L\\times D}$ with $L$ the sequence length. \n",
    "What is the memory and computational cost of attention in terms of the sequence length and where does this come from? \n",
    "4. [0.5 point] A big focus these days is on making attention more computationally efficient. The $i$-th row (as a vector) of attention can be rewritten as, \n",
    "\\begin{align}\n",
    "A(X)_i = \\frac{\\sum_{j=1}^L sim(Q_i,K_j)V_j}{\\sum_{j=1}^L sim(Q_i,K_j)},\n",
    "\\end{align}\n",
    "with $sim(q,k)=exp(q^Tk/\\sqrt{D})$.\n",
    "Linear attention would be given by, \n",
    "\\begin{align}\n",
    "A(X)_i = \\frac{\\sum_{j=1}^L \\phi(Q_i)^T\\phi(K_j)V_j}{\\sum_{j=1}^L\\phi(Q_i)^T\\phi(K_j)}. \n",
    "\\end{align} \n",
    "Explain the relation between regular attentin and linear attention (i.e. what kind of similarity function does linear attention assume and how is the approximation made?) & how the above expression can be rewritten and use this to explain why linear attention computation can be done more efficiently compared to standard attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in your answers below. \n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. On diffusion models [12 points]\n",
    "\n",
    "We will be implementing a diffusion model. We will follow mostly the setup as done here: https://arxiv.org/pdf/2006.11239.pdf so please refer to this work for more details if needed. \n",
    "\n",
    "Your goal will be to implement the code for the diffusion model, find the right configuration to train the model, achieve good accuracy and efficiency and finally interpret how the model learns. You will be marked on clean code, concise answers and good results. \n",
    "\n",
    "We will be working with the FashionMNIST dataset. You can load the data using the below code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# load dataset from the hub\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "channels = 1\n",
    "\n",
    "# define image transformations (e.g. using torchvision)\n",
    "transform = Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "# define function\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset['train'], batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper functions will be useful. In particular, whenever we input a vector (list) of values, this will help to select the right time t output. We will use this in the sampling loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\" \n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. The noising process [2 points]\n",
    "\n",
    "[1.5 pts] Our first step will be to implement the forward process that adds noise to an image. \n",
    "\n",
    "We will be adding noise according to a linear schedule, i.e. the noises $\\beta_t$, $t=1,...,T$ will come from an equally spaced vector. Implement the `linear_beta_schedule` function. Remember $\\alpha_t = 1-\\beta_t$ and $\\bar \\alpha_t = \\prod_{s=1}^t \\alpha_s$. \n",
    "\n",
    "Remember that we have the closed-form solutions for the image at time $t$. Use those in the implementation of `forward_diffusion_sample` instead of a for loop over each noising step. This means you will need to implement `sqrt_one_minus_alphas_cumprod` which represents $\\sqrt{1-\\bar\\alpha_t}$ and `sqrt_alphas_cumprod` which represents $\\sqrt{\\bar\\alpha_t}$. \n",
    "\n",
    "Test the function on an image from the dataset using the given code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    '''\n",
    "    output a vector of size timesteps that is equally spaced between start and end; this will be the noise that is added in each time step. \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, device=\"cpu\"):\n",
    "    \"\"\" \n",
    "    Takes an image and a timestep as input and \n",
    "    returns the noisy version of it\n",
    "    Hint: use the get_index_from_list function to select the right values at time t. \n",
    "    \"\"\"\n",
    "    pass \n",
    "\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "# ADD HERE THE COMPUTATIONS NEEDED for sqrt_alphas_cumprod and sqrt_one_minus_alphas_cumprod\n",
    "\n",
    "# Simulate forward diffusion\n",
    "batch = next(iter(dataloader))[\"pixel_values\"]\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    img, noise = forward_diffusion_sample(batch[0,:,:,:], t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod)\n",
    "    plt.imshow(img.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5 pt] Play around with the noise (compare two different beta schedules), present results for the two settings and discuss (in max. 3 lines) how the hyperparameters change the forward sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. The model for the noise [4 points]\n",
    "\n",
    "We use a simple form of a UNet to predict the noise in the image. The input into the neural network will be a noisy image and the ouput from the model will be the noise in the image. It is important to also pass in the timestep into the neural network (so the model knows at which time we want to denoise the image) and we do this by passing it through a sinusoidal position embedding. \n",
    "\n",
    "[0.5pt] Write code for the Sinusoidal Position Embedding. This should output a matrix $PE$ of size `[timesteps, dimension]` with elements:\n",
    "\n",
    "\\begin{align}\n",
    "&PE_{pos,2i} = \\sin\\left(\\frac{pos}{10000^{2i/dim}}\\right),\\\\\n",
    "&PE_{pos,2i+1} = \\cos\\left(\\frac{pos}{10000^{2i/dim}}\\right),\n",
    "\\end{align}\n",
    "where `pos` refers to the time position and `i` refers to the dimension position and `dim` the total dimension we are working with. \n",
    "\n",
    "Use however the following identity to implement: \n",
    "\\begin{align}\n",
    "\\frac{pos}{10000^{2i/dim}} = \\exp\\left(\\log(pos) - \\frac{2i}{dim}\\log(10000)\\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        '''\n",
    "        Fill in the code\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5pt] Visualise the positional embeddings with arbitary time and dimension and discuss what position embeddings do (why do we add them, why this particular embedding structure) in max. 3 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the code to visualise the positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add here your discussion on the positional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for a Block, that will make up the final UNet. \n",
    "\n",
    "[0.5pt] Fill in the missing parts. \n",
    "\n",
    "Remember that in the UNet architecture, we have a downsampling phase and an upsampling phase. Depending on which phase we are in, we need to make use of the up or down-sample operations. \n",
    "\n",
    "[0.5pt] Add here an explanation as to what the block does including the up and downsample operations in maximum 3 lines:\n",
    "\n",
    "Explanation of a Block: YOUR ANSWER GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        '''\n",
    "        in_ch refers to the number of channels in the input to the operation and out_ch how many should be in the output\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        '''\n",
    "        Define the forward pass making use of the components above. \n",
    "        Time t should get mapped through the time_mlp layer + a relu\n",
    "        The input x should get mapped through a convolutional layer with relu / batchnorm\n",
    "        The time embedding should get added the output from the input convolution \n",
    "        A second convolution should be applied and finally passed through the self.transform. \n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.5pt] Use the above code to fill in the code for the UNet below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 1\n",
    "        down_channels = () # These are the channels that we want to obtain in the downsampling stage; DEFINE YOURSELF!\n",
    "        up_channels = () # These are the channels that we want to obtain in the upsampling stage; DEFINE YOURSELF!\n",
    "        out_dim = 0 # DEFINE THIS CORRECTLY\n",
    "        time_emb_dim = 0 # DEFINE THIS CORRECTLY\n",
    "\n",
    "        # Time embedding consists of a Sinusoidal embedding, a linear map that maintains the dimensions and a rectified linear unit activation. \n",
    "        # TO WRITE CODE\n",
    "        \n",
    "        # Initial projection consisting of a map from image_channels to down_channels[0] with a filter size of e.g. 3 and padding of 1. \n",
    "        # TO WRITE CODE\n",
    "\n",
    "        # Downsample: use the Blocks given above to define down_channels number of downsampling operations. These operations should cha\n",
    "        # TO WRITE CODE HERE; HINT: use something like Block(down_channels[i], down_channels[i+1], time_emb_dim) the right number of times. \n",
    "        \n",
    "        # Upsample\n",
    "        # TO WRITE CODE HERE; same logic as the downsample\n",
    "        \n",
    "        # Final output: given by a final convolution that maps up_channels[-1] to out_dim with a kernel of size 1. \n",
    "        # TO WRITE CODE\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Embedd time\n",
    "        # TO DO \n",
    "        # Initial conv\n",
    "        # TO DO \n",
    "        # Unet: iterate through the downsampling operations and the upsampling operations. Do not forget to include the residual connections \n",
    "        # between the outputs from the downsample stage and the upsample stage. \n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5pt] Finally, define a loss function. Note that this loss function should take x_0 and t to sample the forward diffusion model, get a noisy image, use this noisy image in the model to get the noise added and finally compare true added noise and model outputs added noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    '''\n",
    "    Define the right loss given the model, the true x_0 and the time t\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 3: The sampling [1 point]\n",
    "\n",
    "Write a piece of code that can be used to predict the noise and return the denoised image. This function should work on a single image and make sure that the function can be used in the `sample` function properly. Note that we will need the `posterior_variance` denoted by $\\sigma$ in the paper and the `sqrt_recip_alphas` given by $1/\\alpha_t$. Make sure to set the right choice of $\\sigma$ (see the paper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute posterior_variance \n",
    "posterior_variance = 0 # FILL IN WITH THE RIGHT SETUP;\n",
    "sqrt_recip_alphas = 0 # FILL IN WITH THE RIGHT SETUP; \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_timestep(x, t, i, posterior_variance, sqrt_one_minus_alphas_cumprod, sqrt_alphas_cumprod):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    Note that it also needs additional arguments about the posterior_variance, sqrt_minus_alphas_cumprod and sqrt_recip_alphas. \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "    for i in tqdm(reversed(range(0, T)), desc='sampling loop time step', total=T):\n",
    "        t = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t, i, posterior_variance, sqrt_one_minus_alphas_cumprod, sqrt_recip_alphas)\n",
    "        imgs.append(img.cpu().numpy())\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4. Write the training loop and present & interpret your results [6 points]\n",
    "\n",
    "Code a training loop that instantiates the model, defines an optimiser, defines a number of epochs, iterates over the epochs and the datapoints inside the epoch and for each iteration samples a timestep, uses this timestep to loss function and update parameters based on this. \n",
    "\n",
    "Note: this is the part requiring most computational resources. You may need to train the model for quite a few epochs to get good results. Be sure to start on time to ensure enough access to GPUs on Google Colab. \n",
    "\n",
    "0.5 point will be given for correct implementation. \n",
    "\n",
    "1.5 points will be given according to how many **sensible** samples (with max points given for 10 good samples; scaled accordingly for less samples) you manage to obtain (i.e. the performance of your model in generating good datapoints). \n",
    "\n",
    "2 points will be given according to how efficient you can make the train process to still produce at least 10 good samples. You are free to change anything in the architecture! Your output will be compared to my base setup which uses `50` epochs and has a model with `Num params:  929409`. 1 point will be assigned scaled according to how much less epochs you can use and 1 points scaled with how much less params you manage to use. Be sure to print your number of parameters, training epochs and the 10 sensible images. \n",
    "\n",
    "2 points will be given for the interpretation of your results that allows us to better understand *how* the model learns. For this you are asked to visualise the generated samples during the training process. Can you identify any properties of *how* the model learns? Are there particular aspects the model starts paying attention to / is able to generate before others? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the training loop goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after training is done, use the `sample` to sample new images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the plotting of samples goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include your best generated samples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to show the best samples goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include your most efficient implementation details below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to make your most efficient setup goes here; present all code from above in one Code block\n",
    "# run it to show final runtimes and print out the total number of parameters in the mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of your results goes here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here goes the code for visualising during training and add a discussion above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. On DeepDream [5 points]\n",
    "\n",
    "In this task we will implement a DeepDream model in order to generate ‘dreams’: images enhanced by the neural network. \n",
    "\n",
    "Here we will work with a pre-trained model (Inception-V3). The idea is as follows: we will load some image (you can later on choose to load your own image if you want). We will load the Inception-V3 model. We will pass it an input and obtain the activation outputs from a certain layer. We will 'ask' the neural network to maximise the values of the activation, i.e. enhance the image with what those layer activations 'like' to see. For this we will implement a gradient ascent method in order to maximise a norm of the activations over that input. This will change our input into a `dream'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [1 point] Write a piece of code that will load the Inception-V3 model and allow you to get access to the intermediate activations. \n",
    "\n",
    "2. [2 points] Implement the deepdream optimisation loop. We will use gradient ascent to optimise for the norm of the activation. Implement a function that performs this gradient ascent. This function will take as input the number of iterations to perform, the learning rate, and the start image (i.e. the image which the model will enhance). Define in the function a for loop over the number of iterations, obtain the model output, get the output of the hook (hint: use something like `outputs[-1]`), compute the loss which will be L2 norm of this output (i.e. the L2 norm of the activations from your chosen layer), compute the gradients of this L2 norm loss, scale the gradients by their absolute average (hint: scale using `torch.abs(g).mean()`) and define a gradient ascent step over the image. Do not forget to zero out gradients where needed. \n",
    "\n",
    "3. [1 point] Implement code to load an image (e.g. https://github.com/pytorch/hub/raw/master/images/dog.jpg) and display the output from your optimisation loop (using e.g. numpy). Present the generated images for different choices of layer activations and different number of optimisation steps. Discuss your results. What is the impact of using different layers? What is the impact of the number of optimisation steps? [0.5 point will be given for the implementation and 1.5 point for a very good discussion]\n",
    "\n",
    "4. [1 point] In our previous setting we allowed for optimising for a single layer's activation. Change the method such that it will allow to compute the loss (L2 norm) of multiple layers' outputs. Present three different setups using different numbers & combinations of layers and discuss your results in max. 3 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWERS GO HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
