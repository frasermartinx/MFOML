{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 - Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CID: 02476922\n",
    "\n",
    "**Colab link:** insert colab link here\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Quickfire questions [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 (True risk / Empirical risk):\n",
    "\n",
    "Let $\\hat{\\mathcal{F}}$ be the full class of functions $f:\\mathcal{X} \\to \\mathcal{Y}$. Assume we have data $(x,y) \\sim D = \\mathcal{X} \\times \\mathcal{Y}$. Consider a loss function $L:\\mathcal{Y}\\times\\mathcal{Y} \\to \\mathbb{R}$.\n",
    "\n",
    "We define the true risk, for a sample from the data generating distribution $\\mathcal{D}$, and a specific function in the full class of functions $\\hat{\\mathcal{F}}$ as the expectation of the loss function over the data generating distribution $R(f) := \\mathbb{E}_D[L(f(x),y)]$. We then seek an $\\hat{f}$ that minimizes this.\n",
    "\n",
    "However, in practical situations, we do not know $D$. Hence we approximate the true risk with the empirical risk, which, for a sample $(x^{(i)},y^{(i)})_{i=1}^N$ assumed to be iid from $D$, is the average loss over the sample (which would converge to the true expectation by LLN), which is defined as $\\hat{R}(f) := \\frac{1}{N}\\sum_{i=1}^N L(f(x^{(i)}),y^{(i)})$.\n",
    "\n",
    "The key difference is that the true risk is the actual thing we are trying to minimize, however it is intractable due to the expectation over the unknown data generating distribution, so we approximate it with the empirical risk, the average loss over a sample. We then seek to minimize the empirical risk instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
    "\n",
    "In reality, we do not know the true full class of functions $\\hat{\\mathcal{F}}$, so we choose a candidate $\\mathcal{F}$.\n",
    "\n",
    "The benefits of a large hypothesis class are that they can in general contain a much wider class of function than small hypothesis classes, meaning, assuming we choose it wisely, we have more flexibility and potentially more complexity possible, and a bigger space to search for the optimal function in. If we are searching over a bigger space then we are more likely to find a better function to fit the data.\n",
    "\n",
    "However, as seen with the generalisation bound for a finite hypothesis class, the bound for the generalisation error increases logarithmically with the size of the class of functions. That is, the bound for how well the function generalises to unseen data increases logarithmically with the size of the class of functions. Also, there is always the risk of overfitting with a more complex class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (Dataset splitting):\n",
    "\n",
    "We would not expect the performance of the model on the validation set to perfectly match the performance on unseen data. When we take the training/validation split there is still randomness involved. Perhaps on a different seed a different model would be selected as the best on the validation set. The validation set is also finite, and in this case, small, compared to the training data. Thus, the validation data may have not been able to capture enough variability in the data for it to generalise well to a new test dataset. The selected model may have gotten \"lucky\" on this specific validation set. This is why often there is a training/validation/test split performed, to evaluate performance on data that has not been used to either train the model or tune hyperparamters, since the results can show things such as overfitting/failure to generalise to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (Occamâ€™s razor):\n",
    "\n",
    "Occam's razor in a general sense means that if we have competing hypotheses which have the same performance, we should choose the simplest one. For the specific context of image representation, we should be using low dimensional feature vectors (compressing the data), whilst still being able to represent the important features in the image. For example, in the MNIST dataset, lots of the pixels towards the edge of the images are represented as a 0, since the important distinguishing part of the images are towards the centre, thus we can compress the images by removing the parts of the vector containing the edges of the images (assuming the matrix representation has been flattened to a vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 (Generalisation error):\n",
    "\n",
    "A good model should have a low generalisation error, since, as we estimate it using a second sample (test or validation sample), if this error is low, then it has the meaning that the model can generalise well to unseen data, which is a desirable property. We don't want the model to only be able to perform well on seen data, we want it to be able to perform well on unseen data too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6 (Rademacher complexity pt1):\n",
    "\n",
    "If a function class G has a high empirical Rademacher complexity, then on average, the function class G correlates very well with random noise over the given sample S. This would mean that the function class G would have a high richness, meaning it can produce functions that correlate better with random noise (on average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7 (Rademacher complexity pt2):\n",
    "\n",
    "The problem with the function class dependency presented in the lecture notes is that it is dependent on the size of the class in question. The bound suggests a trade off between reducing the empirical error and the size of the hypothesis set (function class). It is quite an absolute way of assessing things, since it does not take into account how \"good\" the actual function class is, it merely bounds it based on the size.\n",
    "\n",
    "The generalisation bound obtained using Rademacher complexity actually takes into account the richness of the function class, which can lead to better tighter bounds than just considering the size of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8 (Regularisation term in the loss function):\n",
    "\n",
    "Sometimes, certain features can dominate predictions, which is not always desirable. If a regularisation term is added, then the model is penalised for choosing parameters that are very large. This helps to reduce overfitting, since then it limits how much each feature can be relied on for prediction. In the case of the Lasso regularisation term, parameter values can actually be shrunk to 0, meaning the optimisation can result in a model of lower dimensionality, which again helps to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9 (Momentum gradient descent):\n",
    "\n",
    "The issue with gradient descent is that when the Hessian matrix of the objective function is ill conditioned, the condition number is extremely large, meaning that the algorithm is slow to converge. We can think of this as when we make a step, the function takes a bigger step in one direction than others, which can lead to oscillations. Since we only have one parameter $\\alpha$ to control this step size, it becomes very difficult to get a balance of step sizes in different dimensions.\n",
    "\n",
    "Momentum helps this by introducing a memory term that tracks the accumulated gradients across the iterations. We have a hyperparameter $\\beta$ that controls how quickly these contributions of previous gradients exponentially decay. Then, when an update is made, the step is dependent on the current velocity (gradient) and all previous gradients (scaled by this hyperparameter). This helps address slow convergence and oscillations caused by the ill conditioned Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10 (Adam):\n",
    "\n",
    "Adam, or, adaptive moments, is seen as a combination of RMSprop and using momentum to make updates. It initialises the parameter vector and the initial velocity. Then, like most algorithms, it samples a minibatch, before performing an interim parameter update $\\tilde \\theta = \\theta + \\alpha v$ using the inital velocity. Next, the gradient is computed using this interim parameter and the minibatch, $g = \\frac{1}{m} \\nabla_{\\tilde \\theta} \\sum_i L(f(x^{(i)};\\tilde \\theta),y^{(i)})$. Next, the gradient is accumulated $r:= \\rho r + (1-\\rho)g\\cdot g$ where here the dot represents element wise multiplication. Finally, the velocity is updated $v:= \\alpha v - \\frac{\\epsilon}{\\sqrt{r}} \\cdot g$ (element wise division) before the parameter is updated $\\theta:= \\theta +v$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11 (AdaGrad):\n",
    "\n",
    "AdaGrad, or, adaptive gradient descent, adapts the learning rate for each of the parameters by dividing the gradient by a small constant plus the square root of the sum of all previous values of the gradient squared (component wise). This means that parameters who have had large gradients in the past have a large decrease in learning rate, while parameters that have had smaller gradients only get a small decrease in learning rate. This helps to avoid taking such large steps in directions that are already decreasing fast, and bigger steps in directions that have a flatter slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12 (Decaying Learning Rate):\n",
    "\n",
    "A decaying learning rate is often useful, since without one, the descent algorithm can get stuck around a local/global minimum unable to get any deeper, because the steps it takes towards the minimum are too large, resulting in overshooting. When a decaying learning rate is used, the algorithm takes smaller and smaller steps as the iteration counter increases, which hopefully results in no overshooting. However, this has to be tuned, since there is the risk of the step size decreasing too fast, resulting in the algorithm not moving anywhere, despite not being at a local/global minimum. This is why one can opt for a constant learning rate up until iteration $T$ before decaying from then onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "***\n",
    "\n",
    "## Part 2: Short-ish proofs [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 2.1: Bounds on the risk [1 point]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we have $P[\\hat{R}(f) - R(f) \\ge \\epsilon] = P[\\frac{1}{N}\\sum_{i=1}^N L(f(x^{(i)}),y^{(i)}) - R(f) \\ge \\epsilon] = P[\\frac{1}{N}\\sum_{i=1}^N L_i - R(f) \\ge \\epsilon]$. By the requirements of the lemma, we have that $f:X \\to \\{0,1\\}$. This means that, for a given loss function, there are only 4 possible input states for $L_i$. Hence we can think of $L_i$ as a bounded random variable with 4 possible outputs. Set $a_i = \\min L_i, b_i = \\max L_i$. These will be the same for all $i$. Moreover, $L_i$ are independent since the sample is. Set $S_N = \\sum_{i=1}^N L_i$. Then we have $P[\\hat{R}(f) - R(f) \\ge \\epsilon] = P[\\frac{1}{N}S_N - \\frac{1}{N}\\mathbb{E}[S_N] \\ge \\epsilon] = P[S_N - \\mathbb{E}[S_N] \\ge N\\epsilon]$. Finally, we can use Hoeffding's inequality to get $P[S_N - \\mathbb{E}[S_N] \\ge N\\epsilon] \\le \\exp(\\frac{-2(N\\epsilon)^2}{\\sum_{i=1}^N(b_i-a_i)^2})$. Then, since $a_i,b_i$ are constant, set $(b_i - a_i)^2 = K$, then the exponential simplifies to $\\exp(\\frac{-2(N\\epsilon)^2}{NK}) = \\exp(\\frac{-2 N\\epsilon^2}{K}) =  \\exp(-2 N\\epsilon^2)$ since $\\epsilon$ is arbitrary, and $K$ is constant. This gives us the result $P[\\hat{R}(f) - R(f) \\ge \\epsilon] \\le \\exp(-2 N\\epsilon^2)$, as required. The other inequality follows in the exact same way from the other inequality in the Hoeffding's inequality theorem.\n",
    "\n",
    "\n",
    "2. This result shows that as the size of the dataset, $N$, increases, the probability of the empirical risk being more than some value $\\epsilon$ from the true risk for a given hypothesis $f:X \\to \\{0,1\\}$ decreases. This makes sense, we would expect to converge to the true risk in some way when we collect more data.\n",
    "\n",
    "3. The term $|\\mathcal{F}|$ in theorem 4.8 refers to the size of the function class, or, the number of possible functions we are considering. In the context of the theorem, this term means that generalisation bound for a fixed hypothesis increases logarithmically with the size of the function class. So the more functions we are considering, the greater the potential generalisation error. A balance needs to be attained between the error we are willing to accept and the number of functions we want to potentially consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.2: On semi-definiteness [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define $g(t) = f(x+tv)$ for an arbitrary direction $v \\in \\mathbb{R}^d$.\n",
    "\n",
    "\n",
    "\n",
    "To show $g$ is convex, let $t_1,t_2 \\in \\mathbb{R}$ and $\\lambda \\in [0,1]$. Then, using its relation to $f$, and the convexity of $f$:\n",
    "\n",
    "$g(\\lambda t_1 + (1-\\lambda)t_2) = f(x + (\\lambda t_1 + (1-\\lambda)t_2)v) = f(x + \\lambda t_1 v + t_2 v - \\lambda t_2 v)  = f(x + \\lambda t_1 v + t_2 v - \\lambda t_2 v + \\lambda x - \\lambda x) = f(\\lambda (x+t_1 v) + (1-\\lambda)(x+t_2 v)) \\le \\lambda f(x+t_1 v) + (1- \\lambda)f(x + t_2 v) = \\lambda g(t_1) + (1- \\lambda)g(t_2)$\n",
    "\n",
    "Hence $g$ is convex in $\\mathbb{R}$.\n",
    "\n",
    "To show differentiability, we will compute the derivative and show it exists. Letting $x+tv := p(t)$:\n",
    "\n",
    "$g'(t) = \\frac{d}{dt}f(x+tv) = \\sum_i\\frac{\\partial f}{\\partial p_i} \\frac{dp_i}{dt} = \\sum_i[\\nabla f(p(t))]_i v_i =  v \\cdot \\nabla f(p(t)) $.\n",
    "\n",
    "Hence $g$ is once differentiable since $f$ is once differentiable (ie $\\nabla f$ exists).\n",
    "\n",
    "$g''(t) = \\frac{d}{dt}(\\sum_i\\frac{\\partial f}{\\partial p_i} v_i) = \\sum_i \\frac{d}{dt}(\\frac{\\partial f}{\\partial p_i} v_i) = \\sum_i (\\frac{d}{dt}(\\frac{\\partial f}{\\partial p_i}) + \\frac{\\partial f}{\\partial p_i} \\frac{d}{dt}(v_i)) = \\sum_i \\sum_j \\frac{\\partial f}{\\partial p_i \\partial p_j} \\frac{d p_j}{dt} v_i = \\sum_i \\sum_j \\frac{\\partial f}{\\partial p_i \\partial p_j} v_j v_i $\n",
    "\n",
    "$\\implies g''(t)= v^T H v$, where $H$ is the hessian of $f$. Hence $g$ is twice differentiable since $f$ is twice differeniable (ie $H$ exists).\n",
    "\n",
    "We know for $d = 1$, the implication holds, hence, since $g:\\mathbb{R} \\to \\mathbb{R}$, $g$ convex $\\implies g''(t) \\ge 0 \\ \\forall \\ t$.\n",
    "\n",
    "In particular, if we let $t = 0$, we get $0 \\le g''(0) = v^T H v = v^T \\nabla ^2 f(x) v$\n",
    "\n",
    "$\\implies v^T \\nabla ^2 f(x) v \\ge 0 \\ \\forall \\ v \\in \\mathbb{R}^d$, since $v$ was chosen arbitrarily.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.3: A quick recap of momentum [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the analysis with $f(x) = \\frac{1}{2}x^TSx - b^T x, x \\in \\mathbb{R}^d$, and $x^*$ the optimum.\n",
    "\n",
    "1. When we make the change of basis as specified, it allows us to write the error $(x_k - x^*)$ at iteration $k$ in terms of a sum of individual contributions from the initial value $x_0^i$ in a given direction with compounding rate $(1- \\alpha \\lambda_i)^k$. That is, $(x_k - x^*) = \\sum_{i=1}^n w_0^i(1- \\alpha \\lambda_i)^k \\mathbf{q}_i$. We then direclty optimize the learning rate by minimizing this rate, since when it is closer to 1 the convergence is slower.\n",
    "\n",
    "2. The optimal learning rate for this gradient descent was $\\frac{2}{\\lambda_1 + \\lambda_d}$, which leads to a rate of $\\frac{\\frac{\\lambda_d}{\\lambda_1} - 1}{\\frac{\\lambda_d}{\\lambda_1} + 1}$, where $\\frac{\\lambda_d}{\\lambda_1}$ is known as the learning rate. The problem with this learning rate is that for the rate it gives, in order to perform well, it needs this condition number to be closer to 1. Meaning that the two eigenvalues need to be very close, which is problematic since $\\lambda_d$ is the largest eigenvalue and $\\lambda_1$ is the smallest. The further they are apart, the worse the convergence rate is.\n",
    "\n",
    "3. When we consider the matrix $R$, we assume it is diagonalizable and exploit its spectral decomposition. Then, through matrix norm inequalities, we can bound the magnitude of the vector of update components by the maximum absolute value of the eigenvalues of $R$ (and some other constant terms). We then perform further analysis on these eigenvalues to develop the bound further, and analyse the convergence rate of the algorithm.\n",
    "\n",
    "4. We consider eigenvalues of $R$ for which $(\\beta + 1-\\alpha\\lambda_i)^2 -4\\beta< 0 $ in order to ensure that the eigenvalues are complex, since this means that they must be complex conjugates of each other since $R$ is a 2D matrix. This means that these eigenvalues both have the same magnitude of $\\sqrt\\beta$, which means the maximum is fixed, so that based on the bound in 3., the algorithm will be converging  at rate $\\sqrt\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.4: Convergence proof [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "\n",
    "The evolution for netwon's method for gradient descent is as follows:\n",
    "$x_{k+1} = x_k - (\\nabla ^ 2 f(x_k))^{-1} \\nabla f(x_k)$.\n",
    "\n",
    "This basically works by approximating the function $f$ by a quadratic function (using a taylor approximation), and then minimizing that approximation at each iteration wrt the step, which leads to taking steps towards the overall local/global minimum of the function.\n",
    "\n",
    "2. \n",
    "\n",
    "$f(x) = \\frac{1}{2}x^T Q x + b^T x + c$, with $x \\in \\mathbb{R}^d$ and $Q$ is positive definite.\n",
    "\n",
    "Here, $f$ is a quadratic function, so the approximation is exact. The minimum is given when $\\nabla f(x) = 0$, which gives $Qx + b = 0 \\implies x = -Q^{-1}b$ is the minimum.\n",
    "\n",
    "Nevertheless, given a current state $x_k$, we have $\\nabla f(x_k) = Qx_k + b$, and $\\nabla^2 f(x_k) = Q$.\n",
    "\n",
    "Thus, when making the step as per newton's method, we have:\n",
    "\n",
    "$x_{k+1} = x_k - (\\nabla ^ 2 f(x_k))^{-1} \\nabla f(x_k) = x_k - Q^{-1}( Qx_k + b) = - Q^{-1}b$, hence the algorithm converges in one iteration regardless of where we start.\n",
    "\n",
    "3. \n",
    "\n",
    "The statement that we are trying to prove means that given the conditions stated, there exists a region around the optimum such that if the algorithm starts in this region, the iterations are guaranteed to exist and converge to the optimum with a quadratic convergence rate.\n",
    "\n",
    "\n",
    "4. \n",
    "\n",
    "Using Newton's iterative step, $x_{1} = x_0 - (\\nabla ^ 2 f(x_0))^{-1} \\nabla f(x_0)$. \n",
    "\n",
    "$\\implies x_{1} - x^* = x_0 - (\\nabla ^ 2 f(x_0))^{-1} \\nabla f(x_0) - x^*$\n",
    "\n",
    "\n",
    "Then, by thinking of $x_0 - x^*$ as $I(x_0- x^*)$, and noting that $\\nabla ^ 2 f(x_0)$ is invertible:\n",
    "\n",
    "$x_{1} - x^*= (\\nabla ^ 2 f(x_0))^{-1}(\\nabla ^ 2 f(x_0))(x_0 - x^*) - (\\nabla ^ 2 f(x_0))^{-1} \\nabla f(x_0)$\n",
    "\n",
    "$\\implies x_{1} - x^*= (\\nabla ^ 2 f(x_0))^{-1}(\\nabla ^ 2 f(x_0)(x_0 - x^*) - \\nabla f(x_0))$.\n",
    "\n",
    "Then, by using Lemma 0.1, we obtain (noting that $(\\nabla ^ 2 f(x_0))^{-1}$ is a square matrix, and is continuous for all $x$, since it is 3 times continuously differentiable):\n",
    "\n",
    "$ ||x_1 - x^*|| \\le ||(\\nabla ^ 2 f(x_0))^{-1}|| \\ || \\nabla ^ 2 f(x_0)(x_0 - x^*) - \\nabla f(x_0) || $, as required.\n",
    "\n",
    "5.\n",
    "\n",
    " By assumption $f \\in C^3$ and $\\nabla ^2 f(x^*)$ is invertible, hence, by lemma 0.2, there exist scalars $\\epsilon, c_1$ such that for all $x_0 \\in B(x^*,\\epsilon)$, $||(\\nabla ^2 f(x^*))^{-1}|| \\le c_1 $.\n",
    "\n",
    "Also, if we consider the taylor series expansion of $\\nabla f$ about $x_0$, we obtain:\n",
    "\n",
    "$\\nabla f(x)$ = $\\nabla f(x_0) + \\nabla^2 f(x_0)(x-x_0) + O(||x-x_0||^2)$\n",
    "\n",
    "This remainder will contain third derivatives since $f \\in C^3$, these derivatives are continuous and hence bounded on $B(x^*,\\epsilon)$. Then, if we choose $x = x^*$, we obtain (recalling $\\nabla f(x^*) = 0$):\n",
    "\n",
    "$ \\nabla^2 f(x_0)(x_0-x^*) -\\nabla f(x_0)= O(||x^*-x_0||^2)$\n",
    "\n",
    "$\\implies ||\\nabla^2 f(x_0)(x_0-x^*) +\\nabla f(x_0)|| \\le c_2||x_0-x^*||^2$\n",
    "\n",
    "Then, applying these two bounds to step 4. we get the result, there exists an $\\epsilon > 0$ st:\n",
    "\n",
    "$||x_1 - x^*|| \\le c_1c_2||x_0 - x^*||^2$ for all $x_0 \\in B(x^*,\\epsilon)$\n",
    "\n",
    "\n",
    "6.\n",
    "\n",
    "Consider $x_0 \\in B(x^*, \\epsilon)$ such that $||x_0 - x^*|| \\le \\frac{\\alpha}{c_1c_2}$, with $\\alpha \\in (0,1)$, then, $\\epsilon \\le \\frac{\\alpha}{c_1c_2}$, and:\n",
    "\n",
    "$||x_1 - x^*|| \\le c_1 c_2 (\\frac{\\alpha^2}{c_1^2c_2^2}) = \\frac{\\alpha^2}{c_1c_2} \\le \\frac{\\alpha}{c_1c_2}$ since $\\alpha \\in (0,1)$, as required.\n",
    "\n",
    "Also, following this result, we have that $x_1 \\in B(x^*,\\epsilon)$, by definition of the ball.\n",
    "\n",
    "7.\n",
    "\n",
    "By induction, using the same method we just presented, $||x_{k+1} -x^*|| \\le c_1c_2||x_{k}-x^*||^2$.\n",
    "\n",
    "Since  $||x_k - x^*|| \\le \\frac{\\alpha}{c_1c_2}$, we have:\n",
    "\n",
    "$||x_{k+1} -x^*|| \\le \\alpha ||x_{k}-x^*||$ , for all $x_{k} \\in B(x^*, \\epsilon)$ (again meaning $x_{k+1} \\in B(x^*, \\epsilon)$)\n",
    "\n",
    "8. \n",
    "\n",
    "Since $\\alpha \\in (0,1)$, this distance will shrink across the iterations, meaning $x_k \\to x^*$ as $k \\to \\infty$.\n",
    "\n",
    "Also, we have that the convergence is quadratic, since $||x_{k+1} -x^*|| \\le c_1c_2||x_{k}-x^*||^2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Part 3: A deeper dive into neural network implementations [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "#for table plotting later\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39.0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST/raw/train-images-idx3-ubyte.gz to ../../MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST/raw/train-labels-idx1-ubyte.gz to ../../MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST/raw/t10k-images-idx3-ubyte.gz to ../../MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../MNIST/raw\n",
      "\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../cifar-10-python.tar.gz to ../../\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download datasets\n",
    "train_set_mnist = torchvision.datasets.MNIST(root=\"../../\", download=True,\n",
    "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_mnist = torchvision.datasets.MNIST(root=\"../../\",download=True,\n",
    "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)\n",
    "\n",
    "train_set_cifar = torchvision.datasets.CIFAR10(root=\"../../\", download=True,\n",
    "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_cifar = torchvision.datasets.CIFAR10(root=\"../../\",download=True,\n",
    "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ba71810>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed\n",
    "SEED = 2476922\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess MNIST, we will be scaling the inputs to (0,1)\n",
    "print(torch.max(train_set_mnist[100][0]))\n",
    "#it appears the data is already scaled to the correct scaling\n",
    "torch.max(test_set_mnist[100][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 3.1: Implementations [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the Network class, that initializes 5 types of layers, with the Relu layer and hidden layer used multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 1 ###\n",
    "\n",
    "class Network(nn.Module):\n",
    "    #initialise the layers etc\n",
    "    def __init__(self, dim, nclass, width, depth):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            dim - Dimension of the flattened input\n",
    "            nclass - Number of classes we want to predict\n",
    "            width - the width in each layer, same for all layers\n",
    "            depth - the depth of the network    \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #assign depth as an attribute\n",
    "        self.depth = depth\n",
    "        # The code below defines the layers for our model, which includes 5 different types of layer\n",
    "        # The hidden layer and relu layer are used multiple times\n",
    "        layers = []\n",
    "        #input and flatten\n",
    "        layers += [nn.Flatten(),nn.Linear(dim,width),nn.ReLU()]\n",
    "        #only put hidden layers in if depth is more than 1:\n",
    "        for i in range(self.depth-1):\n",
    "            layers += [nn.Linear(width,width), nn.ReLU()]\n",
    "        #output\n",
    "        layers += [nn.Linear(width,nclass)]\n",
    "\n",
    "        #sequential\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    #this gets called on call\n",
    "    def forward(self,input):\n",
    "        #input = input.view(input.size(0), -1)\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the function to load the data, which is already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 2 ###\n",
    "\n",
    "def loading_data(batch_size,train_set,test_set):\n",
    "    #already preprocessed\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    return trainloader,testloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the train and test epochs, taking care to return the average loss over the entire epoch, not just the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3 ###\n",
    "\n",
    "def train_epoch(trainloader, net, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Train a model on the training set of FashionMNIST\n",
    "\n",
    "    Inputs:\n",
    "        trainloader - DataLoader object of the dataset to train on \n",
    "        net - Object of Network\n",
    "        optimizer - the optimizer object\n",
    "        criterion - the loss function\n",
    "    \"\"\"\n",
    "    #set the net to train:\n",
    "    net.train()\n",
    "    loss_count,count = 0,0\n",
    "    #iterate\n",
    "    for imgs, labels in trainloader:\n",
    "        #send to GPU\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        #forward pass\n",
    "        preds = net(imgs)\n",
    "        #find loss\n",
    "        loss = criterion(preds, labels)\n",
    "        #back pass\n",
    "        loss.backward()\n",
    "        #update weights\n",
    "        optimizer.step()\n",
    "        #now compute the additive loss for the epoch (we could do this by specifying reduce = sum in the metric, but this makes things unstable)\n",
    "        #we do batch size in case the batch size does not divide the data size\n",
    "        #loss fn returns batch average on default\n",
    "        batch_size =  labels.shape[0] \n",
    "        loss_count += loss * batch_size\n",
    "        count += batch_size\n",
    "        \n",
    "    return loss_count/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 4 ###\n",
    "\n",
    "\n",
    "def test_epoch(testloader,net,criterion):\n",
    "    \"\"\"\n",
    "    Test a model on a specified dataset.\n",
    "\n",
    "    Inputs:\n",
    "        testloader - DataLoader object of the dataset to test on (validation or test)\n",
    "        net - Trained model of type Network\n",
    "        criterion - the loss function\n",
    "    \"\"\"\n",
    "    #set to evaluate\n",
    "    net.eval()\n",
    "    true_preds, count,loss_count = 0., 0, 0 \n",
    "    for imgs, labels in testloader:\n",
    "        #send to GPU\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)   \n",
    "        #we do not use the gradient\n",
    "        with torch.no_grad():\n",
    "            #find the predicted label\n",
    "            preds = net(imgs)\n",
    "            #now compute the additive loss for the epoch (we could do this by specifying reduce = sum in the metric, but this makes things unstable)\n",
    "            #we do batch size in case the batch size does not divide the data size\n",
    "            #loss fn returns batch average on default\n",
    "            batch_size =  labels.shape[0] \n",
    "            loss_count += criterion(preds, labels) * batch_size\n",
    "            count += batch_size\n",
    "            \n",
    "            #change to label\n",
    "            preds = preds.argmax(dim=-1)\n",
    "\n",
    "            # add the number of correct predictions in the batch to the running total\n",
    "            true_preds += (preds == labels).sum().item()\n",
    "            \n",
    "            \n",
    "    test_err = 1 - true_preds / count\n",
    "    avg_loss = loss_count/count\n",
    "    return  test_err,avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the function that actually trains the model, given the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 5 ###\n",
    "\n",
    "#defining hyperparameters that will be used to initialise the model object\n",
    "\n",
    "#MNIST\n",
    "dim = 784\n",
    "nclass = 10\n",
    "#we set \n",
    "width = 256\n",
    "depth = 1\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model that will train the network\n",
    "def train_model(net,batch_size,lr,num_epochs,train,test):\n",
    "    \"\"\"\n",
    "        Train and test the given model using the other hyperparameters \n",
    "\n",
    "        Inputs:\n",
    "            net - model of type Network\n",
    "            batch_size - the batch size\n",
    "            lr - learning rate\n",
    "            num_epochs - number of epochs to perform\n",
    "            train - the training data\n",
    "            test - the test data\n",
    "    \"\"\"\n",
    "    #define loss, optimizer\n",
    "    #we set loss to be sum otherwise it will average over each batch\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(),lr = lr)\n",
    "    #batch and process data\n",
    "    train_load, test_load = loading_data(batch_size,train,test)\n",
    "    # setup variables to store metrics at each iteration\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    test_err_history = []\n",
    "    #perform epochs\n",
    "    for epoch in range(0,num_epochs):\n",
    "\n",
    "        #train\n",
    "        train_loss =  train_epoch(train_load, net, optimizer, loss_module)\n",
    "        #test recording loss and error:\n",
    "        test_err,test_loss = test_epoch(test_load,net,loss_module)\n",
    "        print(f\"Epoch: {(epoch+1):03} | Train Loss: {train_loss:.04} |Test Loss: {test_loss:.04} | Test Error: {test_err:.04}\")\n",
    "        #record metrics\n",
    "        train_loss_history.append(train_loss.item())\n",
    "        test_loss_history.append(test_loss.item())\n",
    "        test_err_history.append(test_err)\n",
    "    #for later parts in the question, we return the history of each metric:\n",
    "    return train_loss_history, test_loss_history, test_err_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 3.2: Numerical exploration [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 ###\n",
    "For this part of the question, we set the width of each layer to be 256, we shall use ADAM as our optimizer, with the learning rate as 0.001 in order to cater for the larger depths later. We shall run for 100 epochs each time, with a batch size of 1024, based on the size of the dataset. Since we are working with the MNIST dataset, we set dim = 28*28 = 784, and nclass = 10. We shall vary the depth in $\\{1,5,10\\}$ as stated in the question.\n",
    "\n",
    "Upon running the training function, we receive the loss history for both training and testing, as well as the error history for testing. In the table we display the best training loss and best test loss for each depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General MNIST Hyperparameters\n",
    "dim = 784\n",
    "nclass = 10\n",
    "\n",
    "# Task 6 fixed hyperparameters\n",
    "width = 128\n",
    "lr = 0.001\n",
    "batch_size = 1024\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train Loss: 1.003 |Test Loss: 0.4101 | Test Error: 0.1088\n",
      "Epoch: 002 | Train Loss: 0.3585 |Test Loss: 0.3003 | Test Error: 0.0814\n",
      "Epoch: 003 | Train Loss: 0.2889 |Test Loss: 0.2635 | Test Error: 0.075\n",
      "Epoch: 004 | Train Loss: 0.2522 |Test Loss: 0.2343 | Test Error: 0.0668\n",
      "Epoch: 005 | Train Loss: 0.2219 |Test Loss: 0.2099 | Test Error: 0.0592\n",
      "Epoch: 006 | Train Loss: 0.199 |Test Loss: 0.1909 | Test Error: 0.0543\n",
      "Epoch: 007 | Train Loss: 0.1804 |Test Loss: 0.1761 | Test Error: 0.0499\n",
      "Epoch: 008 | Train Loss: 0.1633 |Test Loss: 0.163 | Test Error: 0.0477\n",
      "Epoch: 009 | Train Loss: 0.1498 |Test Loss: 0.1546 | Test Error: 0.0454\n",
      "Epoch: 010 | Train Loss: 0.1377 |Test Loss: 0.1421 | Test Error: 0.0411\n",
      "Epoch: 011 | Train Loss: 0.1272 |Test Loss: 0.135 | Test Error: 0.0388\n",
      "Epoch: 012 | Train Loss: 0.1181 |Test Loss: 0.1288 | Test Error: 0.0381\n",
      "Epoch: 013 | Train Loss: 0.1106 |Test Loss: 0.1225 | Test Error: 0.0372\n",
      "Epoch: 014 | Train Loss: 0.1037 |Test Loss: 0.1175 | Test Error: 0.0349\n",
      "Epoch: 015 | Train Loss: 0.09632 |Test Loss: 0.1133 | Test Error: 0.0347\n",
      "Epoch: 016 | Train Loss: 0.09051 |Test Loss: 0.1102 | Test Error: 0.0327\n",
      "Epoch: 017 | Train Loss: 0.08516 |Test Loss: 0.1055 | Test Error: 0.0324\n",
      "Epoch: 018 | Train Loss: 0.08054 |Test Loss: 0.1031 | Test Error: 0.0316\n",
      "Epoch: 019 | Train Loss: 0.07601 |Test Loss: 0.1014 | Test Error: 0.0298\n",
      "Epoch: 020 | Train Loss: 0.07184 |Test Loss: 0.0973 | Test Error: 0.0296\n",
      "Epoch: 021 | Train Loss: 0.06795 |Test Loss: 0.09693 | Test Error: 0.0295\n",
      "Epoch: 022 | Train Loss: 0.06431 |Test Loss: 0.09338 | Test Error: 0.0286\n",
      "Epoch: 023 | Train Loss: 0.06076 |Test Loss: 0.0914 | Test Error: 0.0284\n",
      "Epoch: 024 | Train Loss: 0.05759 |Test Loss: 0.0896 | Test Error: 0.0273\n",
      "Epoch: 025 | Train Loss: 0.05464 |Test Loss: 0.08825 | Test Error: 0.0269\n",
      "Epoch: 026 | Train Loss: 0.05205 |Test Loss: 0.08722 | Test Error: 0.0263\n",
      "Epoch: 027 | Train Loss: 0.04906 |Test Loss: 0.08485 | Test Error: 0.0255\n",
      "Epoch: 028 | Train Loss: 0.04646 |Test Loss: 0.08505 | Test Error: 0.0258\n",
      "Epoch: 029 | Train Loss: 0.04443 |Test Loss: 0.0859 | Test Error: 0.025\n",
      "Epoch: 030 | Train Loss: 0.04307 |Test Loss: 0.08376 | Test Error: 0.0247\n",
      "Epoch: 031 | Train Loss: 0.04008 |Test Loss: 0.08271 | Test Error: 0.0243\n",
      "Epoch: 032 | Train Loss: 0.03808 |Test Loss: 0.08095 | Test Error: 0.0238\n",
      "Epoch: 033 | Train Loss: 0.03566 |Test Loss: 0.08127 | Test Error: 0.024\n",
      "Epoch: 034 | Train Loss: 0.03462 |Test Loss: 0.08072 | Test Error: 0.0238\n",
      "Epoch: 035 | Train Loss: 0.0324 |Test Loss: 0.08197 | Test Error: 0.0244\n",
      "Epoch: 036 | Train Loss: 0.03121 |Test Loss: 0.0793 | Test Error: 0.0234\n",
      "Epoch: 037 | Train Loss: 0.02939 |Test Loss: 0.08115 | Test Error: 0.0238\n",
      "Epoch: 038 | Train Loss: 0.02805 |Test Loss: 0.07916 | Test Error: 0.0239\n",
      "Epoch: 039 | Train Loss: 0.02679 |Test Loss: 0.08025 | Test Error: 0.0234\n",
      "Epoch: 040 | Train Loss: 0.02524 |Test Loss: 0.0788 | Test Error: 0.0233\n",
      "Epoch: 041 | Train Loss: 0.02393 |Test Loss: 0.07995 | Test Error: 0.0234\n",
      "Epoch: 042 | Train Loss: 0.02317 |Test Loss: 0.07941 | Test Error: 0.023\n",
      "Epoch: 043 | Train Loss: 0.02185 |Test Loss: 0.08124 | Test Error: 0.0234\n",
      "Epoch: 044 | Train Loss: 0.0209 |Test Loss: 0.08084 | Test Error: 0.0239\n",
      "Epoch: 045 | Train Loss: 0.01994 |Test Loss: 0.08085 | Test Error: 0.0237\n",
      "Epoch: 046 | Train Loss: 0.01877 |Test Loss: 0.08134 | Test Error: 0.0235\n",
      "Epoch: 047 | Train Loss: 0.01782 |Test Loss: 0.08028 | Test Error: 0.0223\n",
      "Epoch: 048 | Train Loss: 0.01695 |Test Loss: 0.08132 | Test Error: 0.0225\n",
      "Epoch: 049 | Train Loss: 0.01588 |Test Loss: 0.07911 | Test Error: 0.0231\n",
      "Epoch: 050 | Train Loss: 0.01528 |Test Loss: 0.08105 | Test Error: 0.0227\n",
      "Epoch: 051 | Train Loss: 0.01452 |Test Loss: 0.08123 | Test Error: 0.0237\n",
      "Epoch: 052 | Train Loss: 0.01382 |Test Loss: 0.08162 | Test Error: 0.0233\n",
      "Epoch: 053 | Train Loss: 0.01355 |Test Loss: 0.08219 | Test Error: 0.0237\n",
      "Epoch: 054 | Train Loss: 0.01262 |Test Loss: 0.08139 | Test Error: 0.0227\n",
      "Epoch: 055 | Train Loss: 0.012 |Test Loss: 0.08297 | Test Error: 0.0237\n",
      "Epoch: 056 | Train Loss: 0.01141 |Test Loss: 0.08182 | Test Error: 0.0231\n",
      "Epoch: 057 | Train Loss: 0.01095 |Test Loss: 0.08379 | Test Error: 0.024\n",
      "Epoch: 058 | Train Loss: 0.01042 |Test Loss: 0.08261 | Test Error: 0.0237\n",
      "Epoch: 059 | Train Loss: 0.009893 |Test Loss: 0.08228 | Test Error: 0.0231\n",
      "Epoch: 060 | Train Loss: 0.009598 |Test Loss: 0.08218 | Test Error: 0.0228\n",
      "Epoch: 061 | Train Loss: 0.008893 |Test Loss: 0.08394 | Test Error: 0.0236\n",
      "Epoch: 062 | Train Loss: 0.00854 |Test Loss: 0.08456 | Test Error: 0.0241\n",
      "Epoch: 063 | Train Loss: 0.008203 |Test Loss: 0.0847 | Test Error: 0.0233\n",
      "Epoch: 064 | Train Loss: 0.007843 |Test Loss: 0.08615 | Test Error: 0.0232\n",
      "Epoch: 065 | Train Loss: 0.0074 |Test Loss: 0.08575 | Test Error: 0.0238\n",
      "Epoch: 066 | Train Loss: 0.00723 |Test Loss: 0.0865 | Test Error: 0.0237\n",
      "Epoch: 067 | Train Loss: 0.006794 |Test Loss: 0.08574 | Test Error: 0.0238\n",
      "Epoch: 068 | Train Loss: 0.006632 |Test Loss: 0.08591 | Test Error: 0.0233\n",
      "Epoch: 069 | Train Loss: 0.006113 |Test Loss: 0.08833 | Test Error: 0.0231\n",
      "Epoch: 070 | Train Loss: 0.00585 |Test Loss: 0.08715 | Test Error: 0.0233\n",
      "Epoch: 071 | Train Loss: 0.005516 |Test Loss: 0.08694 | Test Error: 0.0235\n",
      "Epoch: 072 | Train Loss: 0.005422 |Test Loss: 0.08834 | Test Error: 0.0228\n",
      "Epoch: 073 | Train Loss: 0.005275 |Test Loss: 0.08863 | Test Error: 0.0234\n",
      "Epoch: 074 | Train Loss: 0.004754 |Test Loss: 0.08877 | Test Error: 0.0235\n",
      "Epoch: 075 | Train Loss: 0.004607 |Test Loss: 0.09023 | Test Error: 0.0232\n",
      "Epoch: 076 | Train Loss: 0.004377 |Test Loss: 0.09057 | Test Error: 0.0237\n",
      "Epoch: 077 | Train Loss: 0.004213 |Test Loss: 0.08994 | Test Error: 0.0238\n",
      "Epoch: 078 | Train Loss: 0.00396 |Test Loss: 0.09092 | Test Error: 0.0237\n",
      "Epoch: 079 | Train Loss: 0.003729 |Test Loss: 0.09107 | Test Error: 0.0233\n",
      "Epoch: 080 | Train Loss: 0.003632 |Test Loss: 0.09173 | Test Error: 0.0231\n",
      "Epoch: 081 | Train Loss: 0.003458 |Test Loss: 0.09262 | Test Error: 0.0236\n",
      "Epoch: 082 | Train Loss: 0.003366 |Test Loss: 0.09256 | Test Error: 0.0225\n",
      "Epoch: 083 | Train Loss: 0.003156 |Test Loss: 0.09333 | Test Error: 0.024\n",
      "Epoch: 084 | Train Loss: 0.003042 |Test Loss: 0.09343 | Test Error: 0.0238\n",
      "Epoch: 085 | Train Loss: 0.00292 |Test Loss: 0.09414 | Test Error: 0.0237\n",
      "Epoch: 086 | Train Loss: 0.002762 |Test Loss: 0.09434 | Test Error: 0.0229\n",
      "Epoch: 087 | Train Loss: 0.002698 |Test Loss: 0.09493 | Test Error: 0.0226\n",
      "Epoch: 088 | Train Loss: 0.002545 |Test Loss: 0.09507 | Test Error: 0.0229\n",
      "Epoch: 089 | Train Loss: 0.002419 |Test Loss: 0.09538 | Test Error: 0.0226\n",
      "Epoch: 090 | Train Loss: 0.002359 |Test Loss: 0.09653 | Test Error: 0.023\n",
      "Epoch: 091 | Train Loss: 0.002268 |Test Loss: 0.09734 | Test Error: 0.0231\n",
      "Epoch: 092 | Train Loss: 0.002208 |Test Loss: 0.09721 | Test Error: 0.0224\n",
      "Epoch: 093 | Train Loss: 0.002067 |Test Loss: 0.09851 | Test Error: 0.0228\n",
      "Epoch: 094 | Train Loss: 0.001978 |Test Loss: 0.09936 | Test Error: 0.0232\n",
      "Epoch: 095 | Train Loss: 0.001909 |Test Loss: 0.09914 | Test Error: 0.0232\n",
      "Epoch: 096 | Train Loss: 0.001835 |Test Loss: 0.09912 | Test Error: 0.0227\n",
      "Epoch: 097 | Train Loss: 0.001758 |Test Loss: 0.09922 | Test Error: 0.023\n",
      "Epoch: 098 | Train Loss: 0.001744 |Test Loss: 0.09942 | Test Error: 0.022\n",
      "Epoch: 099 | Train Loss: 0.001644 |Test Loss: 0.1004 | Test Error: 0.0228\n",
      "Epoch: 100 | Train Loss: 0.00156 |Test Loss: 0.1005 | Test Error: 0.0225\n"
     ]
    }
   ],
   "source": [
    "# depth = 1 parameters\n",
    "depth = 1\n",
    "# initialize model\n",
    "net1 = Network(dim,nclass,width,depth)\n",
    "#send to GPU\n",
    "net1 = net1.to(device)\n",
    "#train it\n",
    "D1 = train_model(net1,batch_size,lr,num_epochs,train_set_mnist,test_set_mnist)\n",
    "#best = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training loss for depth = 1: 0.00156, best test loss for depth = 1: 0.0788\n"
     ]
    }
   ],
   "source": [
    "D1_min_train = np.argmin(D1[0])\n",
    "D1_min_test = np.argmin(D1[1])\n",
    "print(f\"Best training loss for depth = 1: {D1[0][D1_min_train]:.04}, best test loss for depth = 1: {D1[1][D1_min_test]:.04}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11f664f10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(D1[0],label = \"train\")\n",
    "plt.plot(D1[1],label = \"test\")\n",
    "plt.plot(D1[2],label = \"accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train Loss: 1.204 |Test Loss: 0.4496 | Test Error: 0.1368\n",
      "Epoch: 002 | Train Loss: 0.387 |Test Loss: 0.316 | Test Error: 0.0957\n",
      "Epoch: 003 | Train Loss: 0.2856 |Test Loss: 0.2455 | Test Error: 0.0738\n",
      "Epoch: 004 | Train Loss: 0.2176 |Test Loss: 0.1937 | Test Error: 0.0571\n",
      "Epoch: 005 | Train Loss: 0.1699 |Test Loss: 0.1677 | Test Error: 0.0502\n",
      "Epoch: 006 | Train Loss: 0.1412 |Test Loss: 0.1681 | Test Error: 0.0515\n",
      "Epoch: 007 | Train Loss: 0.1256 |Test Loss: 0.132 | Test Error: 0.0397\n",
      "Epoch: 008 | Train Loss: 0.1045 |Test Loss: 0.1221 | Test Error: 0.0374\n",
      "Epoch: 009 | Train Loss: 0.08999 |Test Loss: 0.1209 | Test Error: 0.0363\n",
      "Epoch: 010 | Train Loss: 0.08293 |Test Loss: 0.1115 | Test Error: 0.0339\n",
      "Epoch: 011 | Train Loss: 0.07161 |Test Loss: 0.1149 | Test Error: 0.0336\n",
      "Epoch: 012 | Train Loss: 0.06764 |Test Loss: 0.1058 | Test Error: 0.0313\n",
      "Epoch: 013 | Train Loss: 0.05707 |Test Loss: 0.1042 | Test Error: 0.0304\n",
      "Epoch: 014 | Train Loss: 0.05217 |Test Loss: 0.1065 | Test Error: 0.0298\n",
      "Epoch: 015 | Train Loss: 0.04661 |Test Loss: 0.1062 | Test Error: 0.031\n",
      "Epoch: 016 | Train Loss: 0.04375 |Test Loss: 0.09848 | Test Error: 0.0283\n",
      "Epoch: 017 | Train Loss: 0.03599 |Test Loss: 0.1026 | Test Error: 0.0282\n",
      "Epoch: 018 | Train Loss: 0.03185 |Test Loss: 0.103 | Test Error: 0.0278\n",
      "Epoch: 019 | Train Loss: 0.02768 |Test Loss: 0.1049 | Test Error: 0.028\n",
      "Epoch: 020 | Train Loss: 0.02645 |Test Loss: 0.1129 | Test Error: 0.0304\n",
      "Epoch: 021 | Train Loss: 0.02599 |Test Loss: 0.1183 | Test Error: 0.031\n",
      "Epoch: 022 | Train Loss: 0.02568 |Test Loss: 0.1108 | Test Error: 0.0272\n",
      "Epoch: 023 | Train Loss: 0.01752 |Test Loss: 0.1066 | Test Error: 0.0257\n",
      "Epoch: 024 | Train Loss: 0.01534 |Test Loss: 0.1182 | Test Error: 0.0288\n",
      "Epoch: 025 | Train Loss: 0.01717 |Test Loss: 0.1192 | Test Error: 0.029\n",
      "Epoch: 026 | Train Loss: 0.0179 |Test Loss: 0.1321 | Test Error: 0.0302\n",
      "Epoch: 027 | Train Loss: 0.01868 |Test Loss: 0.1193 | Test Error: 0.0279\n",
      "Epoch: 028 | Train Loss: 0.01503 |Test Loss: 0.1229 | Test Error: 0.0279\n",
      "Epoch: 029 | Train Loss: 0.01294 |Test Loss: 0.1325 | Test Error: 0.0293\n",
      "Epoch: 030 | Train Loss: 0.01843 |Test Loss: 0.1248 | Test Error: 0.0268\n",
      "Epoch: 031 | Train Loss: 0.01394 |Test Loss: 0.1326 | Test Error: 0.0275\n",
      "Epoch: 032 | Train Loss: 0.01096 |Test Loss: 0.1334 | Test Error: 0.0276\n",
      "Epoch: 033 | Train Loss: 0.006708 |Test Loss: 0.1278 | Test Error: 0.0257\n",
      "Epoch: 034 | Train Loss: 0.004311 |Test Loss: 0.1326 | Test Error: 0.0244\n",
      "Epoch: 035 | Train Loss: 0.00411 |Test Loss: 0.1352 | Test Error: 0.0256\n",
      "Epoch: 036 | Train Loss: 0.004297 |Test Loss: 0.1433 | Test Error: 0.0264\n",
      "Epoch: 037 | Train Loss: 0.003433 |Test Loss: 0.147 | Test Error: 0.026\n",
      "Epoch: 038 | Train Loss: 0.003039 |Test Loss: 0.1527 | Test Error: 0.0265\n",
      "Epoch: 039 | Train Loss: 0.001621 |Test Loss: 0.1529 | Test Error: 0.0257\n",
      "Epoch: 040 | Train Loss: 0.00162 |Test Loss: 0.1546 | Test Error: 0.0249\n",
      "Epoch: 041 | Train Loss: 0.004073 |Test Loss: 0.1554 | Test Error: 0.0271\n",
      "Epoch: 042 | Train Loss: 0.0104 |Test Loss: 0.167 | Test Error: 0.0283\n",
      "Epoch: 043 | Train Loss: 0.02206 |Test Loss: 0.1478 | Test Error: 0.029\n",
      "Epoch: 044 | Train Loss: 0.01557 |Test Loss: 0.1404 | Test Error: 0.0275\n",
      "Epoch: 045 | Train Loss: 0.007535 |Test Loss: 0.1489 | Test Error: 0.0269\n",
      "Epoch: 046 | Train Loss: 0.004496 |Test Loss: 0.1465 | Test Error: 0.0252\n",
      "Epoch: 047 | Train Loss: 0.004081 |Test Loss: 0.1607 | Test Error: 0.028\n",
      "Epoch: 048 | Train Loss: 0.00313 |Test Loss: 0.1569 | Test Error: 0.0257\n",
      "Epoch: 049 | Train Loss: 0.001277 |Test Loss: 0.1645 | Test Error: 0.0255\n",
      "Epoch: 050 | Train Loss: 0.0009279 |Test Loss: 0.1785 | Test Error: 0.0275\n",
      "Epoch: 051 | Train Loss: 0.004443 |Test Loss: 0.1858 | Test Error: 0.0279\n",
      "Epoch: 052 | Train Loss: 0.00874 |Test Loss: 0.1628 | Test Error: 0.0267\n",
      "Epoch: 053 | Train Loss: 0.01605 |Test Loss: 0.1462 | Test Error: 0.0258\n",
      "Epoch: 054 | Train Loss: 0.01103 |Test Loss: 0.1668 | Test Error: 0.0299\n",
      "Epoch: 055 | Train Loss: 0.007091 |Test Loss: 0.1606 | Test Error: 0.0281\n",
      "Epoch: 056 | Train Loss: 0.004813 |Test Loss: 0.1493 | Test Error: 0.024\n",
      "Epoch: 057 | Train Loss: 0.001218 |Test Loss: 0.1506 | Test Error: 0.0237\n",
      "Epoch: 058 | Train Loss: 0.0006964 |Test Loss: 0.1525 | Test Error: 0.0237\n",
      "Epoch: 059 | Train Loss: 0.0002162 |Test Loss: 0.1536 | Test Error: 0.0235\n",
      "Epoch: 060 | Train Loss: 0.0001302 |Test Loss: 0.1555 | Test Error: 0.0231\n",
      "Epoch: 061 | Train Loss: 0.0001051 |Test Loss: 0.1577 | Test Error: 0.0235\n",
      "Epoch: 062 | Train Loss: 9.113e-05 |Test Loss: 0.1596 | Test Error: 0.0234\n",
      "Epoch: 063 | Train Loss: 8.125e-05 |Test Loss: 0.1618 | Test Error: 0.0235\n",
      "Epoch: 064 | Train Loss: 7.346e-05 |Test Loss: 0.1628 | Test Error: 0.0237\n",
      "Epoch: 065 | Train Loss: 6.664e-05 |Test Loss: 0.1647 | Test Error: 0.0235\n",
      "Epoch: 066 | Train Loss: 6.083e-05 |Test Loss: 0.1662 | Test Error: 0.0236\n",
      "Epoch: 067 | Train Loss: 5.659e-05 |Test Loss: 0.1676 | Test Error: 0.0239\n",
      "Epoch: 068 | Train Loss: 5.176e-05 |Test Loss: 0.169 | Test Error: 0.0235\n",
      "Epoch: 069 | Train Loss: 4.848e-05 |Test Loss: 0.1702 | Test Error: 0.0237\n",
      "Epoch: 070 | Train Loss: 4.489e-05 |Test Loss: 0.1715 | Test Error: 0.0237\n",
      "Epoch: 071 | Train Loss: 4.202e-05 |Test Loss: 0.1726 | Test Error: 0.0238\n",
      "Epoch: 072 | Train Loss: 3.947e-05 |Test Loss: 0.1739 | Test Error: 0.0237\n",
      "Epoch: 073 | Train Loss: 3.697e-05 |Test Loss: 0.1748 | Test Error: 0.0239\n",
      "Epoch: 074 | Train Loss: 3.489e-05 |Test Loss: 0.1758 | Test Error: 0.0237\n",
      "Epoch: 075 | Train Loss: 3.299e-05 |Test Loss: 0.177 | Test Error: 0.024\n",
      "Epoch: 076 | Train Loss: 3.119e-05 |Test Loss: 0.1778 | Test Error: 0.0239\n",
      "Epoch: 077 | Train Loss: 2.947e-05 |Test Loss: 0.1789 | Test Error: 0.0239\n",
      "Epoch: 078 | Train Loss: 2.798e-05 |Test Loss: 0.1799 | Test Error: 0.0239\n",
      "Epoch: 079 | Train Loss: 2.653e-05 |Test Loss: 0.1809 | Test Error: 0.0235\n",
      "Epoch: 080 | Train Loss: 2.522e-05 |Test Loss: 0.1817 | Test Error: 0.0239\n",
      "Epoch: 081 | Train Loss: 2.402e-05 |Test Loss: 0.1828 | Test Error: 0.0236\n",
      "Epoch: 082 | Train Loss: 2.282e-05 |Test Loss: 0.1835 | Test Error: 0.0242\n",
      "Epoch: 083 | Train Loss: 2.205e-05 |Test Loss: 0.1843 | Test Error: 0.0238\n",
      "Epoch: 084 | Train Loss: 2.077e-05 |Test Loss: 0.1851 | Test Error: 0.0238\n",
      "Epoch: 085 | Train Loss: 1.978e-05 |Test Loss: 0.1862 | Test Error: 0.0235\n",
      "Epoch: 086 | Train Loss: 1.904e-05 |Test Loss: 0.1871 | Test Error: 0.0235\n",
      "Epoch: 087 | Train Loss: 1.812e-05 |Test Loss: 0.1878 | Test Error: 0.0234\n",
      "Epoch: 088 | Train Loss: 1.741e-05 |Test Loss: 0.1887 | Test Error: 0.0236\n",
      "Epoch: 089 | Train Loss: 1.66e-05 |Test Loss: 0.1895 | Test Error: 0.0236\n",
      "Epoch: 090 | Train Loss: 1.596e-05 |Test Loss: 0.1902 | Test Error: 0.0237\n",
      "Epoch: 091 | Train Loss: 1.528e-05 |Test Loss: 0.1907 | Test Error: 0.0236\n",
      "Epoch: 092 | Train Loss: 1.475e-05 |Test Loss: 0.1915 | Test Error: 0.0238\n",
      "Epoch: 093 | Train Loss: 1.4e-05 |Test Loss: 0.1924 | Test Error: 0.0236\n",
      "Epoch: 094 | Train Loss: 1.349e-05 |Test Loss: 0.1932 | Test Error: 0.0236\n",
      "Epoch: 095 | Train Loss: 1.29e-05 |Test Loss: 0.1941 | Test Error: 0.0234\n",
      "Epoch: 096 | Train Loss: 1.237e-05 |Test Loss: 0.195 | Test Error: 0.0236\n",
      "Epoch: 097 | Train Loss: 1.198e-05 |Test Loss: 0.1954 | Test Error: 0.0237\n",
      "Epoch: 098 | Train Loss: 1.153e-05 |Test Loss: 0.1961 | Test Error: 0.0237\n",
      "Epoch: 099 | Train Loss: 1.102e-05 |Test Loss: 0.1969 | Test Error: 0.0236\n",
      "Epoch: 100 | Train Loss: 1.071e-05 |Test Loss: 0.1975 | Test Error: 0.0235\n"
     ]
    }
   ],
   "source": [
    "# depth = 5 parameters\n",
    "depth = 5\n",
    "lr = 0.001\n",
    "# initialize model\n",
    "net5 = Network(dim,nclass,width,depth)\n",
    "#send to GPU\n",
    "net5 = net5.to(device) \n",
    "#train it\n",
    "D5 = train_model(net5,batch_size,lr,num_epochs,train_set_mnist,test_set_mnist)\n",
    "#print(f\"Final training loss for depth = 5: {D5[0]:.04}, final test loss for depth = 5: {D5[1]:.04}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training loss for depth = 1: 1.071e-05, best test loss for depth = 1: 0.09848\n"
     ]
    }
   ],
   "source": [
    "D5_min_train = np.argmin(D5[0])\n",
    "D5_min_test = np.argmin(D5[1])\n",
    "print(f\"Best training loss for depth = 1: {D5[0][D5_min_train]:.04}, best test loss for depth = 1: {D5[1][D5_min_test]:.04}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x122d49150>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(D5[0],label = \"train\")\n",
    "plt.plot(D5[1],label = \"test\")\n",
    "plt.plot(D5[2],label = \"accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train Loss: 1.675 |Test Loss: 0.9032 | Test Error: 0.3222\n",
      "Epoch: 002 | Train Loss: 0.682 |Test Loss: 0.5066 | Test Error: 0.1567\n",
      "Epoch: 003 | Train Loss: 0.4223 |Test Loss: 0.3667 | Test Error: 0.1053\n",
      "Epoch: 004 | Train Loss: 0.3185 |Test Loss: 0.2774 | Test Error: 0.0786\n",
      "Epoch: 005 | Train Loss: 0.2516 |Test Loss: 0.2481 | Test Error: 0.0709\n",
      "Epoch: 006 | Train Loss: 0.2238 |Test Loss: 0.2309 | Test Error: 0.0653\n",
      "Epoch: 007 | Train Loss: 0.1884 |Test Loss: 0.2078 | Test Error: 0.0607\n",
      "Epoch: 008 | Train Loss: 0.1585 |Test Loss: 0.1771 | Test Error: 0.0487\n",
      "Epoch: 009 | Train Loss: 0.1378 |Test Loss: 0.1798 | Test Error: 0.0513\n",
      "Epoch: 010 | Train Loss: 0.1409 |Test Loss: 0.1684 | Test Error: 0.0473\n",
      "Epoch: 011 | Train Loss: 0.1086 |Test Loss: 0.1544 | Test Error: 0.0436\n",
      "Epoch: 012 | Train Loss: 0.09981 |Test Loss: 0.1503 | Test Error: 0.0417\n",
      "Epoch: 013 | Train Loss: 0.08855 |Test Loss: 0.1504 | Test Error: 0.0417\n",
      "Epoch: 014 | Train Loss: 0.08067 |Test Loss: 0.1395 | Test Error: 0.0383\n",
      "Epoch: 015 | Train Loss: 0.07275 |Test Loss: 0.1429 | Test Error: 0.0382\n",
      "Epoch: 016 | Train Loss: 0.06095 |Test Loss: 0.1484 | Test Error: 0.0377\n",
      "Epoch: 017 | Train Loss: 0.05797 |Test Loss: 0.1414 | Test Error: 0.0363\n",
      "Epoch: 018 | Train Loss: 0.05002 |Test Loss: 0.1388 | Test Error: 0.0346\n",
      "Epoch: 019 | Train Loss: 0.04326 |Test Loss: 0.1497 | Test Error: 0.0361\n",
      "Epoch: 020 | Train Loss: 0.04224 |Test Loss: 0.1558 | Test Error: 0.0364\n",
      "Epoch: 021 | Train Loss: 0.0404 |Test Loss: 0.1395 | Test Error: 0.0323\n",
      "Epoch: 022 | Train Loss: 0.03925 |Test Loss: 0.1429 | Test Error: 0.0319\n",
      "Epoch: 023 | Train Loss: 0.0316 |Test Loss: 0.1578 | Test Error: 0.0353\n",
      "Epoch: 024 | Train Loss: 0.0276 |Test Loss: 0.1553 | Test Error: 0.0336\n",
      "Epoch: 025 | Train Loss: 0.03418 |Test Loss: 0.1468 | Test Error: 0.0361\n",
      "Epoch: 026 | Train Loss: 0.02821 |Test Loss: 0.1482 | Test Error: 0.0346\n",
      "Epoch: 027 | Train Loss: 0.02256 |Test Loss: 0.19 | Test Error: 0.0396\n",
      "Epoch: 028 | Train Loss: 0.03217 |Test Loss: 0.1558 | Test Error: 0.0342\n",
      "Epoch: 029 | Train Loss: 0.02072 |Test Loss: 0.1584 | Test Error: 0.0321\n",
      "Epoch: 030 | Train Loss: 0.02325 |Test Loss: 0.1556 | Test Error: 0.0306\n",
      "Epoch: 031 | Train Loss: 0.0254 |Test Loss: 0.167 | Test Error: 0.0336\n",
      "Epoch: 032 | Train Loss: 0.02027 |Test Loss: 0.1594 | Test Error: 0.0317\n",
      "Epoch: 033 | Train Loss: 0.01402 |Test Loss: 0.1711 | Test Error: 0.0332\n",
      "Epoch: 034 | Train Loss: 0.02112 |Test Loss: 0.1462 | Test Error: 0.0303\n",
      "Epoch: 035 | Train Loss: 0.0158 |Test Loss: 0.1524 | Test Error: 0.0292\n",
      "Epoch: 036 | Train Loss: 0.01243 |Test Loss: 0.1646 | Test Error: 0.0309\n",
      "Epoch: 037 | Train Loss: 0.01498 |Test Loss: 0.1725 | Test Error: 0.0324\n",
      "Epoch: 038 | Train Loss: 0.0165 |Test Loss: 0.1651 | Test Error: 0.031\n",
      "Epoch: 039 | Train Loss: 0.01521 |Test Loss: 0.1734 | Test Error: 0.0322\n",
      "Epoch: 040 | Train Loss: 0.0106 |Test Loss: 0.1834 | Test Error: 0.0318\n",
      "Epoch: 041 | Train Loss: 0.01143 |Test Loss: 0.1743 | Test Error: 0.0321\n",
      "Epoch: 042 | Train Loss: 0.01297 |Test Loss: 0.172 | Test Error: 0.0302\n",
      "Epoch: 043 | Train Loss: 0.02084 |Test Loss: 0.1653 | Test Error: 0.0329\n",
      "Epoch: 044 | Train Loss: 0.01053 |Test Loss: 0.183 | Test Error: 0.0322\n",
      "Epoch: 045 | Train Loss: 0.01559 |Test Loss: 0.1759 | Test Error: 0.0344\n",
      "Epoch: 046 | Train Loss: 0.01855 |Test Loss: 0.1592 | Test Error: 0.0321\n",
      "Epoch: 047 | Train Loss: 0.008814 |Test Loss: 0.1766 | Test Error: 0.0293\n",
      "Epoch: 048 | Train Loss: 0.01247 |Test Loss: 0.1759 | Test Error: 0.0304\n",
      "Epoch: 049 | Train Loss: 0.009596 |Test Loss: 0.1792 | Test Error: 0.0294\n",
      "Epoch: 050 | Train Loss: 0.01184 |Test Loss: 0.1985 | Test Error: 0.0327\n",
      "Epoch: 051 | Train Loss: 0.01136 |Test Loss: 0.185 | Test Error: 0.0321\n",
      "Epoch: 052 | Train Loss: 0.007196 |Test Loss: 0.2086 | Test Error: 0.0322\n",
      "Epoch: 053 | Train Loss: 0.01556 |Test Loss: 0.1793 | Test Error: 0.0309\n",
      "Epoch: 054 | Train Loss: 0.008205 |Test Loss: 0.1782 | Test Error: 0.0312\n",
      "Epoch: 055 | Train Loss: 0.01045 |Test Loss: 0.1748 | Test Error: 0.0305\n",
      "Epoch: 056 | Train Loss: 0.00843 |Test Loss: 0.1845 | Test Error: 0.0306\n",
      "Epoch: 057 | Train Loss: 0.007285 |Test Loss: 0.1866 | Test Error: 0.0295\n",
      "Epoch: 058 | Train Loss: 0.008254 |Test Loss: 0.1936 | Test Error: 0.0305\n",
      "Epoch: 059 | Train Loss: 0.01282 |Test Loss: 0.1808 | Test Error: 0.0315\n",
      "Epoch: 060 | Train Loss: 0.01405 |Test Loss: 0.1589 | Test Error: 0.0287\n",
      "Epoch: 061 | Train Loss: 0.00561 |Test Loss: 0.1805 | Test Error: 0.0299\n",
      "Epoch: 062 | Train Loss: 0.007941 |Test Loss: 0.1852 | Test Error: 0.0334\n",
      "Epoch: 063 | Train Loss: 0.01479 |Test Loss: 0.1608 | Test Error: 0.028\n",
      "Epoch: 064 | Train Loss: 0.006176 |Test Loss: 0.173 | Test Error: 0.0279\n",
      "Epoch: 065 | Train Loss: 0.006569 |Test Loss: 0.2111 | Test Error: 0.0313\n",
      "Epoch: 066 | Train Loss: 0.005988 |Test Loss: 0.1757 | Test Error: 0.0281\n",
      "Epoch: 067 | Train Loss: 0.01384 |Test Loss: 0.1787 | Test Error: 0.033\n",
      "Epoch: 068 | Train Loss: 0.00773 |Test Loss: 0.1673 | Test Error: 0.0292\n",
      "Epoch: 069 | Train Loss: 0.005398 |Test Loss: 0.1756 | Test Error: 0.0267\n",
      "Epoch: 070 | Train Loss: 0.009199 |Test Loss: 0.1698 | Test Error: 0.0273\n",
      "Epoch: 071 | Train Loss: 0.009721 |Test Loss: 0.1671 | Test Error: 0.0266\n",
      "Epoch: 072 | Train Loss: 0.006789 |Test Loss: 0.1695 | Test Error: 0.0292\n",
      "Epoch: 073 | Train Loss: 0.006255 |Test Loss: 0.1735 | Test Error: 0.0274\n",
      "Epoch: 074 | Train Loss: 0.009552 |Test Loss: 0.1662 | Test Error: 0.0279\n",
      "Epoch: 075 | Train Loss: 0.006113 |Test Loss: 0.1919 | Test Error: 0.0291\n",
      "Epoch: 076 | Train Loss: 0.007463 |Test Loss: 0.1623 | Test Error: 0.027\n",
      "Epoch: 077 | Train Loss: 0.006821 |Test Loss: 0.1744 | Test Error: 0.0279\n",
      "Epoch: 078 | Train Loss: 0.007142 |Test Loss: 0.1871 | Test Error: 0.0292\n",
      "Epoch: 079 | Train Loss: 0.009093 |Test Loss: 0.1861 | Test Error: 0.0291\n",
      "Epoch: 080 | Train Loss: 0.007845 |Test Loss: 0.1869 | Test Error: 0.0288\n",
      "Epoch: 081 | Train Loss: 0.00502 |Test Loss: 0.1798 | Test Error: 0.0265\n",
      "Epoch: 082 | Train Loss: 0.008633 |Test Loss: 0.2384 | Test Error: 0.0383\n",
      "Epoch: 083 | Train Loss: 0.01017 |Test Loss: 0.1637 | Test Error: 0.0265\n",
      "Epoch: 084 | Train Loss: 0.007081 |Test Loss: 0.1765 | Test Error: 0.03\n",
      "Epoch: 085 | Train Loss: 0.008224 |Test Loss: 0.1735 | Test Error: 0.0284\n",
      "Epoch: 086 | Train Loss: 0.006472 |Test Loss: 0.1572 | Test Error: 0.025\n",
      "Epoch: 087 | Train Loss: 0.003825 |Test Loss: 0.199 | Test Error: 0.0287\n",
      "Epoch: 088 | Train Loss: 0.007736 |Test Loss: 0.1834 | Test Error: 0.0327\n",
      "Epoch: 089 | Train Loss: 0.007731 |Test Loss: 0.1934 | Test Error: 0.0286\n",
      "Epoch: 090 | Train Loss: 0.008029 |Test Loss: 0.1698 | Test Error: 0.0289\n",
      "Epoch: 091 | Train Loss: 0.003368 |Test Loss: 0.1751 | Test Error: 0.0263\n",
      "Epoch: 092 | Train Loss: 0.009742 |Test Loss: 0.181 | Test Error: 0.0322\n",
      "Epoch: 093 | Train Loss: 0.008486 |Test Loss: 0.147 | Test Error: 0.0253\n",
      "Epoch: 094 | Train Loss: 0.00428 |Test Loss: 0.1748 | Test Error: 0.0263\n",
      "Epoch: 095 | Train Loss: 0.005418 |Test Loss: 0.169 | Test Error: 0.0264\n",
      "Epoch: 096 | Train Loss: 0.00223 |Test Loss: 0.1741 | Test Error: 0.0262\n",
      "Epoch: 097 | Train Loss: 0.00141 |Test Loss: 0.1806 | Test Error: 0.025\n",
      "Epoch: 098 | Train Loss: 0.001768 |Test Loss: 0.2036 | Test Error: 0.0287\n",
      "Epoch: 099 | Train Loss: 0.009611 |Test Loss: 0.1682 | Test Error: 0.0284\n",
      "Epoch: 100 | Train Loss: 0.01153 |Test Loss: 0.1684 | Test Error: 0.0274\n"
     ]
    }
   ],
   "source": [
    "# depth = 10 parameters\n",
    "depth = 10\n",
    "lr = 0.001\n",
    "\n",
    "# initialize model\n",
    "net = Network(dim,nclass,width,depth)\n",
    "#send to GPU\n",
    "net = net.to(device)\n",
    "#train it\n",
    "D10 = train_model(net,batch_size,lr,num_epochs,train_set_mnist,test_set_mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training loss for depth = 1: 0.00141, best test loss for depth = 1: 0.1388\n"
     ]
    }
   ],
   "source": [
    "D10_min_train = np.argmin(D10[0])\n",
    "D10_min_test = np.argmin(D10[1])\n",
    "print(f\"Best training loss for depth = 1: {D10[0][D10_min_train]:.04}, best test loss for depth = 1: {D10[1][D10_min_test]:.04}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x122df2690>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(D10[0],label = \"train\")\n",
    "plt.plot(D10[1],label = \"test\")\n",
    "plt.plot(D10[2],label = \"accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are summarized in the table below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•’â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚   Depth â”‚   Best train loss â”‚   Best test loss â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚       1 â”‚       0.00156027  â”‚        0.078802  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚       5 â”‚       1.07095e-05 â”‚        0.0984787 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚      10 â”‚       0.00140997  â”‚        0.1388    â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "loss_list = [[1,D1[0][D1_min_train],D1[1][D1_min_test]],[5,D5[0][D5_min_train],D5[1][D5_min_test]],[10,D10[0][D10_min_train],D10[1][D10_min_test]]]\n",
    "table = [[\"Depth\", \"Best train loss\", \"Best test loss\"]] + loss_list\n",
    "print(tabulate(table,headers = \"firstrow\",tablefmt = \"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show that in all 3 cases, the model reached convergence for the training set, and began to overfit, which was evident since the test loss began to rise in each case.\n",
    "\n",
    "The table shows that, the deeper (5 and 10 layer) neural networks did infact lead to more overfitting on the data. They obtained smaller best train losses than the shallower (1 layer) network, but also had higher values for the best test loss. This is especially evident in the 10 layer network which had nearly double the best test loss than the 1 layer network.\n",
    "\n",
    "Testing did show that a smaller learning rate was required to train the deeper network. Higher learning rates simply lead to the model getting stuck at certain loss values for both testing and training. The layer 1 network had better performance with a slightly higher learning rate, however it was important to keep this hyperparamter consistent across this experiment since we only wanted to change the depth.\n",
    "\n",
    "In conclusion, the deeper networks appear to be worse at generalising to unseen data than the shallow model, they are more prone to overfitting the data, due to having more parameters to play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for task 7, we will analyse the test and train errors as a function of width for a 1 layer network. We shall again choose dim = 784, nclass = 10, and a learning rate of 0.001. We shall set the batch size to 1024 and vary $\\text{width} \\in \\{4,8,16,32,64,128,256,512,1024\\}$, and run each model for 100 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 7 ###\n",
    "\n",
    "#General MNIST Hyperparameters\n",
    "dim = 784\n",
    "nclass = 10\n",
    "\n",
    "# Task 7 fixed hyperparameters\n",
    "lr = 0.001\n",
    "batch_size = 1024\n",
    "num_epochs = 100\n",
    "depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train Loss: 2.086 |Test Loss: 1.872 | Test Error: 0.6617\n",
      "Epoch: 002 | Train Loss: 1.719 |Test Loss: 1.537 | Test Error: 0.4694\n",
      "Epoch: 003 | Train Loss: 1.414 |Test Loss: 1.256 | Test Error: 0.3862\n",
      "Epoch: 004 | Train Loss: 1.168 |Test Loss: 1.044 | Test Error: 0.3138\n",
      "Epoch: 005 | Train Loss: 0.9944 |Test Loss: 0.9097 | Test Error: 0.2789\n",
      "Epoch: 006 | Train Loss: 0.8842 |Test Loss: 0.8247 | Test Error: 0.2579\n",
      "Epoch: 007 | Train Loss: 0.8148 |Test Loss: 0.7719 | Test Error: 0.243\n",
      "Epoch: 008 | Train Loss: 0.7689 |Test Loss: 0.7346 | Test Error: 0.23\n",
      "Epoch: 009 | Train Loss: 0.736 |Test Loss: 0.7086 | Test Error: 0.2208\n",
      "Epoch: 010 | Train Loss: 0.7102 |Test Loss: 0.687 | Test Error: 0.2095\n",
      "Epoch: 011 | Train Loss: 0.689 |Test Loss: 0.6677 | Test Error: 0.2031\n",
      "Epoch: 012 | Train Loss: 0.6704 |Test Loss: 0.6537 | Test Error: 0.1977\n",
      "Epoch: 013 | Train Loss: 0.6542 |Test Loss: 0.6387 | Test Error: 0.1916\n",
      "Epoch: 014 | Train Loss: 0.6397 |Test Loss: 0.6279 | Test Error: 0.1885\n",
      "Epoch: 015 | Train Loss: 0.6275 |Test Loss: 0.6162 | Test Error: 0.1826\n",
      "Epoch: 016 | Train Loss: 0.6166 |Test Loss: 0.6062 | Test Error: 0.1791\n",
      "Epoch: 017 | Train Loss: 0.6065 |Test Loss: 0.5987 | Test Error: 0.176\n",
      "Epoch: 018 | Train Loss: 0.5982 |Test Loss: 0.5928 | Test Error: 0.172\n",
      "Epoch: 019 | Train Loss: 0.5909 |Test Loss: 0.5841 | Test Error: 0.1706\n",
      "Epoch: 020 | Train Loss: 0.5844 |Test Loss: 0.5783 | Test Error: 0.1683\n",
      "Epoch: 021 | Train Loss: 0.5786 |Test Loss: 0.5733 | Test Error: 0.1686\n",
      "Epoch: 022 | Train Loss: 0.5731 |Test Loss: 0.5695 | Test Error: 0.1665\n",
      "Epoch: 023 | Train Loss: 0.5682 |Test Loss: 0.5643 | Test Error: 0.165\n",
      "Epoch: 024 | Train Loss: 0.5633 |Test Loss: 0.5617 | Test Error: 0.1638\n",
      "Epoch: 025 | Train Loss: 0.5591 |Test Loss: 0.5561 | Test Error: 0.1635\n",
      "Epoch: 026 | Train Loss: 0.5553 |Test Loss: 0.5541 | Test Error: 0.1614\n",
      "Epoch: 027 | Train Loss: 0.5515 |Test Loss: 0.5503 | Test Error: 0.1612\n",
      "Epoch: 028 | Train Loss: 0.5477 |Test Loss: 0.5465 | Test Error: 0.162\n",
      "Epoch: 029 | Train Loss: 0.5447 |Test Loss: 0.545 | Test Error: 0.1595\n",
      "Epoch: 030 | Train Loss: 0.5415 |Test Loss: 0.5408 | Test Error: 0.1591\n",
      "Epoch: 031 | Train Loss: 0.539 |Test Loss: 0.5388 | Test Error: 0.1587\n",
      "Epoch: 032 | Train Loss: 0.5362 |Test Loss: 0.5373 | Test Error: 0.157\n",
      "Epoch: 033 | Train Loss: 0.5335 |Test Loss: 0.5349 | Test Error: 0.1568\n",
      "Epoch: 034 | Train Loss: 0.5305 |Test Loss: 0.5318 | Test Error: 0.1561\n",
      "Epoch: 035 | Train Loss: 0.5285 |Test Loss: 0.5299 | Test Error: 0.1558\n",
      "Epoch: 036 | Train Loss: 0.5255 |Test Loss: 0.5284 | Test Error: 0.1554\n",
      "Epoch: 037 | Train Loss: 0.5235 |Test Loss: 0.5255 | Test Error: 0.1536\n",
      "Epoch: 038 | Train Loss: 0.5217 |Test Loss: 0.525 | Test Error: 0.1527\n",
      "Epoch: 039 | Train Loss: 0.5192 |Test Loss: 0.5234 | Test Error: 0.153\n",
      "Epoch: 040 | Train Loss: 0.517 |Test Loss: 0.5207 | Test Error: 0.1506\n",
      "Epoch: 041 | Train Loss: 0.5151 |Test Loss: 0.5182 | Test Error: 0.1516\n",
      "Epoch: 042 | Train Loss: 0.5131 |Test Loss: 0.518 | Test Error: 0.1519\n",
      "Epoch: 043 | Train Loss: 0.5115 |Test Loss: 0.5157 | Test Error: 0.1493\n",
      "Epoch: 044 | Train Loss: 0.5096 |Test Loss: 0.5142 | Test Error: 0.149\n",
      "Epoch: 045 | Train Loss: 0.5081 |Test Loss: 0.5132 | Test Error: 0.1493\n",
      "Epoch: 046 | Train Loss: 0.506 |Test Loss: 0.5117 | Test Error: 0.1487\n",
      "Epoch: 047 | Train Loss: 0.505 |Test Loss: 0.5098 | Test Error: 0.1475\n",
      "Epoch: 048 | Train Loss: 0.5032 |Test Loss: 0.5093 | Test Error: 0.147\n",
      "Epoch: 049 | Train Loss: 0.5018 |Test Loss: 0.5084 | Test Error: 0.1476\n",
      "Epoch: 050 | Train Loss: 0.5002 |Test Loss: 0.5067 | Test Error: 0.1463\n",
      "Epoch: 051 | Train Loss: 0.4988 |Test Loss: 0.5074 | Test Error: 0.1458\n",
      "Epoch: 052 | Train Loss: 0.4977 |Test Loss: 0.5049 | Test Error: 0.1452\n",
      "Epoch: 053 | Train Loss: 0.4965 |Test Loss: 0.5047 | Test Error: 0.1465\n",
      "Epoch: 054 | Train Loss: 0.4953 |Test Loss: 0.503 | Test Error: 0.1444\n",
      "Epoch: 055 | Train Loss: 0.4938 |Test Loss: 0.5023 | Test Error: 0.144\n",
      "Epoch: 056 | Train Loss: 0.4929 |Test Loss: 0.5028 | Test Error: 0.1453\n",
      "Epoch: 057 | Train Loss: 0.4916 |Test Loss: 0.5019 | Test Error: 0.1445\n",
      "Epoch: 058 | Train Loss: 0.4906 |Test Loss: 0.5004 | Test Error: 0.1444\n",
      "Epoch: 059 | Train Loss: 0.4898 |Test Loss: 0.4999 | Test Error: 0.1456\n",
      "Epoch: 060 | Train Loss: 0.4886 |Test Loss: 0.4999 | Test Error: 0.1428\n",
      "Epoch: 061 | Train Loss: 0.4878 |Test Loss: 0.4999 | Test Error: 0.1424\n",
      "Epoch: 062 | Train Loss: 0.4867 |Test Loss: 0.4979 | Test Error: 0.1431\n",
      "Epoch: 063 | Train Loss: 0.4859 |Test Loss: 0.4973 | Test Error: 0.1442\n",
      "Epoch: 064 | Train Loss: 0.4846 |Test Loss: 0.4967 | Test Error: 0.142\n",
      "Epoch: 065 | Train Loss: 0.4839 |Test Loss: 0.4959 | Test Error: 0.1418\n",
      "Epoch: 066 | Train Loss: 0.4829 |Test Loss: 0.4959 | Test Error: 0.1414\n",
      "Epoch: 067 | Train Loss: 0.4823 |Test Loss: 0.4956 | Test Error: 0.1423\n",
      "Epoch: 068 | Train Loss: 0.4813 |Test Loss: 0.4946 | Test Error: 0.141\n",
      "Epoch: 069 | Train Loss: 0.4807 |Test Loss: 0.4947 | Test Error: 0.141\n",
      "Epoch: 070 | Train Loss: 0.4797 |Test Loss: 0.4934 | Test Error: 0.1414\n",
      "Epoch: 071 | Train Loss: 0.4791 |Test Loss: 0.4929 | Test Error: 0.1423\n",
      "Epoch: 072 | Train Loss: 0.4779 |Test Loss: 0.4923 | Test Error: 0.1404\n",
      "Epoch: 073 | Train Loss: 0.4776 |Test Loss: 0.4926 | Test Error: 0.1402\n",
      "Epoch: 074 | Train Loss: 0.4766 |Test Loss: 0.4907 | Test Error: 0.1401\n",
      "Epoch: 075 | Train Loss: 0.4759 |Test Loss: 0.4909 | Test Error: 0.1393\n",
      "Epoch: 076 | Train Loss: 0.4752 |Test Loss: 0.4906 | Test Error: 0.1398\n",
      "Epoch: 077 | Train Loss: 0.4746 |Test Loss: 0.4893 | Test Error: 0.1393\n",
      "Epoch: 078 | Train Loss: 0.4743 |Test Loss: 0.4892 | Test Error: 0.1389\n",
      "Epoch: 079 | Train Loss: 0.4733 |Test Loss: 0.4891 | Test Error: 0.1387\n",
      "Epoch: 080 | Train Loss: 0.4724 |Test Loss: 0.4887 | Test Error: 0.1382\n",
      "Epoch: 081 | Train Loss: 0.4722 |Test Loss: 0.488 | Test Error: 0.1379\n",
      "Epoch: 082 | Train Loss: 0.4711 |Test Loss: 0.4871 | Test Error: 0.1376\n",
      "Epoch: 083 | Train Loss: 0.4706 |Test Loss: 0.488 | Test Error: 0.137\n",
      "Epoch: 084 | Train Loss: 0.47 |Test Loss: 0.4861 | Test Error: 0.1367\n",
      "Epoch: 085 | Train Loss: 0.4691 |Test Loss: 0.4872 | Test Error: 0.1372\n",
      "Epoch: 086 | Train Loss: 0.4688 |Test Loss: 0.4848 | Test Error: 0.1358\n",
      "Epoch: 087 | Train Loss: 0.4678 |Test Loss: 0.4839 | Test Error: 0.1367\n",
      "Epoch: 088 | Train Loss: 0.4675 |Test Loss: 0.4841 | Test Error: 0.1351\n",
      "Epoch: 089 | Train Loss: 0.4668 |Test Loss: 0.4839 | Test Error: 0.1361\n",
      "Epoch: 090 | Train Loss: 0.466 |Test Loss: 0.4839 | Test Error: 0.1346\n",
      "Epoch: 091 | Train Loss: 0.4655 |Test Loss: 0.4823 | Test Error: 0.1356\n",
      "Epoch: 092 | Train Loss: 0.4647 |Test Loss: 0.4829 | Test Error: 0.1354\n",
      "Epoch: 093 | Train Loss: 0.4638 |Test Loss: 0.4817 | Test Error: 0.134\n",
      "Epoch: 094 | Train Loss: 0.4628 |Test Loss: 0.4801 | Test Error: 0.1337\n",
      "Epoch: 095 | Train Loss: 0.4622 |Test Loss: 0.4808 | Test Error: 0.1345\n",
      "Epoch: 096 | Train Loss: 0.4612 |Test Loss: 0.4795 | Test Error: 0.1338\n",
      "Epoch: 097 | Train Loss: 0.4602 |Test Loss: 0.4779 | Test Error: 0.1335\n",
      "Epoch: 098 | Train Loss: 0.4589 |Test Loss: 0.4776 | Test Error: 0.1325\n",
      "Epoch: 099 | Train Loss: 0.4582 |Test Loss: 0.4769 | Test Error: 0.1327\n",
      "Epoch: 100 | Train Loss: 0.457 |Test Loss: 0.476 | Test Error: 0.1326\n",
      "Epoch: 001 | Train Loss: 1.992 |Test Loss: 1.581 | Test Error: 0.4091\n",
      "Epoch: 002 | Train Loss: 1.28 |Test Loss: 0.9659 | Test Error: 0.2458\n",
      "Epoch: 003 | Train Loss: 0.8398 |Test Loss: 0.6898 | Test Error: 0.1876\n",
      "Epoch: 004 | Train Loss: 0.6422 |Test Loss: 0.5581 | Test Error: 0.1522\n",
      "Epoch: 005 | Train Loss: 0.5391 |Test Loss: 0.4864 | Test Error: 0.1339\n",
      "Epoch: 006 | Train Loss: 0.4788 |Test Loss: 0.4425 | Test Error: 0.1235\n",
      "Epoch: 007 | Train Loss: 0.44 |Test Loss: 0.4136 | Test Error: 0.1166\n",
      "Epoch: 008 | Train Loss: 0.4133 |Test Loss: 0.3948 | Test Error: 0.1114\n",
      "Epoch: 009 | Train Loss: 0.3936 |Test Loss: 0.3791 | Test Error: 0.107\n",
      "Epoch: 010 | Train Loss: 0.3786 |Test Loss: 0.3676 | Test Error: 0.1042\n",
      "Epoch: 011 | Train Loss: 0.3665 |Test Loss: 0.3581 | Test Error: 0.1028\n",
      "Epoch: 012 | Train Loss: 0.3569 |Test Loss: 0.3509 | Test Error: 0.0996\n",
      "Epoch: 013 | Train Loss: 0.3486 |Test Loss: 0.3437 | Test Error: 0.0978\n",
      "Epoch: 014 | Train Loss: 0.3414 |Test Loss: 0.3392 | Test Error: 0.0971\n",
      "Epoch: 015 | Train Loss: 0.3356 |Test Loss: 0.3345 | Test Error: 0.0942\n",
      "Epoch: 016 | Train Loss: 0.3307 |Test Loss: 0.3299 | Test Error: 0.0936\n",
      "Epoch: 017 | Train Loss: 0.3259 |Test Loss: 0.3279 | Test Error: 0.0926\n",
      "Epoch: 018 | Train Loss: 0.3219 |Test Loss: 0.3251 | Test Error: 0.0902\n",
      "Epoch: 019 | Train Loss: 0.3186 |Test Loss: 0.3217 | Test Error: 0.0899\n",
      "Epoch: 020 | Train Loss: 0.3147 |Test Loss: 0.3186 | Test Error: 0.0872\n",
      "Epoch: 021 | Train Loss: 0.3117 |Test Loss: 0.3163 | Test Error: 0.0882\n",
      "Epoch: 022 | Train Loss: 0.3088 |Test Loss: 0.3153 | Test Error: 0.0877\n",
      "Epoch: 023 | Train Loss: 0.3064 |Test Loss: 0.3135 | Test Error: 0.0868\n",
      "Epoch: 024 | Train Loss: 0.3041 |Test Loss: 0.3123 | Test Error: 0.0864\n",
      "Epoch: 025 | Train Loss: 0.3018 |Test Loss: 0.3107 | Test Error: 0.0868\n",
      "Epoch: 026 | Train Loss: 0.2997 |Test Loss: 0.3096 | Test Error: 0.0858\n",
      "Epoch: 027 | Train Loss: 0.2977 |Test Loss: 0.3077 | Test Error: 0.085\n",
      "Epoch: 028 | Train Loss: 0.296 |Test Loss: 0.3058 | Test Error: 0.0848\n",
      "Epoch: 029 | Train Loss: 0.2943 |Test Loss: 0.305 | Test Error: 0.0847\n",
      "Epoch: 030 | Train Loss: 0.2924 |Test Loss: 0.3046 | Test Error: 0.083\n",
      "Epoch: 031 | Train Loss: 0.2908 |Test Loss: 0.3042 | Test Error: 0.0841\n",
      "Epoch: 032 | Train Loss: 0.2893 |Test Loss: 0.3016 | Test Error: 0.0824\n",
      "Epoch: 033 | Train Loss: 0.2879 |Test Loss: 0.3006 | Test Error: 0.0827\n",
      "Epoch: 034 | Train Loss: 0.2863 |Test Loss: 0.3013 | Test Error: 0.0813\n",
      "Epoch: 035 | Train Loss: 0.2849 |Test Loss: 0.3004 | Test Error: 0.0819\n",
      "Epoch: 036 | Train Loss: 0.2835 |Test Loss: 0.2991 | Test Error: 0.0818\n",
      "Epoch: 037 | Train Loss: 0.2827 |Test Loss: 0.2975 | Test Error: 0.0814\n",
      "Epoch: 038 | Train Loss: 0.2814 |Test Loss: 0.2986 | Test Error: 0.0827\n",
      "Epoch: 039 | Train Loss: 0.2801 |Test Loss: 0.2976 | Test Error: 0.0825\n",
      "Epoch: 040 | Train Loss: 0.279 |Test Loss: 0.2949 | Test Error: 0.0796\n",
      "Epoch: 041 | Train Loss: 0.2779 |Test Loss: 0.2956 | Test Error: 0.0797\n",
      "Epoch: 042 | Train Loss: 0.2766 |Test Loss: 0.2938 | Test Error: 0.0806\n",
      "Epoch: 043 | Train Loss: 0.2763 |Test Loss: 0.294 | Test Error: 0.0812\n",
      "Epoch: 044 | Train Loss: 0.2749 |Test Loss: 0.2927 | Test Error: 0.0789\n",
      "Epoch: 045 | Train Loss: 0.2741 |Test Loss: 0.2924 | Test Error: 0.0794\n",
      "Epoch: 046 | Train Loss: 0.2731 |Test Loss: 0.2915 | Test Error: 0.0797\n",
      "Epoch: 047 | Train Loss: 0.2721 |Test Loss: 0.2904 | Test Error: 0.0796\n",
      "Epoch: 048 | Train Loss: 0.2712 |Test Loss: 0.2916 | Test Error: 0.0797\n",
      "Epoch: 049 | Train Loss: 0.2702 |Test Loss: 0.2897 | Test Error: 0.0793\n",
      "Epoch: 050 | Train Loss: 0.2694 |Test Loss: 0.2896 | Test Error: 0.0807\n",
      "Epoch: 051 | Train Loss: 0.2685 |Test Loss: 0.289 | Test Error: 0.0784\n",
      "Epoch: 052 | Train Loss: 0.2676 |Test Loss: 0.2902 | Test Error: 0.0785\n",
      "Epoch: 053 | Train Loss: 0.2672 |Test Loss: 0.2891 | Test Error: 0.0791\n",
      "Epoch: 054 | Train Loss: 0.2666 |Test Loss: 0.2911 | Test Error: 0.0803\n",
      "Epoch: 055 | Train Loss: 0.2659 |Test Loss: 0.288 | Test Error: 0.0789\n",
      "Epoch: 056 | Train Loss: 0.2647 |Test Loss: 0.289 | Test Error: 0.0788\n",
      "Epoch: 057 | Train Loss: 0.2642 |Test Loss: 0.2863 | Test Error: 0.0782\n",
      "Epoch: 058 | Train Loss: 0.2637 |Test Loss: 0.288 | Test Error: 0.0788\n",
      "Epoch: 059 | Train Loss: 0.263 |Test Loss: 0.2854 | Test Error: 0.0776\n",
      "Epoch: 060 | Train Loss: 0.2625 |Test Loss: 0.2864 | Test Error: 0.0776\n",
      "Epoch: 061 | Train Loss: 0.2616 |Test Loss: 0.2847 | Test Error: 0.0781\n",
      "Epoch: 062 | Train Loss: 0.2613 |Test Loss: 0.286 | Test Error: 0.077\n",
      "Epoch: 063 | Train Loss: 0.2605 |Test Loss: 0.2867 | Test Error: 0.0772\n",
      "Epoch: 064 | Train Loss: 0.2598 |Test Loss: 0.2865 | Test Error: 0.0774\n",
      "Epoch: 065 | Train Loss: 0.2593 |Test Loss: 0.2861 | Test Error: 0.0771\n",
      "Epoch: 066 | Train Loss: 0.2589 |Test Loss: 0.2869 | Test Error: 0.078\n",
      "Epoch: 067 | Train Loss: 0.2583 |Test Loss: 0.2835 | Test Error: 0.0765\n",
      "Epoch: 068 | Train Loss: 0.2577 |Test Loss: 0.2835 | Test Error: 0.0768\n",
      "Epoch: 069 | Train Loss: 0.2571 |Test Loss: 0.283 | Test Error: 0.0765\n",
      "Epoch: 070 | Train Loss: 0.2565 |Test Loss: 0.2833 | Test Error: 0.0773\n",
      "Epoch: 071 | Train Loss: 0.2561 |Test Loss: 0.2843 | Test Error: 0.0774\n",
      "Epoch: 072 | Train Loss: 0.2555 |Test Loss: 0.2835 | Test Error: 0.077\n",
      "Epoch: 073 | Train Loss: 0.2551 |Test Loss: 0.2833 | Test Error: 0.0761\n",
      "Epoch: 074 | Train Loss: 0.2544 |Test Loss: 0.2838 | Test Error: 0.0755\n",
      "Epoch: 075 | Train Loss: 0.2544 |Test Loss: 0.2835 | Test Error: 0.0773\n",
      "Epoch: 076 | Train Loss: 0.2537 |Test Loss: 0.2838 | Test Error: 0.0772\n",
      "Epoch: 077 | Train Loss: 0.2535 |Test Loss: 0.2828 | Test Error: 0.0766\n",
      "Epoch: 078 | Train Loss: 0.2526 |Test Loss: 0.2836 | Test Error: 0.0775\n",
      "Epoch: 079 | Train Loss: 0.2524 |Test Loss: 0.2821 | Test Error: 0.0769\n",
      "Epoch: 080 | Train Loss: 0.2519 |Test Loss: 0.2819 | Test Error: 0.0755\n",
      "Epoch: 081 | Train Loss: 0.2518 |Test Loss: 0.2815 | Test Error: 0.0759\n",
      "Epoch: 082 | Train Loss: 0.2512 |Test Loss: 0.2818 | Test Error: 0.077\n",
      "Epoch: 083 | Train Loss: 0.2509 |Test Loss: 0.2837 | Test Error: 0.0769\n",
      "Epoch: 084 | Train Loss: 0.2509 |Test Loss: 0.2824 | Test Error: 0.0761\n",
      "Epoch: 085 | Train Loss: 0.2502 |Test Loss: 0.2813 | Test Error: 0.0753\n",
      "Epoch: 086 | Train Loss: 0.2498 |Test Loss: 0.2819 | Test Error: 0.0765\n",
      "Epoch: 087 | Train Loss: 0.2493 |Test Loss: 0.2824 | Test Error: 0.0766\n",
      "Epoch: 088 | Train Loss: 0.2494 |Test Loss: 0.2821 | Test Error: 0.0759\n",
      "Epoch: 089 | Train Loss: 0.2487 |Test Loss: 0.2806 | Test Error: 0.0759\n",
      "Epoch: 090 | Train Loss: 0.2485 |Test Loss: 0.2811 | Test Error: 0.0759\n",
      "Epoch: 091 | Train Loss: 0.2479 |Test Loss: 0.2811 | Test Error: 0.0762\n",
      "Epoch: 092 | Train Loss: 0.2477 |Test Loss: 0.2826 | Test Error: 0.0751\n",
      "Epoch: 093 | Train Loss: 0.2476 |Test Loss: 0.283 | Test Error: 0.0756\n",
      "Epoch: 094 | Train Loss: 0.2473 |Test Loss: 0.2811 | Test Error: 0.076\n",
      "Epoch: 095 | Train Loss: 0.2471 |Test Loss: 0.2795 | Test Error: 0.0759\n",
      "Epoch: 096 | Train Loss: 0.2463 |Test Loss: 0.2814 | Test Error: 0.076\n",
      "Epoch: 097 | Train Loss: 0.2466 |Test Loss: 0.2821 | Test Error: 0.0764\n",
      "Epoch: 098 | Train Loss: 0.2462 |Test Loss: 0.2825 | Test Error: 0.0765\n",
      "Epoch: 099 | Train Loss: 0.2457 |Test Loss: 0.2813 | Test Error: 0.076\n",
      "Epoch: 100 | Train Loss: 0.2452 |Test Loss: 0.2822 | Test Error: 0.077\n",
      "Epoch: 001 | Train Loss: 1.635 |Test Loss: 1.008 | Test Error: 0.1986\n",
      "Epoch: 002 | Train Loss: 0.7675 |Test Loss: 0.5735 | Test Error: 0.1353\n",
      "Epoch: 003 | Train Loss: 0.5126 |Test Loss: 0.4345 | Test Error: 0.1121\n",
      "Epoch: 004 | Train Loss: 0.4143 |Test Loss: 0.37 | Test Error: 0.0975\n",
      "Epoch: 005 | Train Loss: 0.3635 |Test Loss: 0.3332 | Test Error: 0.091\n",
      "Epoch: 006 | Train Loss: 0.3324 |Test Loss: 0.3089 | Test Error: 0.0848\n",
      "Epoch: 007 | Train Loss: 0.311 |Test Loss: 0.2926 | Test Error: 0.0819\n",
      "Epoch: 008 | Train Loss: 0.2953 |Test Loss: 0.28 | Test Error: 0.0803\n",
      "Epoch: 009 | Train Loss: 0.2827 |Test Loss: 0.2719 | Test Error: 0.0782\n",
      "Epoch: 010 | Train Loss: 0.2726 |Test Loss: 0.2642 | Test Error: 0.0732\n",
      "Epoch: 011 | Train Loss: 0.2643 |Test Loss: 0.2581 | Test Error: 0.0735\n",
      "Epoch: 012 | Train Loss: 0.257 |Test Loss: 0.2523 | Test Error: 0.0717\n",
      "Epoch: 013 | Train Loss: 0.2504 |Test Loss: 0.2492 | Test Error: 0.0701\n",
      "Epoch: 014 | Train Loss: 0.2444 |Test Loss: 0.2459 | Test Error: 0.0705\n",
      "Epoch: 015 | Train Loss: 0.2391 |Test Loss: 0.2404 | Test Error: 0.0677\n",
      "Epoch: 016 | Train Loss: 0.2339 |Test Loss: 0.2377 | Test Error: 0.068\n",
      "Epoch: 017 | Train Loss: 0.2291 |Test Loss: 0.2345 | Test Error: 0.0671\n",
      "Epoch: 018 | Train Loss: 0.2248 |Test Loss: 0.2304 | Test Error: 0.0651\n",
      "Epoch: 019 | Train Loss: 0.221 |Test Loss: 0.2291 | Test Error: 0.0653\n",
      "Epoch: 020 | Train Loss: 0.217 |Test Loss: 0.2254 | Test Error: 0.0643\n",
      "Epoch: 021 | Train Loss: 0.2136 |Test Loss: 0.2226 | Test Error: 0.0641\n",
      "Epoch: 022 | Train Loss: 0.2101 |Test Loss: 0.2201 | Test Error: 0.0619\n",
      "Epoch: 023 | Train Loss: 0.2072 |Test Loss: 0.2187 | Test Error: 0.0614\n",
      "Epoch: 024 | Train Loss: 0.2041 |Test Loss: 0.2167 | Test Error: 0.061\n",
      "Epoch: 025 | Train Loss: 0.2013 |Test Loss: 0.2145 | Test Error: 0.0614\n",
      "Epoch: 026 | Train Loss: 0.1984 |Test Loss: 0.2123 | Test Error: 0.06\n",
      "Epoch: 027 | Train Loss: 0.196 |Test Loss: 0.2096 | Test Error: 0.0601\n",
      "Epoch: 028 | Train Loss: 0.1935 |Test Loss: 0.2083 | Test Error: 0.0602\n",
      "Epoch: 029 | Train Loss: 0.1912 |Test Loss: 0.2082 | Test Error: 0.0584\n",
      "Epoch: 030 | Train Loss: 0.1884 |Test Loss: 0.2051 | Test Error: 0.0582\n",
      "Epoch: 031 | Train Loss: 0.186 |Test Loss: 0.2033 | Test Error: 0.0584\n",
      "Epoch: 032 | Train Loss: 0.1839 |Test Loss: 0.2018 | Test Error: 0.0575\n",
      "Epoch: 033 | Train Loss: 0.1814 |Test Loss: 0.2002 | Test Error: 0.0566\n",
      "Epoch: 034 | Train Loss: 0.1793 |Test Loss: 0.1989 | Test Error: 0.0569\n",
      "Epoch: 035 | Train Loss: 0.1774 |Test Loss: 0.1986 | Test Error: 0.0566\n",
      "Epoch: 036 | Train Loss: 0.1751 |Test Loss: 0.1962 | Test Error: 0.0562\n",
      "Epoch: 037 | Train Loss: 0.1733 |Test Loss: 0.1957 | Test Error: 0.0556\n",
      "Epoch: 038 | Train Loss: 0.1716 |Test Loss: 0.1948 | Test Error: 0.0553\n",
      "Epoch: 039 | Train Loss: 0.1699 |Test Loss: 0.192 | Test Error: 0.0559\n",
      "Epoch: 040 | Train Loss: 0.168 |Test Loss: 0.1928 | Test Error: 0.0548\n",
      "Epoch: 041 | Train Loss: 0.166 |Test Loss: 0.1914 | Test Error: 0.0553\n",
      "Epoch: 042 | Train Loss: 0.1643 |Test Loss: 0.1888 | Test Error: 0.0549\n",
      "Epoch: 043 | Train Loss: 0.1626 |Test Loss: 0.188 | Test Error: 0.0537\n",
      "Epoch: 044 | Train Loss: 0.1615 |Test Loss: 0.1884 | Test Error: 0.0542\n",
      "Epoch: 045 | Train Loss: 0.1598 |Test Loss: 0.1862 | Test Error: 0.0536\n",
      "Epoch: 046 | Train Loss: 0.1585 |Test Loss: 0.1849 | Test Error: 0.0528\n",
      "Epoch: 047 | Train Loss: 0.1567 |Test Loss: 0.185 | Test Error: 0.0526\n",
      "Epoch: 048 | Train Loss: 0.1553 |Test Loss: 0.1839 | Test Error: 0.0525\n",
      "Epoch: 049 | Train Loss: 0.1545 |Test Loss: 0.1834 | Test Error: 0.0525\n",
      "Epoch: 050 | Train Loss: 0.1529 |Test Loss: 0.1816 | Test Error: 0.051\n",
      "Epoch: 051 | Train Loss: 0.1517 |Test Loss: 0.1809 | Test Error: 0.0522\n",
      "Epoch: 052 | Train Loss: 0.1502 |Test Loss: 0.1807 | Test Error: 0.0515\n",
      "Epoch: 053 | Train Loss: 0.1491 |Test Loss: 0.1803 | Test Error: 0.052\n",
      "Epoch: 054 | Train Loss: 0.1484 |Test Loss: 0.1811 | Test Error: 0.0525\n",
      "Epoch: 055 | Train Loss: 0.1471 |Test Loss: 0.1785 | Test Error: 0.051\n",
      "Epoch: 056 | Train Loss: 0.1459 |Test Loss: 0.1778 | Test Error: 0.0506\n",
      "Epoch: 057 | Train Loss: 0.1446 |Test Loss: 0.1774 | Test Error: 0.0507\n",
      "Epoch: 058 | Train Loss: 0.1438 |Test Loss: 0.1768 | Test Error: 0.05\n",
      "Epoch: 059 | Train Loss: 0.1426 |Test Loss: 0.1751 | Test Error: 0.0505\n",
      "Epoch: 060 | Train Loss: 0.1417 |Test Loss: 0.175 | Test Error: 0.0498\n",
      "Epoch: 061 | Train Loss: 0.1406 |Test Loss: 0.1756 | Test Error: 0.0504\n",
      "Epoch: 062 | Train Loss: 0.1396 |Test Loss: 0.1752 | Test Error: 0.0504\n",
      "Epoch: 063 | Train Loss: 0.1386 |Test Loss: 0.1748 | Test Error: 0.0494\n",
      "Epoch: 064 | Train Loss: 0.138 |Test Loss: 0.173 | Test Error: 0.0493\n",
      "Epoch: 065 | Train Loss: 0.1368 |Test Loss: 0.1728 | Test Error: 0.0501\n",
      "Epoch: 066 | Train Loss: 0.1362 |Test Loss: 0.1714 | Test Error: 0.0499\n",
      "Epoch: 067 | Train Loss: 0.1354 |Test Loss: 0.1713 | Test Error: 0.049\n",
      "Epoch: 068 | Train Loss: 0.1343 |Test Loss: 0.1711 | Test Error: 0.0486\n",
      "Epoch: 069 | Train Loss: 0.1336 |Test Loss: 0.171 | Test Error: 0.0494\n",
      "Epoch: 070 | Train Loss: 0.1328 |Test Loss: 0.1727 | Test Error: 0.049\n",
      "Epoch: 071 | Train Loss: 0.1319 |Test Loss: 0.1711 | Test Error: 0.0489\n",
      "Epoch: 072 | Train Loss: 0.1309 |Test Loss: 0.1689 | Test Error: 0.0483\n",
      "Epoch: 073 | Train Loss: 0.1302 |Test Loss: 0.1729 | Test Error: 0.0489\n",
      "Epoch: 074 | Train Loss: 0.1295 |Test Loss: 0.1697 | Test Error: 0.0488\n",
      "Epoch: 075 | Train Loss: 0.1285 |Test Loss: 0.1692 | Test Error: 0.0493\n",
      "Epoch: 076 | Train Loss: 0.1281 |Test Loss: 0.1684 | Test Error: 0.0475\n",
      "Epoch: 077 | Train Loss: 0.1271 |Test Loss: 0.1686 | Test Error: 0.048\n",
      "Epoch: 078 | Train Loss: 0.1265 |Test Loss: 0.1688 | Test Error: 0.0483\n",
      "Epoch: 079 | Train Loss: 0.126 |Test Loss: 0.1695 | Test Error: 0.049\n",
      "Epoch: 080 | Train Loss: 0.1251 |Test Loss: 0.1677 | Test Error: 0.0479\n",
      "Epoch: 081 | Train Loss: 0.1246 |Test Loss: 0.168 | Test Error: 0.0477\n",
      "Epoch: 082 | Train Loss: 0.1238 |Test Loss: 0.1686 | Test Error: 0.0477\n",
      "Epoch: 083 | Train Loss: 0.1233 |Test Loss: 0.1679 | Test Error: 0.0489\n",
      "Epoch: 084 | Train Loss: 0.1229 |Test Loss: 0.167 | Test Error: 0.0479\n",
      "Epoch: 085 | Train Loss: 0.1222 |Test Loss: 0.1682 | Test Error: 0.0479\n",
      "Epoch: 086 | Train Loss: 0.1219 |Test Loss: 0.1674 | Test Error: 0.0478\n",
      "Epoch: 087 | Train Loss: 0.1212 |Test Loss: 0.1684 | Test Error: 0.0488\n",
      "Epoch: 088 | Train Loss: 0.1201 |Test Loss: 0.1671 | Test Error: 0.0477\n",
      "Epoch: 089 | Train Loss: 0.1195 |Test Loss: 0.1677 | Test Error: 0.0483\n",
      "Epoch: 090 | Train Loss: 0.119 |Test Loss: 0.1684 | Test Error: 0.0482\n",
      "Epoch: 091 | Train Loss: 0.1187 |Test Loss: 0.1674 | Test Error: 0.0486\n",
      "Epoch: 092 | Train Loss: 0.118 |Test Loss: 0.169 | Test Error: 0.0487\n",
      "Epoch: 093 | Train Loss: 0.1179 |Test Loss: 0.1668 | Test Error: 0.0476\n",
      "Epoch: 094 | Train Loss: 0.1168 |Test Loss: 0.1662 | Test Error: 0.0478\n",
      "Epoch: 095 | Train Loss: 0.1162 |Test Loss: 0.1654 | Test Error: 0.047\n",
      "Epoch: 096 | Train Loss: 0.116 |Test Loss: 0.1676 | Test Error: 0.0483\n",
      "Epoch: 097 | Train Loss: 0.1151 |Test Loss: 0.1662 | Test Error: 0.0466\n",
      "Epoch: 098 | Train Loss: 0.1147 |Test Loss: 0.1663 | Test Error: 0.0467\n",
      "Epoch: 099 | Train Loss: 0.1143 |Test Loss: 0.1662 | Test Error: 0.0461\n",
      "Epoch: 100 | Train Loss: 0.114 |Test Loss: 0.1654 | Test Error: 0.0464\n",
      "Epoch: 001 | Train Loss: 1.454 |Test Loss: 0.7586 | Test Error: 0.1629\n",
      "Epoch: 002 | Train Loss: 0.5744 |Test Loss: 0.4362 | Test Error: 0.1092\n",
      "Epoch: 003 | Train Loss: 0.3983 |Test Loss: 0.3469 | Test Error: 0.0915\n",
      "Epoch: 004 | Train Loss: 0.337 |Test Loss: 0.3107 | Test Error: 0.0855\n",
      "Epoch: 005 | Train Loss: 0.3059 |Test Loss: 0.2893 | Test Error: 0.0824\n",
      "Epoch: 006 | Train Loss: 0.2848 |Test Loss: 0.2708 | Test Error: 0.077\n",
      "Epoch: 007 | Train Loss: 0.2689 |Test Loss: 0.2588 | Test Error: 0.0729\n",
      "Epoch: 008 | Train Loss: 0.256 |Test Loss: 0.2507 | Test Error: 0.0727\n",
      "Epoch: 009 | Train Loss: 0.2442 |Test Loss: 0.2403 | Test Error: 0.0685\n",
      "Epoch: 010 | Train Loss: 0.2338 |Test Loss: 0.2313 | Test Error: 0.0652\n",
      "Epoch: 011 | Train Loss: 0.2232 |Test Loss: 0.2226 | Test Error: 0.0639\n",
      "Epoch: 012 | Train Loss: 0.2147 |Test Loss: 0.2143 | Test Error: 0.0603\n",
      "Epoch: 013 | Train Loss: 0.206 |Test Loss: 0.206 | Test Error: 0.0596\n",
      "Epoch: 014 | Train Loss: 0.1983 |Test Loss: 0.1991 | Test Error: 0.0582\n",
      "Epoch: 015 | Train Loss: 0.1909 |Test Loss: 0.1928 | Test Error: 0.0559\n",
      "Epoch: 016 | Train Loss: 0.1839 |Test Loss: 0.188 | Test Error: 0.0553\n",
      "Epoch: 017 | Train Loss: 0.1778 |Test Loss: 0.1832 | Test Error: 0.0545\n",
      "Epoch: 018 | Train Loss: 0.1719 |Test Loss: 0.1759 | Test Error: 0.0515\n",
      "Epoch: 019 | Train Loss: 0.1665 |Test Loss: 0.1725 | Test Error: 0.0508\n",
      "Epoch: 020 | Train Loss: 0.1611 |Test Loss: 0.1681 | Test Error: 0.0492\n",
      "Epoch: 021 | Train Loss: 0.1565 |Test Loss: 0.1637 | Test Error: 0.0485\n",
      "Epoch: 022 | Train Loss: 0.1521 |Test Loss: 0.1596 | Test Error: 0.0461\n",
      "Epoch: 023 | Train Loss: 0.1473 |Test Loss: 0.1568 | Test Error: 0.0461\n",
      "Epoch: 024 | Train Loss: 0.1436 |Test Loss: 0.1532 | Test Error: 0.045\n",
      "Epoch: 025 | Train Loss: 0.1394 |Test Loss: 0.1501 | Test Error: 0.0443\n",
      "Epoch: 026 | Train Loss: 0.136 |Test Loss: 0.1481 | Test Error: 0.0428\n",
      "Epoch: 027 | Train Loss: 0.1329 |Test Loss: 0.1454 | Test Error: 0.043\n",
      "Epoch: 028 | Train Loss: 0.1297 |Test Loss: 0.1427 | Test Error: 0.0416\n",
      "Epoch: 029 | Train Loss: 0.1265 |Test Loss: 0.1425 | Test Error: 0.0409\n",
      "Epoch: 030 | Train Loss: 0.1235 |Test Loss: 0.1393 | Test Error: 0.04\n",
      "Epoch: 031 | Train Loss: 0.1208 |Test Loss: 0.1378 | Test Error: 0.0414\n",
      "Epoch: 032 | Train Loss: 0.1185 |Test Loss: 0.1348 | Test Error: 0.039\n",
      "Epoch: 033 | Train Loss: 0.1156 |Test Loss: 0.1353 | Test Error: 0.0387\n",
      "Epoch: 034 | Train Loss: 0.1128 |Test Loss: 0.1317 | Test Error: 0.0378\n",
      "Epoch: 035 | Train Loss: 0.1106 |Test Loss: 0.1305 | Test Error: 0.0368\n",
      "Epoch: 036 | Train Loss: 0.1084 |Test Loss: 0.1289 | Test Error: 0.0376\n",
      "Epoch: 037 | Train Loss: 0.1063 |Test Loss: 0.1267 | Test Error: 0.0362\n",
      "Epoch: 038 | Train Loss: 0.1038 |Test Loss: 0.1269 | Test Error: 0.0372\n",
      "Epoch: 039 | Train Loss: 0.102 |Test Loss: 0.1252 | Test Error: 0.0358\n",
      "Epoch: 040 | Train Loss: 0.1 |Test Loss: 0.1243 | Test Error: 0.0355\n",
      "Epoch: 041 | Train Loss: 0.09825 |Test Loss: 0.1235 | Test Error: 0.0363\n",
      "Epoch: 042 | Train Loss: 0.09613 |Test Loss: 0.1221 | Test Error: 0.0356\n",
      "Epoch: 043 | Train Loss: 0.09445 |Test Loss: 0.1199 | Test Error: 0.0345\n",
      "Epoch: 044 | Train Loss: 0.09267 |Test Loss: 0.1199 | Test Error: 0.0344\n",
      "Epoch: 045 | Train Loss: 0.09083 |Test Loss: 0.1185 | Test Error: 0.0341\n",
      "Epoch: 046 | Train Loss: 0.0895 |Test Loss: 0.1205 | Test Error: 0.0346\n",
      "Epoch: 047 | Train Loss: 0.08836 |Test Loss: 0.117 | Test Error: 0.034\n",
      "Epoch: 048 | Train Loss: 0.08667 |Test Loss: 0.1175 | Test Error: 0.0329\n",
      "Epoch: 049 | Train Loss: 0.08494 |Test Loss: 0.1163 | Test Error: 0.0334\n",
      "Epoch: 050 | Train Loss: 0.08322 |Test Loss: 0.116 | Test Error: 0.0329\n",
      "Epoch: 051 | Train Loss: 0.08186 |Test Loss: 0.117 | Test Error: 0.0333\n",
      "Epoch: 052 | Train Loss: 0.08075 |Test Loss: 0.1162 | Test Error: 0.0346\n",
      "Epoch: 053 | Train Loss: 0.07931 |Test Loss: 0.1141 | Test Error: 0.0327\n",
      "Epoch: 054 | Train Loss: 0.07777 |Test Loss: 0.1135 | Test Error: 0.0329\n",
      "Epoch: 055 | Train Loss: 0.0769 |Test Loss: 0.1138 | Test Error: 0.0324\n",
      "Epoch: 056 | Train Loss: 0.07509 |Test Loss: 0.1125 | Test Error: 0.0329\n",
      "Epoch: 057 | Train Loss: 0.07417 |Test Loss: 0.1128 | Test Error: 0.0329\n",
      "Epoch: 058 | Train Loss: 0.07322 |Test Loss: 0.1141 | Test Error: 0.0334\n",
      "Epoch: 059 | Train Loss: 0.07169 |Test Loss: 0.1128 | Test Error: 0.0324\n",
      "Epoch: 060 | Train Loss: 0.07093 |Test Loss: 0.1122 | Test Error: 0.0321\n",
      "Epoch: 061 | Train Loss: 0.06975 |Test Loss: 0.113 | Test Error: 0.0322\n",
      "Epoch: 062 | Train Loss: 0.06839 |Test Loss: 0.1119 | Test Error: 0.0331\n",
      "Epoch: 063 | Train Loss: 0.06741 |Test Loss: 0.1107 | Test Error: 0.0326\n",
      "Epoch: 064 | Train Loss: 0.06623 |Test Loss: 0.1112 | Test Error: 0.0319\n",
      "Epoch: 065 | Train Loss: 0.06578 |Test Loss: 0.1119 | Test Error: 0.0328\n",
      "Epoch: 066 | Train Loss: 0.0646 |Test Loss: 0.111 | Test Error: 0.0324\n",
      "Epoch: 067 | Train Loss: 0.06338 |Test Loss: 0.1117 | Test Error: 0.0327\n",
      "Epoch: 068 | Train Loss: 0.06238 |Test Loss: 0.1114 | Test Error: 0.0315\n",
      "Epoch: 069 | Train Loss: 0.06161 |Test Loss: 0.1104 | Test Error: 0.0324\n",
      "Epoch: 070 | Train Loss: 0.06087 |Test Loss: 0.1127 | Test Error: 0.0323\n",
      "Epoch: 071 | Train Loss: 0.05991 |Test Loss: 0.1116 | Test Error: 0.0317\n",
      "Epoch: 072 | Train Loss: 0.05942 |Test Loss: 0.1106 | Test Error: 0.0318\n",
      "Epoch: 073 | Train Loss: 0.05821 |Test Loss: 0.1127 | Test Error: 0.0316\n",
      "Epoch: 074 | Train Loss: 0.05759 |Test Loss: 0.1129 | Test Error: 0.0326\n",
      "Epoch: 075 | Train Loss: 0.05706 |Test Loss: 0.112 | Test Error: 0.033\n",
      "Epoch: 076 | Train Loss: 0.05613 |Test Loss: 0.1143 | Test Error: 0.0328\n",
      "Epoch: 077 | Train Loss: 0.05509 |Test Loss: 0.1117 | Test Error: 0.0326\n",
      "Epoch: 078 | Train Loss: 0.05431 |Test Loss: 0.1104 | Test Error: 0.0322\n",
      "Epoch: 079 | Train Loss: 0.05353 |Test Loss: 0.1101 | Test Error: 0.0312\n",
      "Epoch: 080 | Train Loss: 0.05289 |Test Loss: 0.1127 | Test Error: 0.0325\n",
      "Epoch: 081 | Train Loss: 0.05195 |Test Loss: 0.1111 | Test Error: 0.032\n",
      "Epoch: 082 | Train Loss: 0.05117 |Test Loss: 0.1113 | Test Error: 0.032\n",
      "Epoch: 083 | Train Loss: 0.05052 |Test Loss: 0.1111 | Test Error: 0.0319\n",
      "Epoch: 084 | Train Loss: 0.04988 |Test Loss: 0.1123 | Test Error: 0.0323\n",
      "Epoch: 085 | Train Loss: 0.04959 |Test Loss: 0.1103 | Test Error: 0.0314\n",
      "Epoch: 086 | Train Loss: 0.04866 |Test Loss: 0.1131 | Test Error: 0.0322\n",
      "Epoch: 087 | Train Loss: 0.04801 |Test Loss: 0.1119 | Test Error: 0.0324\n",
      "Epoch: 088 | Train Loss: 0.04741 |Test Loss: 0.1108 | Test Error: 0.0317\n",
      "Epoch: 089 | Train Loss: 0.04667 |Test Loss: 0.1119 | Test Error: 0.0312\n",
      "Epoch: 090 | Train Loss: 0.04615 |Test Loss: 0.1118 | Test Error: 0.0319\n",
      "Epoch: 091 | Train Loss: 0.04557 |Test Loss: 0.1125 | Test Error: 0.0306\n",
      "Epoch: 092 | Train Loss: 0.04501 |Test Loss: 0.1138 | Test Error: 0.0319\n",
      "Epoch: 093 | Train Loss: 0.04454 |Test Loss: 0.1141 | Test Error: 0.0322\n",
      "Epoch: 094 | Train Loss: 0.04366 |Test Loss: 0.1129 | Test Error: 0.0325\n",
      "Epoch: 095 | Train Loss: 0.04288 |Test Loss: 0.1126 | Test Error: 0.0321\n",
      "Epoch: 096 | Train Loss: 0.04284 |Test Loss: 0.114 | Test Error: 0.0313\n",
      "Epoch: 097 | Train Loss: 0.04221 |Test Loss: 0.1143 | Test Error: 0.0331\n",
      "Epoch: 098 | Train Loss: 0.04178 |Test Loss: 0.1173 | Test Error: 0.0325\n",
      "Epoch: 099 | Train Loss: 0.04124 |Test Loss: 0.1145 | Test Error: 0.0314\n",
      "Epoch: 100 | Train Loss: 0.04041 |Test Loss: 0.1141 | Test Error: 0.0314\n",
      "Epoch: 001 | Train Loss: 1.218 |Test Loss: 0.5284 | Test Error: 0.1256\n",
      "Epoch: 002 | Train Loss: 0.4305 |Test Loss: 0.3476 | Test Error: 0.0907\n",
      "Epoch: 003 | Train Loss: 0.3309 |Test Loss: 0.2963 | Test Error: 0.0828\n",
      "Epoch: 004 | Train Loss: 0.2895 |Test Loss: 0.269 | Test Error: 0.0767\n",
      "Epoch: 005 | Train Loss: 0.2607 |Test Loss: 0.2462 | Test Error: 0.0682\n",
      "Epoch: 006 | Train Loss: 0.238 |Test Loss: 0.2251 | Test Error: 0.0646\n",
      "Epoch: 007 | Train Loss: 0.2185 |Test Loss: 0.2099 | Test Error: 0.0589\n",
      "Epoch: 008 | Train Loss: 0.2022 |Test Loss: 0.1976 | Test Error: 0.0565\n",
      "Epoch: 009 | Train Loss: 0.1888 |Test Loss: 0.1844 | Test Error: 0.0522\n",
      "Epoch: 010 | Train Loss: 0.1757 |Test Loss: 0.176 | Test Error: 0.0511\n",
      "Epoch: 011 | Train Loss: 0.1651 |Test Loss: 0.167 | Test Error: 0.047\n",
      "Epoch: 012 | Train Loss: 0.1557 |Test Loss: 0.1602 | Test Error: 0.0473\n",
      "Epoch: 013 | Train Loss: 0.147 |Test Loss: 0.1528 | Test Error: 0.0444\n",
      "Epoch: 014 | Train Loss: 0.1389 |Test Loss: 0.147 | Test Error: 0.0434\n",
      "Epoch: 015 | Train Loss: 0.1323 |Test Loss: 0.142 | Test Error: 0.042\n",
      "Epoch: 016 | Train Loss: 0.1258 |Test Loss: 0.1359 | Test Error: 0.0397\n",
      "Epoch: 017 | Train Loss: 0.1203 |Test Loss: 0.1329 | Test Error: 0.0384\n",
      "Epoch: 018 | Train Loss: 0.1142 |Test Loss: 0.1296 | Test Error: 0.0355\n",
      "Epoch: 019 | Train Loss: 0.1089 |Test Loss: 0.1239 | Test Error: 0.0356\n",
      "Epoch: 020 | Train Loss: 0.1045 |Test Loss: 0.1209 | Test Error: 0.0347\n",
      "Epoch: 021 | Train Loss: 0.09977 |Test Loss: 0.1178 | Test Error: 0.0333\n",
      "Epoch: 022 | Train Loss: 0.09568 |Test Loss: 0.1143 | Test Error: 0.0315\n",
      "Epoch: 023 | Train Loss: 0.09195 |Test Loss: 0.1128 | Test Error: 0.0325\n",
      "Epoch: 024 | Train Loss: 0.08808 |Test Loss: 0.1106 | Test Error: 0.0318\n",
      "Epoch: 025 | Train Loss: 0.08528 |Test Loss: 0.1078 | Test Error: 0.031\n",
      "Epoch: 026 | Train Loss: 0.08173 |Test Loss: 0.1067 | Test Error: 0.0312\n",
      "Epoch: 027 | Train Loss: 0.07854 |Test Loss: 0.1041 | Test Error: 0.0306\n",
      "Epoch: 028 | Train Loss: 0.07568 |Test Loss: 0.1032 | Test Error: 0.0298\n",
      "Epoch: 029 | Train Loss: 0.07311 |Test Loss: 0.101 | Test Error: 0.0291\n",
      "Epoch: 030 | Train Loss: 0.07042 |Test Loss: 0.1024 | Test Error: 0.0298\n",
      "Epoch: 031 | Train Loss: 0.06777 |Test Loss: 0.09908 | Test Error: 0.0295\n",
      "Epoch: 032 | Train Loss: 0.06552 |Test Loss: 0.09842 | Test Error: 0.0286\n",
      "Epoch: 033 | Train Loss: 0.06299 |Test Loss: 0.09712 | Test Error: 0.0282\n",
      "Epoch: 034 | Train Loss: 0.0614 |Test Loss: 0.09578 | Test Error: 0.0284\n",
      "Epoch: 035 | Train Loss: 0.05904 |Test Loss: 0.0956 | Test Error: 0.0273\n",
      "Epoch: 036 | Train Loss: 0.0567 |Test Loss: 0.09465 | Test Error: 0.0273\n",
      "Epoch: 037 | Train Loss: 0.05521 |Test Loss: 0.0938 | Test Error: 0.0271\n",
      "Epoch: 038 | Train Loss: 0.05338 |Test Loss: 0.09264 | Test Error: 0.0269\n",
      "Epoch: 039 | Train Loss: 0.05179 |Test Loss: 0.09239 | Test Error: 0.0262\n",
      "Epoch: 040 | Train Loss: 0.04954 |Test Loss: 0.09179 | Test Error: 0.0268\n",
      "Epoch: 041 | Train Loss: 0.04845 |Test Loss: 0.09142 | Test Error: 0.0267\n",
      "Epoch: 042 | Train Loss: 0.04669 |Test Loss: 0.09067 | Test Error: 0.0264\n",
      "Epoch: 043 | Train Loss: 0.0453 |Test Loss: 0.08945 | Test Error: 0.026\n",
      "Epoch: 044 | Train Loss: 0.04393 |Test Loss: 0.08938 | Test Error: 0.0264\n",
      "Epoch: 045 | Train Loss: 0.0421 |Test Loss: 0.08907 | Test Error: 0.025\n",
      "Epoch: 046 | Train Loss: 0.04103 |Test Loss: 0.08922 | Test Error: 0.0251\n",
      "Epoch: 047 | Train Loss: 0.03988 |Test Loss: 0.08813 | Test Error: 0.0249\n",
      "Epoch: 048 | Train Loss: 0.03839 |Test Loss: 0.08795 | Test Error: 0.0247\n",
      "Epoch: 049 | Train Loss: 0.03761 |Test Loss: 0.08952 | Test Error: 0.027\n",
      "Epoch: 050 | Train Loss: 0.03609 |Test Loss: 0.08834 | Test Error: 0.0252\n",
      "Epoch: 051 | Train Loss: 0.03505 |Test Loss: 0.08721 | Test Error: 0.0245\n",
      "Epoch: 052 | Train Loss: 0.03379 |Test Loss: 0.08682 | Test Error: 0.0251\n",
      "Epoch: 053 | Train Loss: 0.03272 |Test Loss: 0.08685 | Test Error: 0.0241\n",
      "Epoch: 054 | Train Loss: 0.03158 |Test Loss: 0.08683 | Test Error: 0.0246\n",
      "Epoch: 055 | Train Loss: 0.03075 |Test Loss: 0.08888 | Test Error: 0.0254\n",
      "Epoch: 056 | Train Loss: 0.02988 |Test Loss: 0.08773 | Test Error: 0.0245\n",
      "Epoch: 057 | Train Loss: 0.02916 |Test Loss: 0.0865 | Test Error: 0.0241\n",
      "Epoch: 058 | Train Loss: 0.02787 |Test Loss: 0.08573 | Test Error: 0.0238\n",
      "Epoch: 059 | Train Loss: 0.02692 |Test Loss: 0.08637 | Test Error: 0.0239\n",
      "Epoch: 060 | Train Loss: 0.02613 |Test Loss: 0.0874 | Test Error: 0.0241\n",
      "Epoch: 061 | Train Loss: 0.02537 |Test Loss: 0.08701 | Test Error: 0.0237\n",
      "Epoch: 062 | Train Loss: 0.02459 |Test Loss: 0.08638 | Test Error: 0.0241\n",
      "Epoch: 063 | Train Loss: 0.0235 |Test Loss: 0.0876 | Test Error: 0.0231\n",
      "Epoch: 064 | Train Loss: 0.02297 |Test Loss: 0.08666 | Test Error: 0.0238\n",
      "Epoch: 065 | Train Loss: 0.02229 |Test Loss: 0.0871 | Test Error: 0.0242\n",
      "Epoch: 066 | Train Loss: 0.02168 |Test Loss: 0.08653 | Test Error: 0.0228\n",
      "Epoch: 067 | Train Loss: 0.02088 |Test Loss: 0.08688 | Test Error: 0.0241\n",
      "Epoch: 068 | Train Loss: 0.02034 |Test Loss: 0.08895 | Test Error: 0.0249\n",
      "Epoch: 069 | Train Loss: 0.01967 |Test Loss: 0.08973 | Test Error: 0.0248\n",
      "Epoch: 070 | Train Loss: 0.01918 |Test Loss: 0.08887 | Test Error: 0.0238\n",
      "Epoch: 071 | Train Loss: 0.01858 |Test Loss: 0.08858 | Test Error: 0.0239\n",
      "Epoch: 072 | Train Loss: 0.01771 |Test Loss: 0.0892 | Test Error: 0.0237\n",
      "Epoch: 073 | Train Loss: 0.01705 |Test Loss: 0.0907 | Test Error: 0.0251\n",
      "Epoch: 074 | Train Loss: 0.01682 |Test Loss: 0.09145 | Test Error: 0.0245\n",
      "Epoch: 075 | Train Loss: 0.0163 |Test Loss: 0.09163 | Test Error: 0.0243\n",
      "Epoch: 076 | Train Loss: 0.01571 |Test Loss: 0.09007 | Test Error: 0.0242\n",
      "Epoch: 077 | Train Loss: 0.01534 |Test Loss: 0.08989 | Test Error: 0.0245\n",
      "Epoch: 078 | Train Loss: 0.01467 |Test Loss: 0.09012 | Test Error: 0.024\n",
      "Epoch: 079 | Train Loss: 0.01402 |Test Loss: 0.09075 | Test Error: 0.0248\n",
      "Epoch: 080 | Train Loss: 0.01364 |Test Loss: 0.09107 | Test Error: 0.0242\n",
      "Epoch: 081 | Train Loss: 0.0134 |Test Loss: 0.09107 | Test Error: 0.0245\n",
      "Epoch: 082 | Train Loss: 0.01278 |Test Loss: 0.09282 | Test Error: 0.025\n",
      "Epoch: 083 | Train Loss: 0.0125 |Test Loss: 0.09332 | Test Error: 0.025\n",
      "Epoch: 084 | Train Loss: 0.01214 |Test Loss: 0.09295 | Test Error: 0.0242\n",
      "Epoch: 085 | Train Loss: 0.01169 |Test Loss: 0.0925 | Test Error: 0.0238\n",
      "Epoch: 086 | Train Loss: 0.0113 |Test Loss: 0.09356 | Test Error: 0.0243\n",
      "Epoch: 087 | Train Loss: 0.01078 |Test Loss: 0.09269 | Test Error: 0.0241\n",
      "Epoch: 088 | Train Loss: 0.01038 |Test Loss: 0.09296 | Test Error: 0.0237\n",
      "Epoch: 089 | Train Loss: 0.01009 |Test Loss: 0.0946 | Test Error: 0.0251\n",
      "Epoch: 090 | Train Loss: 0.009914 |Test Loss: 0.09481 | Test Error: 0.0247\n",
      "Epoch: 091 | Train Loss: 0.009644 |Test Loss: 0.09398 | Test Error: 0.0243\n",
      "Epoch: 092 | Train Loss: 0.009146 |Test Loss: 0.09602 | Test Error: 0.0242\n",
      "Epoch: 093 | Train Loss: 0.00881 |Test Loss: 0.09504 | Test Error: 0.0244\n",
      "Epoch: 094 | Train Loss: 0.008606 |Test Loss: 0.0954 | Test Error: 0.0239\n",
      "Epoch: 095 | Train Loss: 0.008341 |Test Loss: 0.09592 | Test Error: 0.0247\n",
      "Epoch: 096 | Train Loss: 0.007945 |Test Loss: 0.09696 | Test Error: 0.024\n",
      "Epoch: 097 | Train Loss: 0.00798 |Test Loss: 0.09698 | Test Error: 0.0239\n",
      "Epoch: 098 | Train Loss: 0.007471 |Test Loss: 0.09768 | Test Error: 0.0245\n",
      "Epoch: 099 | Train Loss: 0.007248 |Test Loss: 0.09712 | Test Error: 0.0244\n",
      "Epoch: 100 | Train Loss: 0.007131 |Test Loss: 0.09934 | Test Error: 0.0245\n",
      "Epoch: 001 | Train Loss: 0.9993 |Test Loss: 0.4034 | Test Error: 0.1048\n",
      "Epoch: 002 | Train Loss: 0.3535 |Test Loss: 0.297 | Test Error: 0.0832\n",
      "Epoch: 003 | Train Loss: 0.2857 |Test Loss: 0.255 | Test Error: 0.0726\n",
      "Epoch: 004 | Train Loss: 0.2459 |Test Loss: 0.2248 | Test Error: 0.0638\n",
      "Epoch: 005 | Train Loss: 0.2155 |Test Loss: 0.1985 | Test Error: 0.0586\n",
      "Epoch: 006 | Train Loss: 0.1907 |Test Loss: 0.18 | Test Error: 0.0523\n",
      "Epoch: 007 | Train Loss: 0.1729 |Test Loss: 0.1658 | Test Error: 0.0481\n",
      "Epoch: 008 | Train Loss: 0.1565 |Test Loss: 0.1517 | Test Error: 0.0442\n",
      "Epoch: 009 | Train Loss: 0.1431 |Test Loss: 0.1425 | Test Error: 0.0408\n",
      "Epoch: 010 | Train Loss: 0.1316 |Test Loss: 0.1342 | Test Error: 0.038\n",
      "Epoch: 011 | Train Loss: 0.1216 |Test Loss: 0.1279 | Test Error: 0.0381\n",
      "Epoch: 012 | Train Loss: 0.1129 |Test Loss: 0.1216 | Test Error: 0.0349\n",
      "Epoch: 013 | Train Loss: 0.105 |Test Loss: 0.1159 | Test Error: 0.0336\n",
      "Epoch: 014 | Train Loss: 0.09858 |Test Loss: 0.1127 | Test Error: 0.0333\n",
      "Epoch: 015 | Train Loss: 0.09151 |Test Loss: 0.107 | Test Error: 0.0317\n",
      "Epoch: 016 | Train Loss: 0.08621 |Test Loss: 0.1033 | Test Error: 0.0302\n",
      "Epoch: 017 | Train Loss: 0.08037 |Test Loss: 0.1012 | Test Error: 0.0301\n",
      "Epoch: 018 | Train Loss: 0.07576 |Test Loss: 0.0992 | Test Error: 0.0291\n",
      "Epoch: 019 | Train Loss: 0.07186 |Test Loss: 0.09504 | Test Error: 0.0285\n",
      "Epoch: 020 | Train Loss: 0.0681 |Test Loss: 0.09097 | Test Error: 0.0285\n",
      "Epoch: 021 | Train Loss: 0.06421 |Test Loss: 0.08933 | Test Error: 0.0273\n",
      "Epoch: 022 | Train Loss: 0.06069 |Test Loss: 0.0887 | Test Error: 0.0267\n",
      "Epoch: 023 | Train Loss: 0.05767 |Test Loss: 0.08633 | Test Error: 0.0266\n",
      "Epoch: 024 | Train Loss: 0.05445 |Test Loss: 0.08683 | Test Error: 0.0268\n",
      "Epoch: 025 | Train Loss: 0.05182 |Test Loss: 0.08535 | Test Error: 0.0265\n",
      "Epoch: 026 | Train Loss: 0.04883 |Test Loss: 0.08214 | Test Error: 0.0246\n",
      "Epoch: 027 | Train Loss: 0.04668 |Test Loss: 0.08291 | Test Error: 0.0256\n",
      "Epoch: 028 | Train Loss: 0.04471 |Test Loss: 0.07901 | Test Error: 0.024\n",
      "Epoch: 029 | Train Loss: 0.04181 |Test Loss: 0.0792 | Test Error: 0.0239\n",
      "Epoch: 030 | Train Loss: 0.04019 |Test Loss: 0.07889 | Test Error: 0.0246\n",
      "Epoch: 031 | Train Loss: 0.03821 |Test Loss: 0.07707 | Test Error: 0.0234\n",
      "Epoch: 032 | Train Loss: 0.03628 |Test Loss: 0.07683 | Test Error: 0.0231\n",
      "Epoch: 033 | Train Loss: 0.03432 |Test Loss: 0.07672 | Test Error: 0.0229\n",
      "Epoch: 034 | Train Loss: 0.03302 |Test Loss: 0.07707 | Test Error: 0.0234\n",
      "Epoch: 035 | Train Loss: 0.03131 |Test Loss: 0.07444 | Test Error: 0.0225\n",
      "Epoch: 036 | Train Loss: 0.0298 |Test Loss: 0.07444 | Test Error: 0.0219\n",
      "Epoch: 037 | Train Loss: 0.02819 |Test Loss: 0.07536 | Test Error: 0.0229\n",
      "Epoch: 038 | Train Loss: 0.0271 |Test Loss: 0.07429 | Test Error: 0.0227\n",
      "Epoch: 039 | Train Loss: 0.02605 |Test Loss: 0.07458 | Test Error: 0.0217\n",
      "Epoch: 040 | Train Loss: 0.02443 |Test Loss: 0.0754 | Test Error: 0.0227\n",
      "Epoch: 041 | Train Loss: 0.02302 |Test Loss: 0.076 | Test Error: 0.0219\n",
      "Epoch: 042 | Train Loss: 0.02228 |Test Loss: 0.07506 | Test Error: 0.0222\n",
      "Epoch: 043 | Train Loss: 0.02084 |Test Loss: 0.07352 | Test Error: 0.022\n",
      "Epoch: 044 | Train Loss: 0.01986 |Test Loss: 0.07507 | Test Error: 0.0222\n",
      "Epoch: 045 | Train Loss: 0.01952 |Test Loss: 0.07627 | Test Error: 0.0224\n",
      "Epoch: 046 | Train Loss: 0.01804 |Test Loss: 0.07521 | Test Error: 0.0222\n",
      "Epoch: 047 | Train Loss: 0.01752 |Test Loss: 0.07524 | Test Error: 0.0219\n",
      "Epoch: 048 | Train Loss: 0.01639 |Test Loss: 0.07512 | Test Error: 0.0217\n",
      "Epoch: 049 | Train Loss: 0.01558 |Test Loss: 0.07609 | Test Error: 0.0216\n",
      "Epoch: 050 | Train Loss: 0.01531 |Test Loss: 0.07471 | Test Error: 0.0217\n",
      "Epoch: 051 | Train Loss: 0.01427 |Test Loss: 0.07624 | Test Error: 0.0219\n",
      "Epoch: 052 | Train Loss: 0.01376 |Test Loss: 0.07576 | Test Error: 0.021\n",
      "Epoch: 053 | Train Loss: 0.01297 |Test Loss: 0.07548 | Test Error: 0.0212\n",
      "Epoch: 054 | Train Loss: 0.0125 |Test Loss: 0.07648 | Test Error: 0.0214\n",
      "Epoch: 055 | Train Loss: 0.0118 |Test Loss: 0.07651 | Test Error: 0.0213\n",
      "Epoch: 056 | Train Loss: 0.01116 |Test Loss: 0.07645 | Test Error: 0.0206\n",
      "Epoch: 057 | Train Loss: 0.01072 |Test Loss: 0.07727 | Test Error: 0.0214\n",
      "Epoch: 058 | Train Loss: 0.01029 |Test Loss: 0.0779 | Test Error: 0.022\n",
      "Epoch: 059 | Train Loss: 0.009719 |Test Loss: 0.07749 | Test Error: 0.0226\n",
      "Epoch: 060 | Train Loss: 0.009255 |Test Loss: 0.07763 | Test Error: 0.0217\n",
      "Epoch: 061 | Train Loss: 0.008952 |Test Loss: 0.07788 | Test Error: 0.022\n",
      "Epoch: 062 | Train Loss: 0.008478 |Test Loss: 0.07793 | Test Error: 0.0213\n",
      "Epoch: 063 | Train Loss: 0.008113 |Test Loss: 0.07837 | Test Error: 0.0216\n",
      "Epoch: 064 | Train Loss: 0.007662 |Test Loss: 0.07742 | Test Error: 0.0211\n",
      "Epoch: 065 | Train Loss: 0.007109 |Test Loss: 0.07904 | Test Error: 0.0209\n",
      "Epoch: 066 | Train Loss: 0.006916 |Test Loss: 0.07902 | Test Error: 0.0205\n",
      "Epoch: 067 | Train Loss: 0.006561 |Test Loss: 0.07861 | Test Error: 0.0219\n",
      "Epoch: 068 | Train Loss: 0.006187 |Test Loss: 0.07945 | Test Error: 0.0211\n",
      "Epoch: 069 | Train Loss: 0.005848 |Test Loss: 0.08129 | Test Error: 0.022\n",
      "Epoch: 070 | Train Loss: 0.005768 |Test Loss: 0.08098 | Test Error: 0.0214\n",
      "Epoch: 071 | Train Loss: 0.005317 |Test Loss: 0.08003 | Test Error: 0.0211\n",
      "Epoch: 072 | Train Loss: 0.005087 |Test Loss: 0.08117 | Test Error: 0.0215\n",
      "Epoch: 073 | Train Loss: 0.004874 |Test Loss: 0.08123 | Test Error: 0.0219\n",
      "Epoch: 074 | Train Loss: 0.004727 |Test Loss: 0.08145 | Test Error: 0.0207\n",
      "Epoch: 075 | Train Loss: 0.004354 |Test Loss: 0.08216 | Test Error: 0.0211\n",
      "Epoch: 076 | Train Loss: 0.004236 |Test Loss: 0.08234 | Test Error: 0.0211\n",
      "Epoch: 077 | Train Loss: 0.004135 |Test Loss: 0.08327 | Test Error: 0.0214\n",
      "Epoch: 078 | Train Loss: 0.003966 |Test Loss: 0.08355 | Test Error: 0.0212\n",
      "Epoch: 079 | Train Loss: 0.003666 |Test Loss: 0.0831 | Test Error: 0.0214\n",
      "Epoch: 080 | Train Loss: 0.003595 |Test Loss: 0.08379 | Test Error: 0.0215\n",
      "Epoch: 081 | Train Loss: 0.003306 |Test Loss: 0.08436 | Test Error: 0.0213\n",
      "Epoch: 082 | Train Loss: 0.003213 |Test Loss: 0.08485 | Test Error: 0.0208\n",
      "Epoch: 083 | Train Loss: 0.003074 |Test Loss: 0.08633 | Test Error: 0.0221\n",
      "Epoch: 084 | Train Loss: 0.002983 |Test Loss: 0.08495 | Test Error: 0.021\n",
      "Epoch: 085 | Train Loss: 0.002837 |Test Loss: 0.08611 | Test Error: 0.0211\n",
      "Epoch: 086 | Train Loss: 0.002657 |Test Loss: 0.08617 | Test Error: 0.0209\n",
      "Epoch: 087 | Train Loss: 0.002584 |Test Loss: 0.08618 | Test Error: 0.0212\n",
      "Epoch: 088 | Train Loss: 0.002476 |Test Loss: 0.08683 | Test Error: 0.0206\n",
      "Epoch: 089 | Train Loss: 0.002444 |Test Loss: 0.08728 | Test Error: 0.0217\n",
      "Epoch: 090 | Train Loss: 0.002265 |Test Loss: 0.08769 | Test Error: 0.0211\n",
      "Epoch: 091 | Train Loss: 0.002164 |Test Loss: 0.08793 | Test Error: 0.0211\n",
      "Epoch: 092 | Train Loss: 0.00212 |Test Loss: 0.08841 | Test Error: 0.0213\n",
      "Epoch: 093 | Train Loss: 0.002031 |Test Loss: 0.08907 | Test Error: 0.0213\n",
      "Epoch: 094 | Train Loss: 0.001887 |Test Loss: 0.09026 | Test Error: 0.0216\n",
      "Epoch: 095 | Train Loss: 0.00184 |Test Loss: 0.09025 | Test Error: 0.0211\n",
      "Epoch: 096 | Train Loss: 0.001763 |Test Loss: 0.08963 | Test Error: 0.0213\n",
      "Epoch: 097 | Train Loss: 0.001688 |Test Loss: 0.09112 | Test Error: 0.0216\n",
      "Epoch: 098 | Train Loss: 0.001621 |Test Loss: 0.09105 | Test Error: 0.0212\n",
      "Epoch: 099 | Train Loss: 0.001611 |Test Loss: 0.09216 | Test Error: 0.0213\n",
      "Epoch: 100 | Train Loss: 0.001538 |Test Loss: 0.09083 | Test Error: 0.0211\n",
      "Epoch: 001 | Train Loss: 0.8164 |Test Loss: 0.3398 | Test Error: 0.0933\n",
      "Epoch: 002 | Train Loss: 0.3057 |Test Loss: 0.2615 | Test Error: 0.0755\n",
      "Epoch: 003 | Train Loss: 0.2469 |Test Loss: 0.2192 | Test Error: 0.0619\n",
      "Epoch: 004 | Train Loss: 0.2065 |Test Loss: 0.1912 | Test Error: 0.0559\n",
      "Epoch: 005 | Train Loss: 0.1764 |Test Loss: 0.1647 | Test Error: 0.0488\n",
      "Epoch: 006 | Train Loss: 0.1539 |Test Loss: 0.1484 | Test Error: 0.0433\n",
      "Epoch: 007 | Train Loss: 0.1342 |Test Loss: 0.1331 | Test Error: 0.0386\n",
      "Epoch: 008 | Train Loss: 0.1196 |Test Loss: 0.1218 | Test Error: 0.034\n",
      "Epoch: 009 | Train Loss: 0.1061 |Test Loss: 0.1162 | Test Error: 0.0338\n",
      "Epoch: 010 | Train Loss: 0.09622 |Test Loss: 0.1073 | Test Error: 0.0309\n",
      "Epoch: 011 | Train Loss: 0.08659 |Test Loss: 0.1013 | Test Error: 0.0295\n",
      "Epoch: 012 | Train Loss: 0.07865 |Test Loss: 0.1004 | Test Error: 0.0306\n",
      "Epoch: 013 | Train Loss: 0.07187 |Test Loss: 0.09326 | Test Error: 0.0272\n",
      "Epoch: 014 | Train Loss: 0.06554 |Test Loss: 0.08911 | Test Error: 0.0271\n",
      "Epoch: 015 | Train Loss: 0.06049 |Test Loss: 0.08596 | Test Error: 0.0262\n",
      "Epoch: 016 | Train Loss: 0.05554 |Test Loss: 0.08599 | Test Error: 0.0265\n",
      "Epoch: 017 | Train Loss: 0.05134 |Test Loss: 0.0824 | Test Error: 0.0252\n",
      "Epoch: 018 | Train Loss: 0.04734 |Test Loss: 0.07959 | Test Error: 0.0241\n",
      "Epoch: 019 | Train Loss: 0.04374 |Test Loss: 0.0775 | Test Error: 0.0233\n",
      "Epoch: 020 | Train Loss: 0.04002 |Test Loss: 0.07807 | Test Error: 0.0238\n",
      "Epoch: 021 | Train Loss: 0.0377 |Test Loss: 0.07619 | Test Error: 0.0237\n",
      "Epoch: 022 | Train Loss: 0.03487 |Test Loss: 0.07368 | Test Error: 0.023\n",
      "Epoch: 023 | Train Loss: 0.03216 |Test Loss: 0.07284 | Test Error: 0.0214\n",
      "Epoch: 024 | Train Loss: 0.02989 |Test Loss: 0.07372 | Test Error: 0.0216\n",
      "Epoch: 025 | Train Loss: 0.02799 |Test Loss: 0.07158 | Test Error: 0.0211\n",
      "Epoch: 026 | Train Loss: 0.02579 |Test Loss: 0.07278 | Test Error: 0.0214\n",
      "Epoch: 027 | Train Loss: 0.02367 |Test Loss: 0.07229 | Test Error: 0.0221\n",
      "Epoch: 028 | Train Loss: 0.02253 |Test Loss: 0.07263 | Test Error: 0.0216\n",
      "Epoch: 029 | Train Loss: 0.02101 |Test Loss: 0.07229 | Test Error: 0.0213\n",
      "Epoch: 030 | Train Loss: 0.01949 |Test Loss: 0.07196 | Test Error: 0.0217\n",
      "Epoch: 031 | Train Loss: 0.01822 |Test Loss: 0.07356 | Test Error: 0.0213\n",
      "Epoch: 032 | Train Loss: 0.0169 |Test Loss: 0.0726 | Test Error: 0.0213\n",
      "Epoch: 033 | Train Loss: 0.01582 |Test Loss: 0.07172 | Test Error: 0.0209\n",
      "Epoch: 034 | Train Loss: 0.01442 |Test Loss: 0.07165 | Test Error: 0.0214\n",
      "Epoch: 035 | Train Loss: 0.01343 |Test Loss: 0.07115 | Test Error: 0.0205\n",
      "Epoch: 036 | Train Loss: 0.01279 |Test Loss: 0.07251 | Test Error: 0.0211\n",
      "Epoch: 037 | Train Loss: 0.01143 |Test Loss: 0.07254 | Test Error: 0.0212\n",
      "Epoch: 038 | Train Loss: 0.01096 |Test Loss: 0.07269 | Test Error: 0.021\n",
      "Epoch: 039 | Train Loss: 0.009997 |Test Loss: 0.07273 | Test Error: 0.0205\n",
      "Epoch: 040 | Train Loss: 0.00941 |Test Loss: 0.07424 | Test Error: 0.0212\n",
      "Epoch: 041 | Train Loss: 0.008941 |Test Loss: 0.07311 | Test Error: 0.0195\n",
      "Epoch: 042 | Train Loss: 0.008123 |Test Loss: 0.07236 | Test Error: 0.0201\n",
      "Epoch: 043 | Train Loss: 0.007627 |Test Loss: 0.07186 | Test Error: 0.02\n",
      "Epoch: 044 | Train Loss: 0.007214 |Test Loss: 0.07491 | Test Error: 0.0212\n",
      "Epoch: 045 | Train Loss: 0.00665 |Test Loss: 0.07293 | Test Error: 0.0205\n",
      "Epoch: 046 | Train Loss: 0.00617 |Test Loss: 0.07659 | Test Error: 0.0212\n",
      "Epoch: 047 | Train Loss: 0.005909 |Test Loss: 0.07487 | Test Error: 0.0201\n",
      "Epoch: 048 | Train Loss: 0.005418 |Test Loss: 0.07502 | Test Error: 0.0197\n",
      "Epoch: 049 | Train Loss: 0.005054 |Test Loss: 0.07445 | Test Error: 0.0199\n",
      "Epoch: 050 | Train Loss: 0.004762 |Test Loss: 0.07553 | Test Error: 0.02\n",
      "Epoch: 051 | Train Loss: 0.004614 |Test Loss: 0.07602 | Test Error: 0.0199\n",
      "Epoch: 052 | Train Loss: 0.004194 |Test Loss: 0.07616 | Test Error: 0.0199\n",
      "Epoch: 053 | Train Loss: 0.00397 |Test Loss: 0.07574 | Test Error: 0.0197\n",
      "Epoch: 054 | Train Loss: 0.003686 |Test Loss: 0.0765 | Test Error: 0.0196\n",
      "Epoch: 055 | Train Loss: 0.00345 |Test Loss: 0.07665 | Test Error: 0.0195\n",
      "Epoch: 056 | Train Loss: 0.003232 |Test Loss: 0.07717 | Test Error: 0.0196\n",
      "Epoch: 057 | Train Loss: 0.003056 |Test Loss: 0.07821 | Test Error: 0.0203\n",
      "Epoch: 058 | Train Loss: 0.002922 |Test Loss: 0.07829 | Test Error: 0.0194\n",
      "Epoch: 059 | Train Loss: 0.002747 |Test Loss: 0.07858 | Test Error: 0.0199\n",
      "Epoch: 060 | Train Loss: 0.002665 |Test Loss: 0.08014 | Test Error: 0.0202\n",
      "Epoch: 061 | Train Loss: 0.002458 |Test Loss: 0.07917 | Test Error: 0.02\n",
      "Epoch: 062 | Train Loss: 0.002373 |Test Loss: 0.07953 | Test Error: 0.0198\n",
      "Epoch: 063 | Train Loss: 0.002249 |Test Loss: 0.07878 | Test Error: 0.0191\n",
      "Epoch: 064 | Train Loss: 0.002179 |Test Loss: 0.07929 | Test Error: 0.0198\n",
      "Epoch: 065 | Train Loss: 0.002008 |Test Loss: 0.07941 | Test Error: 0.0196\n",
      "Epoch: 066 | Train Loss: 0.001923 |Test Loss: 0.08194 | Test Error: 0.0193\n",
      "Epoch: 067 | Train Loss: 0.001823 |Test Loss: 0.08053 | Test Error: 0.0194\n",
      "Epoch: 068 | Train Loss: 0.001711 |Test Loss: 0.08132 | Test Error: 0.0195\n",
      "Epoch: 069 | Train Loss: 0.001635 |Test Loss: 0.08148 | Test Error: 0.0198\n",
      "Epoch: 070 | Train Loss: 0.001557 |Test Loss: 0.0818 | Test Error: 0.0192\n",
      "Epoch: 071 | Train Loss: 0.001455 |Test Loss: 0.08231 | Test Error: 0.0192\n",
      "Epoch: 072 | Train Loss: 0.001407 |Test Loss: 0.0829 | Test Error: 0.0195\n",
      "Epoch: 073 | Train Loss: 0.001346 |Test Loss: 0.08274 | Test Error: 0.0195\n",
      "Epoch: 074 | Train Loss: 0.001279 |Test Loss: 0.0835 | Test Error: 0.0196\n",
      "Epoch: 075 | Train Loss: 0.001229 |Test Loss: 0.08379 | Test Error: 0.0195\n",
      "Epoch: 076 | Train Loss: 0.001166 |Test Loss: 0.08415 | Test Error: 0.0194\n",
      "Epoch: 077 | Train Loss: 0.001108 |Test Loss: 0.08462 | Test Error: 0.019\n",
      "Epoch: 078 | Train Loss: 0.001076 |Test Loss: 0.08433 | Test Error: 0.0197\n",
      "Epoch: 079 | Train Loss: 0.001006 |Test Loss: 0.08531 | Test Error: 0.0194\n",
      "Epoch: 080 | Train Loss: 0.0009814 |Test Loss: 0.08496 | Test Error: 0.0197\n",
      "Epoch: 081 | Train Loss: 0.0009272 |Test Loss: 0.08537 | Test Error: 0.0195\n",
      "Epoch: 082 | Train Loss: 0.0008885 |Test Loss: 0.08588 | Test Error: 0.0196\n",
      "Epoch: 083 | Train Loss: 0.0008671 |Test Loss: 0.08676 | Test Error: 0.0198\n",
      "Epoch: 084 | Train Loss: 0.0008232 |Test Loss: 0.08788 | Test Error: 0.0195\n",
      "Epoch: 085 | Train Loss: 0.0007799 |Test Loss: 0.08627 | Test Error: 0.0192\n",
      "Epoch: 086 | Train Loss: 0.0007514 |Test Loss: 0.08758 | Test Error: 0.0193\n",
      "Epoch: 087 | Train Loss: 0.0007171 |Test Loss: 0.08728 | Test Error: 0.0194\n",
      "Epoch: 088 | Train Loss: 0.0006829 |Test Loss: 0.08839 | Test Error: 0.0195\n",
      "Epoch: 089 | Train Loss: 0.000658 |Test Loss: 0.08857 | Test Error: 0.0195\n",
      "Epoch: 090 | Train Loss: 0.0006288 |Test Loss: 0.08905 | Test Error: 0.0195\n",
      "Epoch: 091 | Train Loss: 0.0006093 |Test Loss: 0.08886 | Test Error: 0.0194\n",
      "Epoch: 092 | Train Loss: 0.0005859 |Test Loss: 0.09032 | Test Error: 0.0198\n",
      "Epoch: 093 | Train Loss: 0.0005617 |Test Loss: 0.08978 | Test Error: 0.0196\n",
      "Epoch: 094 | Train Loss: 0.0005348 |Test Loss: 0.0909 | Test Error: 0.0191\n",
      "Epoch: 095 | Train Loss: 0.0005095 |Test Loss: 0.09012 | Test Error: 0.0197\n",
      "Epoch: 096 | Train Loss: 0.0004951 |Test Loss: 0.09048 | Test Error: 0.0194\n",
      "Epoch: 097 | Train Loss: 0.0004743 |Test Loss: 0.09129 | Test Error: 0.0193\n",
      "Epoch: 098 | Train Loss: 0.0004646 |Test Loss: 0.0922 | Test Error: 0.0193\n",
      "Epoch: 099 | Train Loss: 0.0004442 |Test Loss: 0.09208 | Test Error: 0.0192\n",
      "Epoch: 100 | Train Loss: 0.0004398 |Test Loss: 0.09293 | Test Error: 0.0191\n",
      "Epoch: 001 | Train Loss: 0.6907 |Test Loss: 0.3026 | Test Error: 0.0864\n",
      "Epoch: 002 | Train Loss: 0.2702 |Test Loss: 0.2301 | Test Error: 0.0668\n",
      "Epoch: 003 | Train Loss: 0.2107 |Test Loss: 0.181 | Test Error: 0.0518\n",
      "Epoch: 004 | Train Loss: 0.168 |Test Loss: 0.154 | Test Error: 0.0443\n",
      "Epoch: 005 | Train Loss: 0.1389 |Test Loss: 0.1316 | Test Error: 0.0379\n",
      "Epoch: 006 | Train Loss: 0.1167 |Test Loss: 0.1175 | Test Error: 0.0348\n",
      "Epoch: 007 | Train Loss: 0.09998 |Test Loss: 0.108 | Test Error: 0.0318\n",
      "Epoch: 008 | Train Loss: 0.0876 |Test Loss: 0.1012 | Test Error: 0.0286\n",
      "Epoch: 009 | Train Loss: 0.07766 |Test Loss: 0.09173 | Test Error: 0.0279\n",
      "Epoch: 010 | Train Loss: 0.06825 |Test Loss: 0.0858 | Test Error: 0.0258\n",
      "Epoch: 011 | Train Loss: 0.06109 |Test Loss: 0.08031 | Test Error: 0.0243\n",
      "Epoch: 012 | Train Loss: 0.05429 |Test Loss: 0.07797 | Test Error: 0.0242\n",
      "Epoch: 013 | Train Loss: 0.04897 |Test Loss: 0.07554 | Test Error: 0.0228\n",
      "Epoch: 014 | Train Loss: 0.04471 |Test Loss: 0.07134 | Test Error: 0.0215\n",
      "Epoch: 015 | Train Loss: 0.03937 |Test Loss: 0.07115 | Test Error: 0.0216\n",
      "Epoch: 016 | Train Loss: 0.03539 |Test Loss: 0.07102 | Test Error: 0.0225\n",
      "Epoch: 017 | Train Loss: 0.03204 |Test Loss: 0.06712 | Test Error: 0.021\n",
      "Epoch: 018 | Train Loss: 0.02908 |Test Loss: 0.06718 | Test Error: 0.0204\n",
      "Epoch: 019 | Train Loss: 0.02615 |Test Loss: 0.06685 | Test Error: 0.0203\n",
      "Epoch: 020 | Train Loss: 0.02387 |Test Loss: 0.06476 | Test Error: 0.0202\n",
      "Epoch: 021 | Train Loss: 0.02166 |Test Loss: 0.06432 | Test Error: 0.02\n",
      "Epoch: 022 | Train Loss: 0.01953 |Test Loss: 0.06448 | Test Error: 0.0195\n",
      "Epoch: 023 | Train Loss: 0.01787 |Test Loss: 0.06426 | Test Error: 0.0197\n",
      "Epoch: 024 | Train Loss: 0.01681 |Test Loss: 0.063 | Test Error: 0.0195\n",
      "Epoch: 025 | Train Loss: 0.01463 |Test Loss: 0.064 | Test Error: 0.0191\n",
      "Epoch: 026 | Train Loss: 0.01351 |Test Loss: 0.06348 | Test Error: 0.0197\n",
      "Epoch: 027 | Train Loss: 0.01234 |Test Loss: 0.06308 | Test Error: 0.0191\n",
      "Epoch: 028 | Train Loss: 0.01089 |Test Loss: 0.06243 | Test Error: 0.019\n",
      "Epoch: 029 | Train Loss: 0.009785 |Test Loss: 0.06338 | Test Error: 0.0198\n",
      "Epoch: 030 | Train Loss: 0.009095 |Test Loss: 0.06381 | Test Error: 0.019\n",
      "Epoch: 031 | Train Loss: 0.008225 |Test Loss: 0.06311 | Test Error: 0.0191\n",
      "Epoch: 032 | Train Loss: 0.007747 |Test Loss: 0.06239 | Test Error: 0.0191\n",
      "Epoch: 033 | Train Loss: 0.006934 |Test Loss: 0.0641 | Test Error: 0.0188\n",
      "Epoch: 034 | Train Loss: 0.006489 |Test Loss: 0.06383 | Test Error: 0.0192\n",
      "Epoch: 035 | Train Loss: 0.005709 |Test Loss: 0.06277 | Test Error: 0.0184\n",
      "Epoch: 036 | Train Loss: 0.005388 |Test Loss: 0.06435 | Test Error: 0.0189\n",
      "Epoch: 037 | Train Loss: 0.004938 |Test Loss: 0.06523 | Test Error: 0.0195\n",
      "Epoch: 038 | Train Loss: 0.004618 |Test Loss: 0.06517 | Test Error: 0.019\n",
      "Epoch: 039 | Train Loss: 0.004181 |Test Loss: 0.06682 | Test Error: 0.02\n",
      "Epoch: 040 | Train Loss: 0.003795 |Test Loss: 0.06481 | Test Error: 0.0196\n",
      "Epoch: 041 | Train Loss: 0.003641 |Test Loss: 0.06612 | Test Error: 0.0189\n",
      "Epoch: 042 | Train Loss: 0.003384 |Test Loss: 0.06634 | Test Error: 0.0186\n",
      "Epoch: 043 | Train Loss: 0.003096 |Test Loss: 0.06586 | Test Error: 0.0192\n",
      "Epoch: 044 | Train Loss: 0.00287 |Test Loss: 0.0666 | Test Error: 0.0192\n",
      "Epoch: 045 | Train Loss: 0.00268 |Test Loss: 0.0673 | Test Error: 0.0183\n",
      "Epoch: 046 | Train Loss: 0.002574 |Test Loss: 0.06736 | Test Error: 0.0189\n",
      "Epoch: 047 | Train Loss: 0.002334 |Test Loss: 0.06699 | Test Error: 0.0189\n",
      "Epoch: 048 | Train Loss: 0.002228 |Test Loss: 0.06663 | Test Error: 0.0184\n",
      "Epoch: 049 | Train Loss: 0.00207 |Test Loss: 0.06746 | Test Error: 0.0192\n",
      "Epoch: 050 | Train Loss: 0.001906 |Test Loss: 0.06841 | Test Error: 0.0191\n",
      "Epoch: 051 | Train Loss: 0.001813 |Test Loss: 0.06852 | Test Error: 0.0194\n",
      "Epoch: 052 | Train Loss: 0.001741 |Test Loss: 0.06842 | Test Error: 0.0191\n",
      "Epoch: 053 | Train Loss: 0.001574 |Test Loss: 0.06874 | Test Error: 0.0191\n",
      "Epoch: 054 | Train Loss: 0.001518 |Test Loss: 0.06887 | Test Error: 0.0192\n",
      "Epoch: 055 | Train Loss: 0.001407 |Test Loss: 0.06972 | Test Error: 0.0194\n",
      "Epoch: 056 | Train Loss: 0.001362 |Test Loss: 0.06997 | Test Error: 0.019\n",
      "Epoch: 057 | Train Loss: 0.001277 |Test Loss: 0.07082 | Test Error: 0.0191\n",
      "Epoch: 058 | Train Loss: 0.001225 |Test Loss: 0.07097 | Test Error: 0.0191\n",
      "Epoch: 059 | Train Loss: 0.001176 |Test Loss: 0.07063 | Test Error: 0.0194\n",
      "Epoch: 060 | Train Loss: 0.00108 |Test Loss: 0.07116 | Test Error: 0.0192\n",
      "Epoch: 061 | Train Loss: 0.001031 |Test Loss: 0.07082 | Test Error: 0.0188\n",
      "Epoch: 062 | Train Loss: 0.0009874 |Test Loss: 0.07242 | Test Error: 0.0194\n",
      "Epoch: 063 | Train Loss: 0.0009228 |Test Loss: 0.07206 | Test Error: 0.0193\n",
      "Epoch: 064 | Train Loss: 0.0008748 |Test Loss: 0.07231 | Test Error: 0.0189\n",
      "Epoch: 065 | Train Loss: 0.0008324 |Test Loss: 0.07285 | Test Error: 0.0194\n",
      "Epoch: 066 | Train Loss: 0.0007845 |Test Loss: 0.07247 | Test Error: 0.0191\n",
      "Epoch: 067 | Train Loss: 0.0007518 |Test Loss: 0.07276 | Test Error: 0.019\n",
      "Epoch: 068 | Train Loss: 0.000714 |Test Loss: 0.07299 | Test Error: 0.0187\n",
      "Epoch: 069 | Train Loss: 0.0006793 |Test Loss: 0.07388 | Test Error: 0.0192\n",
      "Epoch: 070 | Train Loss: 0.0006522 |Test Loss: 0.07434 | Test Error: 0.0191\n",
      "Epoch: 071 | Train Loss: 0.0006202 |Test Loss: 0.07383 | Test Error: 0.0187\n",
      "Epoch: 072 | Train Loss: 0.0006022 |Test Loss: 0.07439 | Test Error: 0.0191\n",
      "Epoch: 073 | Train Loss: 0.0005672 |Test Loss: 0.07491 | Test Error: 0.019\n",
      "Epoch: 074 | Train Loss: 0.0005495 |Test Loss: 0.07516 | Test Error: 0.0193\n",
      "Epoch: 075 | Train Loss: 0.0005163 |Test Loss: 0.07516 | Test Error: 0.0194\n",
      "Epoch: 076 | Train Loss: 0.0004942 |Test Loss: 0.07565 | Test Error: 0.0194\n",
      "Epoch: 077 | Train Loss: 0.0004801 |Test Loss: 0.07644 | Test Error: 0.019\n",
      "Epoch: 078 | Train Loss: 0.0004586 |Test Loss: 0.07612 | Test Error: 0.0193\n",
      "Epoch: 079 | Train Loss: 0.0004376 |Test Loss: 0.07693 | Test Error: 0.0194\n",
      "Epoch: 080 | Train Loss: 0.0004179 |Test Loss: 0.077 | Test Error: 0.019\n",
      "Epoch: 081 | Train Loss: 0.0003995 |Test Loss: 0.07647 | Test Error: 0.0192\n",
      "Epoch: 082 | Train Loss: 0.0003853 |Test Loss: 0.07727 | Test Error: 0.0195\n",
      "Epoch: 083 | Train Loss: 0.0003728 |Test Loss: 0.07713 | Test Error: 0.019\n",
      "Epoch: 084 | Train Loss: 0.0003546 |Test Loss: 0.07768 | Test Error: 0.0187\n",
      "Epoch: 085 | Train Loss: 0.0003392 |Test Loss: 0.07808 | Test Error: 0.0188\n",
      "Epoch: 086 | Train Loss: 0.0003241 |Test Loss: 0.07842 | Test Error: 0.0192\n",
      "Epoch: 087 | Train Loss: 0.0003134 |Test Loss: 0.07838 | Test Error: 0.019\n",
      "Epoch: 088 | Train Loss: 0.0003117 |Test Loss: 0.079 | Test Error: 0.0193\n",
      "Epoch: 089 | Train Loss: 0.000287 |Test Loss: 0.07839 | Test Error: 0.0193\n",
      "Epoch: 090 | Train Loss: 0.0002773 |Test Loss: 0.07918 | Test Error: 0.0192\n",
      "Epoch: 091 | Train Loss: 0.0002619 |Test Loss: 0.08003 | Test Error: 0.0189\n",
      "Epoch: 092 | Train Loss: 0.0002521 |Test Loss: 0.07979 | Test Error: 0.0191\n",
      "Epoch: 093 | Train Loss: 0.0002447 |Test Loss: 0.08025 | Test Error: 0.0193\n",
      "Epoch: 094 | Train Loss: 0.000234 |Test Loss: 0.08054 | Test Error: 0.0192\n",
      "Epoch: 095 | Train Loss: 0.0002245 |Test Loss: 0.081 | Test Error: 0.019\n",
      "Epoch: 096 | Train Loss: 0.0002203 |Test Loss: 0.08032 | Test Error: 0.0186\n",
      "Epoch: 097 | Train Loss: 0.0002102 |Test Loss: 0.08144 | Test Error: 0.0186\n",
      "Epoch: 098 | Train Loss: 0.0002032 |Test Loss: 0.08167 | Test Error: 0.0194\n",
      "Epoch: 099 | Train Loss: 0.0001938 |Test Loss: 0.08154 | Test Error: 0.019\n",
      "Epoch: 100 | Train Loss: 0.0001863 |Test Loss: 0.08157 | Test Error: 0.019\n",
      "Epoch: 001 | Train Loss: 0.5941 |Test Loss: 0.2699 | Test Error: 0.0774\n",
      "Epoch: 002 | Train Loss: 0.2397 |Test Loss: 0.1974 | Test Error: 0.0561\n",
      "Epoch: 003 | Train Loss: 0.1754 |Test Loss: 0.1549 | Test Error: 0.0448\n",
      "Epoch: 004 | Train Loss: 0.1353 |Test Loss: 0.1294 | Test Error: 0.0373\n",
      "Epoch: 005 | Train Loss: 0.1079 |Test Loss: 0.1068 | Test Error: 0.0317\n",
      "Epoch: 006 | Train Loss: 0.08771 |Test Loss: 0.09545 | Test Error: 0.0271\n",
      "Epoch: 007 | Train Loss: 0.07413 |Test Loss: 0.08759 | Test Error: 0.0262\n",
      "Epoch: 008 | Train Loss: 0.06233 |Test Loss: 0.08161 | Test Error: 0.0246\n",
      "Epoch: 009 | Train Loss: 0.05324 |Test Loss: 0.0761 | Test Error: 0.0231\n",
      "Epoch: 010 | Train Loss: 0.04741 |Test Loss: 0.07257 | Test Error: 0.0213\n",
      "Epoch: 011 | Train Loss: 0.04053 |Test Loss: 0.06938 | Test Error: 0.0221\n",
      "Epoch: 012 | Train Loss: 0.03486 |Test Loss: 0.06732 | Test Error: 0.021\n",
      "Epoch: 013 | Train Loss: 0.03026 |Test Loss: 0.06543 | Test Error: 0.0204\n",
      "Epoch: 014 | Train Loss: 0.02614 |Test Loss: 0.06158 | Test Error: 0.0197\n",
      "Epoch: 015 | Train Loss: 0.02301 |Test Loss: 0.06252 | Test Error: 0.0187\n",
      "Epoch: 016 | Train Loss: 0.01983 |Test Loss: 0.06305 | Test Error: 0.0189\n",
      "Epoch: 017 | Train Loss: 0.01717 |Test Loss: 0.06124 | Test Error: 0.0187\n",
      "Epoch: 018 | Train Loss: 0.01516 |Test Loss: 0.06083 | Test Error: 0.0194\n",
      "Epoch: 019 | Train Loss: 0.01316 |Test Loss: 0.06168 | Test Error: 0.0197\n",
      "Epoch: 020 | Train Loss: 0.01173 |Test Loss: 0.06008 | Test Error: 0.0186\n",
      "Epoch: 021 | Train Loss: 0.01053 |Test Loss: 0.06371 | Test Error: 0.0195\n",
      "Epoch: 022 | Train Loss: 0.009355 |Test Loss: 0.06095 | Test Error: 0.019\n",
      "Epoch: 023 | Train Loss: 0.00798 |Test Loss: 0.06156 | Test Error: 0.019\n",
      "Epoch: 024 | Train Loss: 0.007126 |Test Loss: 0.06241 | Test Error: 0.0184\n",
      "Epoch: 025 | Train Loss: 0.006903 |Test Loss: 0.06292 | Test Error: 0.0184\n",
      "Epoch: 026 | Train Loss: 0.006088 |Test Loss: 0.06168 | Test Error: 0.0188\n",
      "Epoch: 027 | Train Loss: 0.005153 |Test Loss: 0.06144 | Test Error: 0.019\n",
      "Epoch: 028 | Train Loss: 0.004503 |Test Loss: 0.06201 | Test Error: 0.0187\n",
      "Epoch: 029 | Train Loss: 0.004047 |Test Loss: 0.06281 | Test Error: 0.0188\n",
      "Epoch: 030 | Train Loss: 0.003611 |Test Loss: 0.06304 | Test Error: 0.0186\n",
      "Epoch: 031 | Train Loss: 0.003372 |Test Loss: 0.06234 | Test Error: 0.0186\n",
      "Epoch: 032 | Train Loss: 0.003046 |Test Loss: 0.0634 | Test Error: 0.0184\n",
      "Epoch: 033 | Train Loss: 0.00274 |Test Loss: 0.06355 | Test Error: 0.0185\n",
      "Epoch: 034 | Train Loss: 0.002552 |Test Loss: 0.06412 | Test Error: 0.0179\n",
      "Epoch: 035 | Train Loss: 0.002316 |Test Loss: 0.06384 | Test Error: 0.0187\n",
      "Epoch: 036 | Train Loss: 0.002101 |Test Loss: 0.06404 | Test Error: 0.018\n",
      "Epoch: 037 | Train Loss: 0.001927 |Test Loss: 0.06516 | Test Error: 0.0182\n",
      "Epoch: 038 | Train Loss: 0.001747 |Test Loss: 0.06591 | Test Error: 0.0184\n",
      "Epoch: 039 | Train Loss: 0.001668 |Test Loss: 0.06536 | Test Error: 0.0182\n",
      "Epoch: 040 | Train Loss: 0.001536 |Test Loss: 0.06636 | Test Error: 0.0186\n",
      "Epoch: 041 | Train Loss: 0.00141 |Test Loss: 0.06682 | Test Error: 0.0186\n",
      "Epoch: 042 | Train Loss: 0.00133 |Test Loss: 0.06583 | Test Error: 0.0184\n",
      "Epoch: 043 | Train Loss: 0.001268 |Test Loss: 0.06781 | Test Error: 0.0186\n",
      "Epoch: 044 | Train Loss: 0.001196 |Test Loss: 0.06745 | Test Error: 0.0176\n",
      "Epoch: 045 | Train Loss: 0.001091 |Test Loss: 0.06779 | Test Error: 0.018\n",
      "Epoch: 046 | Train Loss: 0.00102 |Test Loss: 0.06834 | Test Error: 0.0181\n",
      "Epoch: 047 | Train Loss: 0.0009635 |Test Loss: 0.06771 | Test Error: 0.0179\n",
      "Epoch: 048 | Train Loss: 0.0009007 |Test Loss: 0.06863 | Test Error: 0.0182\n",
      "Epoch: 049 | Train Loss: 0.0008605 |Test Loss: 0.06881 | Test Error: 0.0183\n",
      "Epoch: 050 | Train Loss: 0.0008163 |Test Loss: 0.06905 | Test Error: 0.0182\n",
      "Epoch: 051 | Train Loss: 0.0007627 |Test Loss: 0.06984 | Test Error: 0.0184\n",
      "Epoch: 052 | Train Loss: 0.0007182 |Test Loss: 0.06944 | Test Error: 0.0182\n",
      "Epoch: 053 | Train Loss: 0.0006748 |Test Loss: 0.07047 | Test Error: 0.0186\n",
      "Epoch: 054 | Train Loss: 0.0006406 |Test Loss: 0.07059 | Test Error: 0.0182\n",
      "Epoch: 055 | Train Loss: 0.0006035 |Test Loss: 0.07058 | Test Error: 0.0185\n",
      "Epoch: 056 | Train Loss: 0.0005819 |Test Loss: 0.0714 | Test Error: 0.0184\n",
      "Epoch: 057 | Train Loss: 0.0005486 |Test Loss: 0.07156 | Test Error: 0.0185\n",
      "Epoch: 058 | Train Loss: 0.0005222 |Test Loss: 0.07146 | Test Error: 0.0185\n",
      "Epoch: 059 | Train Loss: 0.0005029 |Test Loss: 0.07227 | Test Error: 0.0182\n",
      "Epoch: 060 | Train Loss: 0.000474 |Test Loss: 0.07203 | Test Error: 0.0185\n",
      "Epoch: 061 | Train Loss: 0.0004568 |Test Loss: 0.07247 | Test Error: 0.0181\n",
      "Epoch: 062 | Train Loss: 0.0004306 |Test Loss: 0.07227 | Test Error: 0.0183\n",
      "Epoch: 063 | Train Loss: 0.0004041 |Test Loss: 0.07326 | Test Error: 0.0181\n",
      "Epoch: 064 | Train Loss: 0.0003887 |Test Loss: 0.07289 | Test Error: 0.0183\n",
      "Epoch: 065 | Train Loss: 0.0003704 |Test Loss: 0.07353 | Test Error: 0.0183\n",
      "Epoch: 066 | Train Loss: 0.0003512 |Test Loss: 0.07413 | Test Error: 0.0179\n",
      "Epoch: 067 | Train Loss: 0.0003395 |Test Loss: 0.07365 | Test Error: 0.0184\n",
      "Epoch: 068 | Train Loss: 0.0003198 |Test Loss: 0.0742 | Test Error: 0.0186\n",
      "Epoch: 069 | Train Loss: 0.0003079 |Test Loss: 0.07454 | Test Error: 0.0188\n",
      "Epoch: 070 | Train Loss: 0.0002962 |Test Loss: 0.07566 | Test Error: 0.0185\n",
      "Epoch: 071 | Train Loss: 0.0002834 |Test Loss: 0.07528 | Test Error: 0.0183\n",
      "Epoch: 072 | Train Loss: 0.0002687 |Test Loss: 0.07608 | Test Error: 0.0183\n",
      "Epoch: 073 | Train Loss: 0.0002581 |Test Loss: 0.07638 | Test Error: 0.0182\n",
      "Epoch: 074 | Train Loss: 0.0002483 |Test Loss: 0.07575 | Test Error: 0.0181\n",
      "Epoch: 075 | Train Loss: 0.0002372 |Test Loss: 0.07637 | Test Error: 0.0183\n",
      "Epoch: 076 | Train Loss: 0.0002279 |Test Loss: 0.07717 | Test Error: 0.0185\n",
      "Epoch: 077 | Train Loss: 0.0002183 |Test Loss: 0.07651 | Test Error: 0.0185\n",
      "Epoch: 078 | Train Loss: 0.000208 |Test Loss: 0.07699 | Test Error: 0.0179\n",
      "Epoch: 079 | Train Loss: 0.0001987 |Test Loss: 0.07797 | Test Error: 0.0183\n",
      "Epoch: 080 | Train Loss: 0.0001917 |Test Loss: 0.07719 | Test Error: 0.0185\n",
      "Epoch: 081 | Train Loss: 0.0001823 |Test Loss: 0.07788 | Test Error: 0.0184\n",
      "Epoch: 082 | Train Loss: 0.0001755 |Test Loss: 0.07846 | Test Error: 0.0184\n",
      "Epoch: 083 | Train Loss: 0.0001695 |Test Loss: 0.07879 | Test Error: 0.0181\n",
      "Epoch: 084 | Train Loss: 0.0001626 |Test Loss: 0.07913 | Test Error: 0.0181\n",
      "Epoch: 085 | Train Loss: 0.0001551 |Test Loss: 0.07925 | Test Error: 0.0186\n",
      "Epoch: 086 | Train Loss: 0.0001485 |Test Loss: 0.07958 | Test Error: 0.0182\n",
      "Epoch: 087 | Train Loss: 0.000143 |Test Loss: 0.0794 | Test Error: 0.0185\n",
      "Epoch: 088 | Train Loss: 0.0001384 |Test Loss: 0.07903 | Test Error: 0.0184\n",
      "Epoch: 089 | Train Loss: 0.0001321 |Test Loss: 0.07972 | Test Error: 0.0184\n",
      "Epoch: 090 | Train Loss: 0.0001275 |Test Loss: 0.07996 | Test Error: 0.0183\n",
      "Epoch: 091 | Train Loss: 0.0001223 |Test Loss: 0.07968 | Test Error: 0.0184\n",
      "Epoch: 092 | Train Loss: 0.0001176 |Test Loss: 0.08019 | Test Error: 0.0181\n",
      "Epoch: 093 | Train Loss: 0.0001131 |Test Loss: 0.08121 | Test Error: 0.0187\n",
      "Epoch: 094 | Train Loss: 0.0001093 |Test Loss: 0.08095 | Test Error: 0.0184\n",
      "Epoch: 095 | Train Loss: 0.0001046 |Test Loss: 0.08158 | Test Error: 0.0182\n",
      "Epoch: 096 | Train Loss: 0.0001007 |Test Loss: 0.08146 | Test Error: 0.0184\n",
      "Epoch: 097 | Train Loss: 9.721e-05 |Test Loss: 0.08199 | Test Error: 0.0186\n",
      "Epoch: 098 | Train Loss: 9.315e-05 |Test Loss: 0.08225 | Test Error: 0.0182\n",
      "Epoch: 099 | Train Loss: 8.929e-05 |Test Loss: 0.0823 | Test Error: 0.0187\n",
      "Epoch: 100 | Train Loss: 8.672e-05 |Test Loss: 0.08288 | Test Error: 0.0182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "widths = [4,8,16,32,64,128,256,512,1024]\n",
    "best_loss = []\n",
    "for w in widths:\n",
    "    # initialize model\n",
    "    net = Network(dim,nclass,w,depth)\n",
    "    #send to GPU\n",
    "    net = net.to(device)\n",
    "    #train it\n",
    "    out = train_model(net,batch_size,lr,100,train_set_mnist,test_set_mnist)\n",
    "    #find best train and test loss\n",
    "    out_min_train = np.argmin(out[0])\n",
    "    out_min_test = np.argmin(out[1])\n",
    "    best_loss.append([w,out[0][out_min_train],out[1][out_min_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•’â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚   Depth â”‚   Best train loss â”‚   Best test loss â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚       4 â”‚       0.456997    â”‚        0.475968  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚       8 â”‚       0.245164    â”‚        0.2795    â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚      16 â”‚       0.113966    â”‚        0.16542   â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚      32 â”‚       0.0404088   â”‚        0.110095  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚      64 â”‚       0.00713072  â”‚        0.0857254 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚     128 â”‚       0.0015377   â”‚        0.0735244 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚     256 â”‚       0.00043981  â”‚        0.0711542 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚     512 â”‚       0.000186318 â”‚        0.0623934 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚    1024 â”‚       8.67211e-05 â”‚        0.0600755 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "table = [[\"Depth\", \"Best train loss\", \"Best test loss\"]] + best_loss\n",
    "print(tabulate(table,headers = \"firstrow\",tablefmt = \"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_np = np.array(best_loss)\n",
    "plt.plot(best_loss_np[:,0],best_loss_np[:,1],\"r\", label = \"training loss\")\n",
    "plt.plot(best_loss_np[:,0],best_loss_np[:,2],\"b\",label = \"test loss\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot and table both show that as the width increases, the best training error and test error decrease, but both do reach convergence by approximately a width of 128. The training loss is very close to 0 at this point, whilst the test loss converges to a value of around 0.06-0.07. Increasing the width past this point does decrease the losses by a very small amount, however these gains are very marginal. Realistically, a width of 128 or 256 would be acceptable to fit the data. One could use larger width models, however, as stated with Occam's razor, we should be favouring sparser models if they perform just as well as more complex models, which is the case here.\n",
    "\n",
    "Although, if as stated in the question, we were to use stochastic gradient descent, and the test error continunally decreased, then, so long as the practitioner were happy with the computational cost/time, the model could be as wide as they liked, in order to obtain a desired test loss/accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Proper weight scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variances are chosen in this way to ensure the stability of the variance of the activations for each layer. This is because, when finding the distribution for each activation in layer l, when the CLT is applied, it leads to a multiplicative factor $N_{l-1}$, which would lead to variances that grow with each layer. Hence, in order to stabilise this, the variances of the weight are given this form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Derive the GP relation for a single hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the output layer $f_i^{(2)}(x) = \\sum_{j=1}^{N_1}w_{ij}^{(2)}g_{j}^{(1)}(x)+b_i^{(2)}$, $i = 1,...,N_2$.\n",
    "\n",
    "Here, the post activations $g_j^{(2)}$ are independent, due to the iid weight and bias parameters.\n",
    "\n",
    "\n",
    "First, we consider each hidden unit in the sum:\n",
    "\n",
    "$\\mathbb{E}[w_{ij}^{(2)}g_{j}^{(1)}(x)] =\\mathbb{E}[w_{ij}^{(2)}]\\mathbb{E}[g_{j}^{(1)}(x)] = 0$, because the weights in this layer are independent from the weights and bias in the previous layer (which determine g), and since $\\mathbb{E}[w_{ij}^{(2)}] = 0$\n",
    "\n",
    "$\\text{Var}(w_{ij}^{(2)}g_{j}^{(1)}(x)) = \\mathbb{E}[(w_{ij}^{(2)}g_{j}^{(1)}(x))^2] = \\mathbb{E}[w_{ij}^{(2)2}]\\mathbb{E}[g_{j}^{(1)}(x)^2] = \\frac{\\sigma_w^{(2)}}{N_1}\\mathbb{E}[g_{j}^{(1)}(x)^2] $.\n",
    "\n",
    "Then by the CLT, $\\sum_{j=1}^{N_1}w_{ij}^{(2)}g_{j}^{(1)}(x)$ is a sum of iid variables hence is gaussian with mean 0 and variance $\\sigma_w^{(2)}\\mathbb{E}[g_{j}^{(1)}(x)^2]$.\n",
    "\n",
    "This means, since $b_i^{(2)}$ is also Gaussian, independent from the sum, that $f_i^{(2)}(x)$ is Gaussian with mean 0 and variance $\\sigma_b^{(2)} + \\sigma_w^{(2)}\\mathbb{E}[g^{(1)}(x)^2]$, where we drop the subscript on the expectation since it is the same for all $j$.\n",
    "\n",
    "Now, we generalize to any collection of inputs and outputs eg $f_i^{(2)}(x), f_k^{(2)}(x'),...$.\n",
    "\n",
    "\n",
    "\n",
    "First, by the above, if this collection is stacked into a vector, then this vector will have mean 0 by the above argument.\n",
    "\n",
    "Next, we consider the covariance matrix for this vector:\n",
    "\n",
    "$\\text{Cov}(w_{ij}^{(2)}g_{j}^{(1)}(x),w_{kj}^{(2)}g_{j}^{(1)}(x')) = \\mathbb{E}[w_{ij}^{(2)}g_{j}^{(1)}(x)w_{kj}^{(2)}g_{j}^{(1)}(x')] =  \\mathbb{E}[w_{ij}^{(2)}w_{kj}^{(2)}] \\mathbb{E}[g_{j}^{(1)}(x)g_{j}^{(1)}(x')]$ due to the independence of the weights from the weights and bias in the previous layer as above. Clearly this is 0 unless $i = k$, in which case:\n",
    "\n",
    "$\\text{Cov}(w_{ij}^{(2)}g_{j}^{(1)}(x),w_{ij}^{(2)}g_{j}^{(1)}(x')) = \\frac{\\sigma_w^{(2)}}{N_1}\\mathbb{E}[g_{j}^{(1)}(x)g_{j}^{(1)}(x')]$.\n",
    "\n",
    "Hence, using the multivariate CLT, $(\\sum_{j=1}^{N_1}w_{ij}^{(2)}g_{j}^{(1)}(x),\\sum_{j=1}^{N_1}w_{ij}^{(2)}g_{j}^{(1)}(x'),...)$ is Gaussian with mean 0 and covariance matrix defined by\n",
    "\n",
    "$\\text{Cov}(\\sum_{j=1}^{N_1}w_{ij}^{(2)}g_{j}^{(1)}(x),\\sum_{j=1}^{N_1}w_{ij}^{(2)}g_{j}^{(1)}(x')) = \\sigma_w^{(2)}\\mathbb{E}[g^{(1)}(x)g^{(1)}(x')]$\n",
    "\n",
    "\n",
    "Finally, we consider the bias term, which is added to the vector of sums, which we construct to be the vector containing the bias $b_i^{(2)}$ where $i$ corresponds to the output in question. This vector is Gaussian with mean 0 and covariance matrix given by $\\sigma_b^{(2)}$ for all entries with the same $i$.\n",
    "\n",
    "Thus, $(f_i^{(2)}(x), f_i^{(2)}(x'),...)$ is multivariate Gaussian with mean 0 and covariance $ \\sigma_b^{(2)} +\\sigma_w^{(2)}\\mathbb{E}[g^{(1)}(x)g^{(1)}(x')]$. Again, any outputs with non-matching index, regardless of the input, have a covariance of 0.\n",
    "\n",
    "Since this was for any collection, meaning any collection is jointly mulitvariate Gaussian, we conclude that $f_{i}^{(2)}(x)$ follow a Gaussian proess, with mean $\\mu^{1} = 0$, and covariance matrix $K^1$  st $k^1 (x,x')=  \\sigma_b^{(2)} +\\sigma_w^{(2)}\\mathbb{E}[g^{(1)}(x)g^{(1)}(x')]$, in the limit that $N_1 \\to \\infty$.\n",
    "\n",
    "Note that in this question, the expectations were taken over the unknown parameters in the previous layer and this layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Why in succession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the limits in succession since, in order to apply the logic iteratively, we require that the previous layer has post activations that are distributed as GPs. In order for this to happen, we require that the previous layer's width has already increased to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Derive the GP relation for multiple hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for the $l$ th layer is $f_i^{(l)}(x) = \\sum_{j=1}^{N_{l-1}}w_{ik}^{(l)}g_j^{(l-1)}(x) + b_i^{(l)}$, where $g_j^{(l-1)}(x) = \\phi(f_j^{(l-1)}(x))$.\n",
    "\n",
    "Taking a very similar approach to before, if we consider each term in the summation $w_{ij}^{(l)}g_j^{(l-1)}$, we see that $\\mathbb{E}[w_{ij}^{(l)}g_j^{(l-1)}] = \\mathbb{E}[w_{ij}^{(l)}]\\mathbb{E}[g_j^{(l-1)}] = 0$ since the expectation of the weight is 0 and the two are independent.\n",
    "\n",
    "Also, \n",
    "\n",
    "$\\text{Cov}(w_{ij}^{(l)}g_{j}^{(l-1)}(x),w_{ij}^{(l)}g_{j}^{(l-1)}(x')) = \\mathbb{E}[w_{ij}^{(l)}g_{j}^{(l-1)}(x)w_{ij}^{(l)}g_{j}^{(1)}(x')] =  \\mathbb{E}[w_{ij}^{(l)}w_{ij}^{(l)}] \\mathbb{E}[g_{j}^{(l-1)}(x)g_{j}^{(l-1)}(x')] = \\frac{\\sigma_w^{(l)}}{N_{l-1}}\\mathbb{E}[g_{j}^{(l-1)}(x)g_{j}^{(l-1)}(x')]$ due to the independence of the weights from the weights and bias in the previous layer as above. \n",
    "\n",
    "Since $f_j^{(l-1)}(x)$ is a GP over the inputs, this means that $g_j^{(l-1)}(x)$ are iid given the input. Hence, $w_{ij}^{(l)}g_{j}^{(l-1)}(x)$ are also iid, since $w_{ij}^{(l)} $ are iid, independent from the gs. Thus, when we are considering collections of outputs from a layer, the vectors $(w_{ij}^{(l)}g_{j}^{(l-1)}(x), w_{ij}^{(l)}g_{j}^{(l-1)}(x'),...)^T$ are iid as we vary $j$.\n",
    "\n",
    "Hence we can apply the multivariate CLT, that is, the sum $\\sum_{j=1}^{N_{l-1}}(w_{ij}^{(l)}g_{j}^{(l-1)}(x), w_{ij}^{(l)}g_{j}^{(l-1)}(x'),...)^T$ is Gaussian with mean 0, and covariance matrix defined by $\\sigma_w^{(l)}\\mathbb{E}[g_{j}^{(l-1)}(x)g_{j}^{(l-1)}(x')]$ (between two of the entries).\n",
    "\n",
    "Finally, as before, we add the bias term vector, where each component is $b_i^{(l)}$, which is Gaussian with mean 0 and covariance matrix given by a matrix of the same dimension as the covariance matrix for the sum, but filled with $\\sigma_b^{(l)}$.\n",
    "\n",
    "Hence, since this was for any length collection, the collection $f_i^{(l)}(x)$ is a Gaussian process with mean $\\mu_{l-1} = 0$, and covariance matrix $K^{(l-1)}$, which is the matrix of covariances from $k^{(l-1)}(x,x') = \\sigma_b^{(l)}+\\sigma_w^{(l)}\\mathbb{E}[g^{(l-1)}(x)g^{(l-1)}(x')]$, where again we drop the subscript since the expectation product is the same for every $j$.\n",
    "\n",
    "As for a recursive expression, we can write the covariance as \n",
    "\n",
    "$k^{(l-1)}(x,x') = \\sigma_b^{(l)}+\\sigma_w^{(l)}\\mathbb{E}[g^{(l-1)}(x)g^{(l-1)}(x')] = \\sigma_b^{(l)}+\\sigma_w^{(l)}\\mathbb{E}_{f^{(l-1)} \\sim GP(0,K^{(l-2)})}[g^{(l-1)}(x)g^{(l-1)}(x')] $ \n",
    "\n",
    "Hence the integral would be performed over the distribution of the previous outputs, which form a Gaussian process with mean 0 and variance $K^{(l-2)}$, hence this is an iterative expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall choose the two labels \"cat\" and \"ship\" for this analysis. These correspond to labels 3 and 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# sort out the data\n",
    "train_data = train_set_cifar.data\n",
    "train_labels = np.array(train_set_cifar.targets)\n",
    "#pick the 2 classes\n",
    "idx = np.where((train_labels==3) | (train_labels==8) )\n",
    "train_data = train_data[idx]\n",
    "train_labels = train_labels[idx]\n",
    "print(train_labels.shape)\n",
    "\n",
    "#this means we have 10000 data points, so we randomly sample:\n",
    "sample = np.random.randint(0,train_labels.shape[0],size = 1000)\n",
    "train_data = train_data[sample]\n",
    "train_labels = train_labels[sample]\n",
    "#check imbalance\n",
    "print(train_labels[train_labels == 3].shape[0])\n",
    "#this is roughly balanced\n",
    "\n",
    "#put in tensors and scale to (0,1)\n",
    "train_data = torch.tensor(train_data,dtype = torch.float64)\n",
    "train_labels = torch.tensor(train_labels,dtype = torch.float64)\n",
    "train_data = train_data / 255\n",
    "\n",
    "#flatten\n",
    "train_data = torch.flatten(train_data,1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 0 ###\n",
    "\n",
    "#change the class labels to -0.5 for cat and 0.5 for ship:\n",
    "train_labels[train_labels==3] = -0.5\n",
    "train_labels[train_labels==8] = 0.5\n",
    "#print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall change the output to be -0.5 for cat and 0.5 for ship, before proceeding with the analysis. Then, since we are going to be taking a regression output, whenever the model returns an output less than 0, we classify as cat, and anything greater than 0 as ship.\n",
    "\n",
    "First, we define the kernel function, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1 ###\n",
    "\n",
    "\n",
    "def kernel(L,sigma_w,sigma_b,X1,X2):\n",
    "    \"\"\"\n",
    "        Function that calculates the kernel for layer L\n",
    "\n",
    "        Args:\n",
    "        L - The number of layers\n",
    "        sigma_w - variance of weights \n",
    "        sigma_b - variance of bias\n",
    "        X1 - first dataset, size M1 x N0\n",
    "        X2 - second dataset, size M2 x N0 \n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Kernel matrix of shape (n, m).\n",
    "        \"\"\"\n",
    "    N0 = X1.shape[1]\n",
    "    #K0:\n",
    "    K = sigma_b + sigma_w/N0 * torch.matmul(X1,X2.t())\n",
    "    #this ensures the inner product works as intended\n",
    "    K_x1x1 = sigma_b + sigma_w/N0 * torch.sum(torch.square(X1),dim=1).unsqueeze(1)\n",
    "    K_x2x2 = sigma_b + sigma_w/N0 * torch.sum(torch.square(X2),dim=1).unsqueeze(1)\n",
    "    #now iteratively calculate:\n",
    "    for i in range(L):\n",
    "        #theta for previous layer\n",
    "        #we get some NaNs start to appear, so clamp this\n",
    "        costheta = torch.clamp(K/torch.sqrt(torch.matmul(K_x1x1,K_x2x2.t())),min = -1+1e-6, max = 1-1e-6)\n",
    "\n",
    "        theta = torch.arccos(costheta)\n",
    "        #new K\n",
    "        K = sigma_b + sigma_w/(2 * torch.pi) * torch.sqrt(torch.matmul(K_x1x1,K_x2x2.t())) * (torch.sin(theta) + (torch.pi - theta)*torch.cos(theta))\n",
    "        #update inner product matrices. Noticing that in the formula, for (x,x), theta = arccos(1) = 0\n",
    "        #meaning K(X,X) = sigma_b + sigma_w/2K\n",
    "        K_x1x1 = sigma_b + sigma_w/2 * K_x1x1\n",
    "        K_x2x2 = sigma_b + sigma_w/2 * K_x2x2\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a method to compute the mean and covariance of the Gaussian process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2 ###\n",
    "\n",
    "class GP:\n",
    "    # THIS CLASS IS ADAPTED FROM LECTURES\n",
    "    def __init__(self, kernel: callable):\n",
    "        \"\"\"\n",
    "        Initialize the Gaussian Process (GP) with a specified kernel.\n",
    "\n",
    "        Args:\n",
    "        kernel (callable): The kernel function to use in the GP.\n",
    "        \"\"\"\n",
    "        self.k = kernel\n",
    "\n",
    "    def posterior(self, x_star, sigma_w, sigma_b, L, X=None, y=None, sigma=0 ):\n",
    "        \"\"\"\n",
    "        Given observations (X, y) and test points x_star, fit a GP model\n",
    "        and draw posterior samples for f(x_star) from the fitted model.\n",
    "\n",
    "        Args:\n",
    "        x_star (torch.Tensor): Test points at which predictions will be made.\n",
    "        sigma_w (float): weight variance\n",
    "        sigma_b(float): bias variance\n",
    "        L (int)- Layer\n",
    "        X (torch.Tensor): Observed features.\n",
    "        y (torch.Tensor): Observed response variables.\n",
    "        sigma (float): Noise level in observations.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Posterior samples for f(x_star).\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute kernel matrices\n",
    "        #same format as the one we just made\n",
    "        k_xs_x = self.k(L,sigma_w,sigma_b,x_star,X)\n",
    "        k_x_xs = self.k(L,sigma_w,sigma_b,X,x_star)\n",
    "        k_x_x = self.k(L,sigma_w,sigma_b,X,X)\n",
    "        k_xs_xs = self.k(L,sigma_w,sigma_b,x_star,x_star)\n",
    "   \n",
    "\n",
    "        cov_x_x = k_x_x + sigma *  torch.eye(X.shape[0]) \n",
    "        # Compute posterior mean and covariance\n",
    "        posterior_mean = torch.linalg.matmul(k_xs_x ,torch.linalg.solve(cov_x_x,y))\n",
    "        posterior_var = k_xs_xs - torch.matmul(k_xs_x,torch.linalg.solve(cov_x_x,k_x_xs))\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        # Enforce symmetry and positive definiteness that may be lost due to numerical errors\n",
    "        posterior_var = (posterior_var + posterior_var.T) / 2  # Enforce symmetry\n",
    "        # Add a small amount of noise to the diagonal to make the covariance matrix positive definite\n",
    "        posterior_var = posterior_var + 1e-6 * torch.eye(posterior_var.shape[0])\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        \n",
    "        self.posterior_mean = posterior_mean\n",
    "        self.posterior_var = posterior_var\n",
    "        \n",
    "        return self.posterior_mean, self.posterior_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we analyse the performance of the NNGP, measuring performance in terms of the accuracy on a test set. We use 1000 test samples from the cifar test set, making sure these 1000 samples are from the subset containing only points with label \"cat\" or \"ship\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "509\n"
     ]
    }
   ],
   "source": [
    "### Test data ###\n",
    "\n",
    "#test data\n",
    "test_data = test_set_cifar.data\n",
    "test_labels = np.array(test_set_cifar.targets)\n",
    "#pick the two classes\n",
    "idx = np.where((test_labels==3) | (test_labels==8) )\n",
    "test_data = test_data[idx]\n",
    "test_labels = test_labels[idx]\n",
    "\n",
    "print(test_labels.shape[0])\n",
    "#we need to sample:\n",
    "sample = np.random.randint(0,test_labels.shape[0],size = 1000)\n",
    "test_data = test_data[sample]\n",
    "test_labels = test_labels[sample]\n",
    "print(test_labels[test_labels == 3].shape[0])\n",
    "#this is rougly balanced\n",
    "\n",
    "#put in tensors and scale to (0,1)\n",
    "test_data = torch.tensor(test_data,dtype = torch.float64)\n",
    "test_labels = torch.tensor(test_labels,dtype = torch.float64)\n",
    "test_data = test_data / 255\n",
    "#flatten\n",
    "test_data = torch.flatten(test_data,1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose values for $\\sigma_w$ and $\\sigma_b$. For this, we choose a grid for both of them, this grid being $\\{0.01,0.1,1,2,2.5,5,10,100\\}$. We iterative over these grids, computing the posterior mean each time, and using it to classify each test point as specified above. From this, we can then compute the accuracy of the model by comparing the predicted classes to the true labels. We will choose the parameters with the best test performance.\n",
    "\n",
    "(note we could draw posterior samples for each point and take the average for each point, however this would converge to the posterior mean since they are drawn from the posterior distribution with this mean)\n",
    "\n",
    "We shall assume a small error variance of $0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3 ###\n",
    "\n",
    "#define the grids\n",
    "grid = [0.01,0.1,1,2,2.5,5,10]\n",
    "layers = [1,2,3,4,5,6,7,8,9,10]\n",
    "#iterate\n",
    "layer_best_list = []\n",
    "for l in layers:\n",
    "    best = \"\"\n",
    "    best_acc = -1\n",
    "    for sig_w in grid:\n",
    "        for sig_b in grid:\n",
    "            Gauss = GP(kernel)\n",
    "            mean, var = Gauss.posterior(test_data, sig_w,sig_b,l,train_data, train_labels,sigma = 0.001)\n",
    "            #make predictions based on the mean\n",
    "            mean[mean>0] = 8\n",
    "            mean[mean<0] = 3\n",
    "            acc = sum(mean == test_labels)/mean.shape[0]\n",
    "            #update if better\n",
    "            if acc> best_acc:\n",
    "                best_acc = acc.item()\n",
    "                best = [l,sig_w,sig_b,best_acc]\n",
    "    #at the end of each layer, append the best sigma and w varaince as well as the accuracy for that layer\n",
    "    layer_best_list.append(best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the results below in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚   Number of layers â”‚   Best sigma_w â”‚   Best sigma_b â”‚   Accuracy â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚                  1 â”‚            0.1 â”‚           0.01 â”‚      0.867 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                  2 â”‚            1   â”‚           0.01 â”‚      0.868 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                  3 â”‚            1   â”‚           0.01 â”‚      0.877 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                  4 â”‚            2   â”‚           0.01 â”‚      0.884 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                  5 â”‚            2.5 â”‚           0.01 â”‚      0.884 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                  6 â”‚            2   â”‚           0.01 â”‚      0.884 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                  7 â”‚            5   â”‚           0.01 â”‚      0.886 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                  8 â”‚            5   â”‚           0.01 â”‚      0.886 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                  9 â”‚            5   â”‚           0.01 â”‚      0.884 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                 10 â”‚            5   â”‚           0.01 â”‚      0.888 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "table = [[\"Number of layers\", \"Best sigma_w\", \"Best sigma_b\", \"Accuracy\"]] + layer_best_list\n",
    "print(tabulate(table,headers = \"firstrow\",tablefmt = \"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each number of layers, we built models on every possible combination in the grid specified above, and have presented the best combination based on accuracy. We see that as we optimize across the layes, the accuracy increases for each best set of parameters. Which suggests that, since each layer had access to the same possible combinations of parameters, increasing the number of layers improves the performance. However, the accuracy increase per layer does seem to be increasing slowly, suggesting some form of convergence.\n",
    "\n",
    "Despite this, we see that for every number of layers, the best $\\sigma_b$ is 0.01, while $\\sigma_w$ is a bit more varied. We can say that for the grid we defined, $\\sigma_b = 0.01$ is definitely optimal, while it seems a bit more uncertain for $\\sigma_w$. \n",
    "\n",
    "Going forwards, we shall choose $L=10,\\sigma_w = 5, \\sigma_b = 0.01$ based on the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for task 4, we analyse the uncertainty. We want to determine the two test samples for which the covariance is the highest and the two for which the covariance is the lowest. We do so by selecting the samples that correspond to these values in the posterior covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test sample with the largest covariance had index [483 483], making it a variance. \n",
      "This sample had a variance of 907.1929111280366\n",
      "Hence, since we want the 2 test samples with the largest covariance, we shall find the second largest.\n",
      "The test sample with the second largest covariance had index [892 892], making it a variance. \n",
      "This sample had a variance of 870.3048587177578\n",
      "====================================================================================================\n",
      "The test samples with the smallest covariance had indices [ 41 245]. \n",
      "These samples had a covariance of -27.267716742147968\n"
     ]
    }
   ],
   "source": [
    "### Task 4 ###\n",
    "\n",
    "Gauss = GP(kernel)\n",
    "_,posterior_variance = Gauss.posterior(test_data, 5,0.01,10,train_data, train_labels,sigma = 0.001)\n",
    "\n",
    "argmax = (posterior_variance == torch.max(posterior_variance)).nonzero()[0].numpy()\n",
    "print(f\"The test sample with the largest covariance had index {argmax}, making it a variance. \\nThis sample had a variance of {posterior_variance[argmax[0],argmax[1]]}\" )\n",
    "print(\"Hence, since we want the 2 test samples with the largest covariance, we shall find the second largest.\")\n",
    "#in order to preserve indices, we replace the max index with rows and columns of 0s\n",
    "posterior_variance_2 = posterior_variance[:]\n",
    "posterior_variance_2[argmax[0],:] = torch.full((1000,),0)\n",
    "posterior_variance_2[:,argmax[0]] = torch.full((1000,),0)\n",
    "argmax_2 = (posterior_variance_2 == torch.max(posterior_variance_2)).nonzero()[0].numpy()\n",
    "print(f\"The test sample with the second largest covariance had index {argmax_2}, making it a variance. \\nThis sample had a variance of {posterior_variance[argmax_2[0],argmax_2[1]]}\" )\n",
    "\n",
    "print('='*100)\n",
    "#Now for the least\n",
    "argmin = (posterior_variance == torch.min(posterior_variance)).nonzero()[0].numpy()\n",
    "# we get two vectors, which makes sense since they will have the same value due to symmetry\n",
    "print(f\"The test samples with the smallest covariance had indices {argmin}. \\nThese samples had a covariance of {posterior_variance[argmin[0],argmin[1]]}\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we took smallest to mean in actual value, smallest in absolute value just yielded two indices that had a covariance of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational cost of the NNGP model lies in the inversion of the matrix $K(X,X) + \\sigma^2 I$. This inversion has computational complexity $O(N^3)$. Hence as the size of the training dataset increases, this inverse will become harder and harder to compute. This is why other inversion methods like the Cholesky inverse can be utilized. However, even with this method, it can become infeasible to invert should the training dataset become large enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
