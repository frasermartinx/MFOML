{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this week's tutorial we will look into vision transformers. At the end of this tutorial you should be able to:\n",
        "- Build a vision transformer\n",
        "- Use it to classify images\n",
        "- Understand attention\n",
        "\n",
        "**If possible, use a GPU to train the model (if you don't have a GPU you can try to run on Google Colab).**"
      ],
      "metadata": {
        "id": "AnjBZ2VrXl7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Vision Transformer (ViT)"
      ],
      "metadata": {
        "id": "z7ldEhr3X5qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yd7N8HBjZCvk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device.type}.\")"
      ],
      "metadata": {
        "id": "TBdNtnLMkwQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the data"
      ],
      "metadata": {
        "id": "WeuHmOUYfcq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will work with the FashionMNIST dataset. Let's load the data."
      ],
      "metadata": {
        "id": "jZD4QC1wYJtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize((120, 120), antialias=True)\n",
        "     ]\n",
        "    )\n",
        "\n",
        "batch_size = 2048\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        "    )\n",
        "\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
      ],
      "metadata": {
        "id": "2joMbW-7afsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21fa50e-6286-49de-ef72-6a641db7ab96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 15196146.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 273350.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4975764.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 20389307.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualise one of the images in the dataset. Note that we have resized the images to be of size 120x120."
      ],
      "metadata": {
        "id": "mzYEnKNMYQhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_tensor_image(img, label=None):\n",
        "    img = img.permute(1,2,0)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    if label:\n",
        "      plt.title(classes[label])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "i6MPoDUImbqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_tensor_image(img=trainset[42][0], label=trainset[42][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "kHywqdvrmPYJ",
        "outputId": "b4cbb453-5843-436b-d99e-21f456b61047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step will be to split the data into the required dimenions. We want to partition each image into a square grid with size of side num_patches_per_row. Each patch or square in our grid will be inputted as a component to the images in the batch.\n",
        "\n",
        "Let's define a function that partitions an image into num_patches_per_row x num_patches_per_row different images and returns them as a batch of flattened tensors. For example, if we split one image of size 120x120 into 9 patches (3 per row), we want to obtain a tensor of size (9x1600) - 9 patches of size 40x40 flattened into 1-dimensional tensors of size 1600."
      ],
      "metadata": {
        "id": "AmXesLzWYXCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(object):\n",
        "  def __init__(self, num_patches_per_row):\n",
        "    self.num_patches_per_row = num_patches_per_row\n",
        "\n",
        "  def __call__(self, image):\n",
        "    # Fill in steps\n",
        "    # compute patch size\n",
        "    patch_size =\n",
        "    # compute patches (hint: use .unfold) - should get a tensor of dimension (num_patches, patch_size, patch_size)\n",
        "    patches =\n",
        "    # flatten patches\n",
        "    patches_flat = patches.contiguous().view(self.num_patches_per_row**2, -1)\n",
        "    return patches_flat"
      ],
      "metadata": {
        "id": "mMJQ00oermiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply the transformation when we load the data, so that we obtain our dataset in the desired format."
      ],
      "metadata": {
        "id": "IllR9NVhaTMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_patches_per_row=3\n",
        "\n",
        "transform_patches = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize((120, 120), antialias=True),\n",
        "     Patches(num_patches_per_row)\n",
        "     ]\n",
        "    )\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_patches)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_patches)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "TCUFLDm3uoIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the dimension of an image in our dataset."
      ],
      "metadata": {
        "id": "vAPgE4zaaasK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainset[42][0].size())"
      ],
      "metadata": {
        "id": "_F0CC6UyrPvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualise each of the patches separately to confirm that they make up the images in our train set."
      ],
      "metadata": {
        "id": "wU97pw21akQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_image_patches(img, label=None):\n",
        "  image_list = []\n",
        "  # get length of side of patch\n",
        "  img_size = int(np.sqrt(img.shape[1]))\n",
        "  # iterate through each patch in img\n",
        "  for i in range(img.shape[0]):\n",
        "    # reshape image\n",
        "    reshaped_img = img[i].view(1, img_size, img_size)\n",
        "    # add to list\n",
        "    image_list.append(reshaped_img)\n",
        "\n",
        "  # display in grid\n",
        "  num_rows = int(np.sqrt(img.shape[0]))\n",
        "  img_grid = torchvision.utils.make_grid(image_list, nrow=num_rows, normalize=True, pad_value=0.9)\n",
        "  img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "  # plot images\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  plt.title(f\"{classes[label]} - patches\")\n",
        "  plt.imshow(img_grid)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "# visualise image\n",
        "view_image_patches(trainset[42][0], label=trainset[42][1])"
      ],
      "metadata": {
        "id": "Ckabh_LXnqE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a Vision Transformer\n",
        "\n",
        "Let's begin the code for the Vision Transformer. We follow the ideas from https://arxiv.org/pdf/2010.11929.pdf."
      ],
      "metadata": {
        "id": "gH6qubrWfjQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional embeddings\n",
        "\n",
        "Each initial tensor containing the pixel values of the flattened patches will be encoded into a smaller dimension which we call the model dimension. We will denote this as d_model. Hence, if we denote length as the number of patches, w will end up with a tensor of size length x d_model for each train image in our batch.\n",
        "\n",
        "It is important to make sure that the model is aware of the position of each of the patches within the image (that is, the patches are not just randomly placed together, they are placed in a certain order that is important in order for the image to make sense). To do this, we will use a positional embedding of the image. We will add the position number (e.g., if we have 9 patches, 0 to 8) to the encoded values of the corresponding patch.\n",
        "\n",
        "For now, let's define an object that returns a tensor of the same size as length x d_model. Each entry will contain the positional embedding of the patch it would belong to if we were considering the values of the entries for each patch instead."
      ],
      "metadata": {
        "id": "z7c9xGBVcl74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    compute trivial positional encoding\n",
        "    \"\"\"\n",
        "    def __init__(self, length, d_model, device):\n",
        "        \"\"\"\n",
        "        constructor of positional encoding class\n",
        "        :param length: first dimension of input (number of patches)\n",
        "        :param d_model: dimension of input after mapping\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pos = torch.arange(0, length, device=device)\n",
        "        pos = pos.float().unsqueeze(dim=1)\n",
        "\n",
        "        self.encoding = pos.tile((1, d_model))\n",
        "        self.encoding.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoding"
      ],
      "metadata": {
        "id": "amzOECAZe4Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task: explore what the above function does\n",
        "\n",
        "Copy out the steps of computation so you understand what the function is doing exactly and what the dimension of the output is."
      ],
      "metadata": {
        "id": "VnOEjfpwV53B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 2\n",
        "length = 5\n",
        "d_model = 10\n",
        "x = torch.randn((bs, length, d_model))\n",
        "\n",
        "# implement steps in forward function, first step is done below\n",
        "pos = torch.arange(0, length, device=device)\n",
        "\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8jm1SpqU9g4",
        "outputId": "bf2ac7c0-c72f-499e-aa9c-0dd520b7119c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention\n",
        "\n",
        "Our next step will be to define the multi-head attention layers. We first define an attention layer. Given our matrices $Q, K$ and $V$, the output will be given by\n",
        "$$\n",
        "\\text{output} = softmax \\left (\\frac{QK^T}{\\sqrt{d}}\\right ).\n",
        "$$\n",
        "\n",
        "Instead of using all our input directly in one attention layer, we will split it and apply attention on each of the parts separately. The number of attention heads determines how many chunks of our input we use. Our input will be split along the dimension of size d_model. Thus, if n_heads is the number of heads for our multi-attention layer, we will end up with tensors of size determined by the batch size, head number, length (number of patches) and d_model/n_heads. This is done for our $Q, K$ and $V$ tensors.\n",
        "\n",
        "Note that $d$ in the above fomula will be given by d_model/n_heads."
      ],
      "metadata": {
        "id": "927N5_tmfqZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaleDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaleDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # input is of size (batch_size, n_heads, length, d_model/n_heads)\n",
        "        batch_size, head, length, d_tensor = k.size()\n",
        "        # fill in steps\n",
        "        # calculate Key^T\n",
        "        k_t =\n",
        "        # do dot-product between Query and Key^T\n",
        "        # score should be [batch_size, n_heads, length, length]\n",
        "        score =\n",
        "        # softmax\n",
        "        score =\n",
        "        # output is [batch_size, n_heads, length, d_model/n_heads]\n",
        "        output =\n",
        "        return output, score"
      ],
      "metadata": {
        "id": "5ifEPW-ZcQFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can define a class that applies the necessary transformations to our input to get it to the right dimensions, and then applies the attention layer.\n",
        "\n",
        "1.   Scale our input with the weight matrices $W^Q, W^K$ and $W^V$ to get $Q, K$ and $V$\n",
        "2.   Split $Q, K$ and $V$ into smaller parts\n",
        "3.   Pass through attention layer\n",
        "4.   Concatenate outputs from all attention heads into one final output\n",
        "\n"
      ],
      "metadata": {
        "id": "_GeMb1cOcXbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_head, device):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = ScaleDotProductAttention()\n",
        "        # to get the q,v,k we do a linear multiplication\n",
        "        self.w_q = nn.Linear(d_model, d_model, device=device)\n",
        "        self.w_k = nn.Linear(d_model, d_model, device=device)\n",
        "        self.w_v = nn.Linear(d_model, d_model, device=device)\n",
        "        self.w_concat = nn.Linear(d_model, d_model, device=device)\n",
        "\n",
        "    def forward(self, q, k, v): # in our case q, k and v will be our input (three times)\n",
        "        # fill in steps\n",
        "        # take in Q,K,V and do dot product with big weight matrices\n",
        "        q, k, v =\n",
        "\n",
        "        # split tensor by number of heads\n",
        "        # (i.e. split big vector into smaller chunks = equivalent to multiplying each q,v,k with many smaller weight matrices)\n",
        "        q, k, v =\n",
        "\n",
        "        # do attention on each head\n",
        "        out, attention_score =\n",
        "\n",
        "        # concatenate and pass through a linear layer; output has size [batch_size, 1, length, d_model]\n",
        "        out = self.concat(out)\n",
        "        out = self.w_concat(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, tensor):\n",
        "        batch_size, length, d_model = tensor.size()\n",
        "        if d_model % self.n_head != 0:\n",
        "            raise AssertionError('Model dimension d_model must be divisible by number of attention heads n_head.')\n",
        "        d_tensor = d_model // self.n_head\n",
        "        # this splits a big vector of size (batch_size, length, d_model) into (batch_size,length, n_head, d_model/n_head)\n",
        "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor)\n",
        "        # transpose to (batch_size, n_head, length, d_model/n_head)\n",
        "        tensor = tensor.transpose(1, 2)\n",
        "        return tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def concat(tensor):\n",
        "        # d_tensor is d_model / n_head\n",
        "        batch_size, head, length, d_tensor = tensor.size()\n",
        "        d_model = head * d_tensor\n",
        "        # contiguous is used to avoid error from view() as transpose makes tensor non-contiguous\n",
        "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "sTFrUhzkgLAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structure of layers\n",
        "\n",
        "The structure of the (group of) layers will be as follows:\n",
        "\n",
        "1.   Layer normalisation\n",
        "2.   Multihead attention layer + skip connection\n",
        "3.   Layer normalisation\n",
        "4.   Linear layer\n",
        "6.   ReLU\n",
        "5.   Linear layer + skip connection\n",
        "\n",
        "Let's first start with implementing LayerNorm. We want to normalise along the patches of each image in the batch."
      ],
      "metadata": {
        "id": "oWEgNE8-g632"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, device, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model, device=device))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model, device=device))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # mean is taken over last dimension; this is def of layernorm; so over d_model\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        var = x.var(-1, unbiased=False, keepdim=True)\n",
        "        # so output remains of size x\n",
        "        out = (x-mean) / torch.sqrt(var + self.eps)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zgLX1CLEFk09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now implement the structure of the layer defined above."
      ],
      "metadata": {
        "id": "bK0f1WReIrqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_hidden, n_head, device):\n",
        "        super(OneLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head, device=device)\n",
        "        self.norm1 = LayerNorm(d_model=d_model, device=device)\n",
        "        # this next piece is the positionwise feedforward - to maintain sizes\n",
        "        self.linear2 = nn.Linear(d_model, n_hidden, device=device)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear3 = nn.Linear(n_hidden, d_model, device=device)\n",
        "        self.norm2 = LayerNorm(d_model=d_model, device=device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_ = self.norm1(x)\n",
        "        x = self.attention(q = x_, k = x_, v = x_) + x\n",
        "        x_ = self.norm2(x)\n",
        "        x = self.linear3(self.relu(self.linear2(x_))) + x\n",
        "        return x"
      ],
      "metadata": {
        "id": "XWqYyJNjIzDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structure of Vision Transformer\n",
        "\n",
        "We can now combine all the code above to obtain the structure of the Vision Transformer in order to perform classification.\n",
        "\n",
        "1.   Map flattened patches of size d_init into d_model\n",
        "2.   Add positional encoding\n",
        "3.   Apply layers (OneLayer defined above) for n_layers number of times\n",
        "4.   Average pool to reshape output\n",
        "5.   Linear layer to map to a tensor with size correposnding to the number of classes (10 in our case)\n",
        "6.   Softmax to get probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "GmldBDdUI8Cl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfmMTEcvU_7U"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, length, d_init, d_model, n_hidden, n_head, n_layers, n_classes, device):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_init, d_model, device=device)  # maps initial dimension to d_model\n",
        "        self.encoding = PositionalEncoding(length=length, d_model=d_model, device=device)\n",
        "\n",
        "        #list of layers\n",
        "        layer_list = []\n",
        "        for _ in range(n_layers):\n",
        "            layer_list.append(OneLayer(d_model=d_model, n_hidden=n_hidden, n_head=n_head, device=device))\n",
        "        self.layers = nn.ModuleList(layer_list)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, d_model))\n",
        "        self.linear2 = nn.Linear(d_model, n_classes, device=device)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Map the input dimension to model dimension\n",
        "        x = self.linear1(x)\n",
        "        # Add positional encoding\n",
        "        x = self.encoding(x) + x\n",
        "        # Attention layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        # Global 1D pooling layer (x is of size [n_batches, length, d_model])\n",
        "        x = self.avgpool(x).squeeze()\n",
        "        # Final linear layer and softmax to map to the correct dimension\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        # All done :)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"vision_transformer\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a function to evaluate accuracy."
      ],
      "metadata": {
        "id": "57mWcFmfKt8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(dataloader, model, n=1):\n",
        "  num_samples = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for i, data in enumerate(dataloader):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(images)\n",
        "    _, inds = torch.topk(output, n)\n",
        "    num_correct += int(torch.sum(labels == inds.flatten()))\n",
        "    num_samples += len(labels)\n",
        "\n",
        "  return num_correct / num_samples"
      ],
      "metadata": {
        "id": "_pa60LCSwYLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have to define the hyperparameters of our network. The number of patches and length of the flattened patches can be extracted from the structure of our dataset. Batch size was also specified when we loaded the dataset.\n",
        "\n",
        "The other hyperparameters we have to choose ourselves. Feel free to modify these to try to obtain a better performance of the model."
      ],
      "metadata": {
        "id": "4icN2YNpKxfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_patches, d_init = trainset[0][0].shape\n",
        "\n",
        "params = {\n",
        "    'length': num_patches,  # Number of patches the image is broken up into\n",
        "    'd_init': d_init,  # Size of the flattened image patch\n",
        "    'd_model': 100,\n",
        "    'n_hidden': 100,\n",
        "    'n_head': 5,\n",
        "    'n_layers': 5,\n",
        "    'n_classes': 10,\n",
        "    'device': device\n",
        "}\n",
        "\n",
        "model = VisionTransformer(**params)"
      ],
      "metadata": {
        "id": "gVaS2qpvZk-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model. If it is taking too long to train just make sure that the model works (that is, that it runs one epoch)."
      ],
      "metadata": {
        "id": "jbubQwMELfcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 60\n",
        "\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
        "losses = []\n",
        "accuracies = []\n",
        "pbar = tqdm(range(num_epochs))\n",
        "\n",
        "for epoch in pbar:\n",
        "  avg_loss = []  # average loss across batches\n",
        "  for i, data in enumerate(trainloader):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    output = model(images)\n",
        "    loss = cross_entropy_loss(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    avg_loss.append(loss.detach().item())\n",
        "\n",
        "  loss_averaged = np.array(avg_loss).mean()\n",
        "  scheduler.step()\n",
        "\n",
        "  # Evaluate the model every 25 time steps on the test set\n",
        "  if epoch % 25 == 0:\n",
        "    accuracies.append(accuracy(testloader, model, n=1))\n",
        "\n",
        "  # Update progress bar\n",
        "  description = (\n",
        "    f'Loss {loss_averaged:.3f} | '\n",
        "    f'learning rate {optimizer.param_groups[0][\"lr\"]:.6f}'\n",
        "  )\n",
        "  pbar.set_description(description)\n",
        "  losses.append(np.array(avg_loss).mean())\n"
      ],
      "metadata": {
        "id": "07s12VcvdzZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model."
      ],
      "metadata": {
        "id": "G9JvhSPLZJJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross entropy loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NjVBxCeA0bt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(25*np.arange(len(accuracies)), accuracies)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Test set top-1 accuracy')\n",
        "plt.xticks(25*np.arange(len(accuracies)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZgNox1A3nc36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy(testloader, model, n=1)\n",
        "print(f\"Model achieved {100*acc:.2f}% accuracy on test set.\")"
      ],
      "metadata": {
        "id": "WmT2WhfdzdEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "- [An image is worth 16X16 words: Transformers for image recognition at scale](https://arxiv.org/pdf/2010.11929.pdf)\n",
        "- [Better plain ViT baselines for ImageNet-1k](https://arxiv.org/pdf/2205.01580.pdf)"
      ],
      "metadata": {
        "id": "3QFTlWgj18j8"
      }
    }
  ]
}